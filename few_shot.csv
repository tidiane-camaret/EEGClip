,summary,config,name
0,"{'_wandb': {'runtime': 60}, 'val_acc': 0.6416696144322603, 'val_loss': 1.2843178510665894, '_timestamp': 1686369811.7521365, 'val_acc_rec': 0.6575875486381323, 'val_acc_balanced': 0.4764230183224597, '_step': 9, 'epoch': 9, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.47568399942701617, '_runtime': 60.969754457473755}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",morning-leaf-916
1,"{'val_acc_balanced': 0.4343266137958875, 'val_acc_rec_balanced': 0.4132645752757485, '_wandb': {'runtime': 59}, 'val_acc': 0.5360806508666431, 'val_loss': 1.1301122903823853, 'val_acc_rec': 0.5252918287937743, 'trainer/global_step': 29, '_step': 9, 'epoch': 9, '_runtime': 59.75222206115723, '_timestamp': 1686369665.726522}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",eager-planet-915
2,"{'val_acc_rec_balanced': 0.5030081650193382, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 59}, 'val_acc': 0.5675627874071454, 'val_loss': 1.297390103340149, '_timestamp': 1686369519.6461494, 'trainer/global_step': 29, '_runtime': 60.361825466156006, 'val_acc_rec': 0.5797665369649806, 'val_acc_balanced': 0.4950694091755544}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",logical-wind-914
3,"{'val_acc_balanced': 0.4272408224922191, 'val_acc_rec_balanced': 0.4294513679988541, 'epoch': 9, '_wandb': {'runtime': 60}, 'val_loss': 0.8213265538215637, 'val_acc_rec': 0.3463035019455253, 'trainer/global_step': 29, '_step': 9, 'val_acc': 0.3427661832331093, '_runtime': 61.37761735916138, '_timestamp': 1686369371.7669537}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",stilted-river-913
4,"{'val_acc_balanced': 0.500126968004063, '_step': 9, '_runtime': 59.3674042224884, 'val_loss': 0.8423414826393127, '_timestamp': 1686369221.7562222, 'val_acc_rec': 0.3035019455252918, 'epoch': 9, '_wandb': {'runtime': 58}, 'val_acc': 0.30367881146091263, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",peach-frog-912
5,"{'_runtime': 62.25266623497009, '_timestamp': 1686369076.5675802, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 61}, 'val_acc': 0.3035019455252918, 'val_loss': 0.8829079866409302, 'val_acc_rec': 0.3035019455252918, 'val_acc_balanced': 0.5, '_step': 9, 'epoch': 9}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",jolly-armadillo-911
6,"{'trainer/global_step': 29, 'val_acc_rec_balanced': 0.44252256123764505, '_step': 9, 'epoch': 9, 'val_loss': 0.7677521705627441, '_timestamp': 1686368928.4084878, 'val_acc_balanced': 0.454538943365759, '_wandb': {'runtime': 57}, 'val_acc': 0.5415634948708878, '_runtime': 57.60061168670654, 'val_acc_rec': 0.5408560311284046}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",neat-capybara-910
7,"{'val_loss': 0.6811920404434204, '_timestamp': 1686368785.2786384, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 59}, 'val_acc': 0.6185001768659356, 'val_acc_rec_balanced': 0.5813995129637588, '_runtime': 59.46077632904053, 'val_acc_rec': 0.6536964980544747, 'val_acc_balanced': 0.5623803570730945, 'trainer/global_step': 29}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",sandy-spaceship-909
8,"{'val_acc_balanced': 0.5125877381463974, '_wandb': {'runtime': 60}, '_timestamp': 1686368639.111317, 'val_acc_rec': 0.5758754863813229, '_runtime': 60.83736991882324, 'val_loss': 0.8096601963043213, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5291505514969201, '_step': 9, 'epoch': 9, 'val_acc': 0.5461620091970286}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",valiant-terrain-908
9,"{'_step': 9, '_wandb': {'runtime': 57}, 'val_acc': 0.6964980544747081, '_runtime': 57.95758271217346, 'val_loss': 0.6480657458305359, 'val_acc_balanced': 0.5, 'epoch': 9, '_timestamp': 1686368491.719529, 'val_acc_rec': 0.6964980544747081, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",fanciful-dew-907
10,"{'val_acc_rec_balanced': 0.5, 'epoch': 9, 'val_acc': 0.6961443226034666, 'train_loss': 0.6670753359794617, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.4997460639918741, 'trainer/global_step': 79, '_step': 10, '_wandb': {'runtime': 60}, '_runtime': 60.79263973236084, 'val_loss': 0.6502408981323242, '_timestamp': 1686368345.5939438}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",robust-snow-906
11,"{'_wandb': {'runtime': 61}, '_runtime': 62.43207144737244, 'val_loss': 0.623629093170166, '_timestamp': 1686368197.2959924, 'val_acc_rec': 0.6964980544747081, 'epoch': 9, 'val_acc': 0.6947293951185002, 'train_loss': 0.6579524874687195, 'val_acc_balanced': 0.5061286478884244, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5072339206417419, '_step': 10}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",neat-darkness-905
12,"{'val_loss': 0.7065249681472778, '_timestamp': 1686368047.7815475, 'train_loss': 0.6585268378257751, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.48384413538044824, 'trainer/global_step': 79, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 59}, 'val_acc': 0.5157410682702511, '_runtime': 60.227829456329346, 'val_acc_rec_balanced': 0.5048345509239364}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",fiery-plasma-904
13,"{'_wandb': {'runtime': 59}, 'val_acc': 0.5753448885744605, 'val_loss': 0.7179556488990784, '_timestamp': 1686367899.8227353, 'val_acc_rec': 0.5836575875486382, '_step': 10, 'epoch': 9, 'val_acc_balanced': 0.4672813220299254, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.4623979372582725, '_runtime': 59.647380352020264, 'train_loss': 0.6425521373748779}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",fanciful-paper-903
14,"{'train_loss': 0.6646053791046143, 'val_acc_balanced': 0.48984744305973354, 'trainer/global_step': 79, 'epoch': 9, '_wandb': {'runtime': 63}, '_runtime': 64.46263456344604, 'val_loss': 0.8304433226585388, '_timestamp': 1686367752.7332077, 'val_acc_rec': 0.33073929961089493, 'val_acc_rec_balanced': 0.4978513107004727, '_step': 10, 'val_acc': 0.33126989741775736}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",sandy-bee-902
15,"{'_step': 10, 'epoch': 9, 'val_acc_rec': 0.3035019455252918, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 62}, 'val_acc': 0.3035019455252918, '_runtime': 63.40634226799011, 'val_loss': 0.8126745223999023, '_timestamp': 1686367600.5849314, 'train_loss': 0.6973329186439514, 'trainer/global_step': 79}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",bumbling-sky-901
16,"{'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 41}, 'val_acc': 0.6964980544747081, '_runtime': 41.866557359695435, '_timestamp': 1686367446.1642714, 'train_loss': 0.6733483672142029, 'val_acc_balanced': 0.5, 'trainer/global_step': 79, '_step': 10, 'epoch': 9, 'val_loss': 0.6515263915061951, 'val_acc_rec': 0.6964980544747081}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",pretty-planet-900
17,"{'epoch': 9, 'val_acc': 0.6193845065440396, '_timestamp': 1686367305.8022945, 'train_loss': 0.64664226770401, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5161509812347801, '_step': 10, '_wandb': {'runtime': 62}, '_runtime': 62.92315459251404, 'val_loss': 0.7166375517845154, 'val_acc_rec': 0.603112840466926, 'val_acc_balanced': 0.5529863525673582}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",blooming-plant-899
18,"{'_runtime': 59.67747616767883, 'val_acc_rec': 0.6848249027237354, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.49885403237358544, '_step': 10, 'epoch': 9, 'val_acc': 0.6765122037495578, 'train_loss': 0.5491310358047485, 'val_acc_balanced': 0.4961746819288719, '_wandb': {'runtime': 59}, 'val_loss': 0.6206061840057373, '_timestamp': 1686367155.4658952}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",breezy-wildflower-898
19,"{'_step': 10, 'val_acc': 0.6964980544747081, '_runtime': 63.45897102355957, 'val_acc_rec': 0.6964980544747081, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 63}, 'val_loss': 0.7377200722694397, '_timestamp': 1686367006.997807, 'train_loss': 0.37117820978164673, 'val_acc_balanced': 0.5, 'trainer/global_step': 79}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",scarlet-planet-897
20,"{'val_acc_rec_balanced': 0.4916201117318435, '_step': 12, '_wandb': {'runtime': 59}, '_runtime': 60.36589455604553, 'val_loss': 0.6464356184005737, 'val_acc_rec': 0.6848249027237354, 'val_acc_balanced': 0.4905978565196442, 'epoch': 9, 'val_acc': 0.6811107180756987, '_timestamp': 1686366856.0513957, 'train_loss': 0.668929934501648, 'trainer/global_step': 169}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",worldly-grass-896
21,"{'_step': 12, 'val_acc': 0.6414927484966395, '_runtime': 66.80165123939514, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.4581005586592179, 'val_acc_balanced': 0.4721858681355888, 'epoch': 9, '_wandb': {'runtime': 66}, 'val_loss': 0.6529330015182495, '_timestamp': 1686366706.4803271, 'train_loss': 0.6831351518630981, 'val_acc_rec': 0.6381322957198443}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",dazzling-cherry-895
22,"{'_wandb': {'runtime': 66}, 'val_acc': 0.6964980544747081, '_runtime': 67.0249490737915, 'train_loss': 0.6651170253753662, 'val_acc_rec': 0.6964980544747081, 'trainer/global_step': 169, '_step': 12, 'epoch': 9, 'val_loss': 0.6387712359428406, '_timestamp': 1686366550.641031, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",cosmic-planet-894
23,"{'epoch': 9, 'val_acc': 0.6805801202688362, 'train_loss': 0.6538059115409851, 'val_acc_balanced': 0.5073153103879361, '_timestamp': 1686366395.9861178, 'val_acc_rec': 0.6926070038910506, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.5189084658358402, '_step': 12, '_wandb': {'runtime': 61}, '_runtime': 62.20801591873169, 'val_loss': 0.6260367035865784}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",lucky-morning-893
24,"{'epoch': 9, '_wandb': {'runtime': 96}, 'val_acc': 0.44534842589317297, 'val_loss': 0.7000455856323242, 'train_loss': 0.6686840057373047, 'trainer/global_step': 179, '_step': 12, '_runtime': 96.56227946281432, '_timestamp': 1686366244.1432054, 'val_acc_rec': 0.4591439688715953, 'val_acc_balanced': 0.5299823547030251, 'val_acc_rec_balanced': 0.5430095974788712}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",noble-serenity-892
25,"{'_step': 12, '_runtime': 65.81166219711304, 'train_loss': 0.6772854924201965, 'val_acc_balanced': 0.6091176049276608, 'val_acc_rec_balanced': 0.62795444778685, 'val_acc_rec': 0.5875486381322957, 'trainer/global_step': 179, 'epoch': 9, '_wandb': {'runtime': 65}, 'val_acc': 0.5631411390166254, 'val_loss': 0.6619358658790588, '_timestamp': 1686366060.7489102}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",worthy-wood-891
26,"{'val_loss': 0.6458853483200073, '_timestamp': 1686365906.515113, 'train_loss': 0.685546875, 'val_acc_balanced': 0.5, 'trainer/global_step': 189, '_step': 12, '_wandb': {'runtime': 65}, 'val_acc': 0.6964980544747081, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_runtime': 65.36408805847168, 'val_acc_rec': 0.6964980544747081}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",sunny-moon-890
27,"{'_timestamp': 1686365753.874683, 'val_acc_rec': 0.7003891050583657, 'val_acc': 0.6980898478952954, '_runtime': 64.06191897392273, 'val_loss': 0.5979186296463013, 'train_loss': 0.6370499730110168, 'val_acc_balanced': 0.5050884869320623, 'trainer/global_step': 189, 'val_acc_rec_balanced': 0.5064102564102564, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 63}}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",prime-flower-889
28,"{'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 64}, 'val_acc': 0.6964980544747081, '_runtime': 64.53781914710999, 'val_loss': 0.6165125370025635, '_timestamp': 1686365603.5201242, 'train_loss': 0.647226095199585, '_step': 12, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, 'trainer/global_step': 159}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",dauntless-frog-888
29,"{'_step': 12, 'val_acc': 0.6961443226034666, '_runtime': 65.65805172920227, 'val_loss': 0.5998196005821228, 'epoch': 9, '_wandb': {'runtime': 65}, '_timestamp': 1686365450.7503898, 'train_loss': 0.5989307761192322, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5244071570887213, 'trainer/global_step': 159, 'val_acc_rec_balanced': 0.5180848016043547}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",polar-firefly-887
30,"{'val_loss': 0.638592541217804, 'train_loss': 0.5981317162513733, 'val_acc_rec': 0.6926070038910506, 'trainer/global_step': 369, 'epoch': 9, 'val_acc': 0.6915458082773258, '_runtime': 68.54046845436096, 'val_acc_balanced': 0.4964448958862367, 'val_acc_rec_balanced': 0.4972067039106145, '_step': 16, '_wandb': {'runtime': 68}, '_timestamp': 1686365293.3742774}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",deft-terrain-886
31,"{'val_loss': 0.6259406805038452, 'train_loss': 0.5693730711936951, 'val_acc_rec': 0.6964980544747081, 'trainer/global_step': 369, 'epoch': 9, 'val_acc': 0.6901308807923594, '_runtime': 66.61217784881592, '_timestamp': 1686365134.112119, 'val_acc_balanced': 0.4978952611634176, 'val_acc_rec_balanced': 0.5, '_step': 16, '_wandb': {'runtime': 66}}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",jolly-shadow-885
32,"{'_wandb': {'runtime': 69}, 'val_acc': 0.6927838698266714, 'train_loss': 0.6900650858879089, 'val_acc_balanced': 0.4976624864893021, '_step': 16, '_runtime': 69.97710061073303, 'val_loss': 0.6689837574958801, '_timestamp': 1686364978.0547087, 'val_acc_rec': 0.6964980544747081, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.5, 'epoch': 9}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",glorious-shape-884
33,"{'_wandb': {'runtime': 70}, 'val_acc': 0.6805801202688362, '_runtime': 70.90521955490112, '_timestamp': 1686364817.5969956, 'val_acc_rec': 0.6770428015564203, 'trainer/global_step': 369, '_step': 16, 'epoch': 9, 'val_acc_balanced': 0.5819562188277272, 'val_acc_rec_balanced': 0.5728405672539751, 'val_loss': 0.6137411594390869, 'train_loss': 0.6104863286018372}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",misty-glitter-883
34,"{'_step': 16, 'epoch': 9, 'val_loss': 0.6654232740402222, 'train_loss': 0.6373473405838013, 'val_acc_rec': 0.5291828793774319, 'val_acc_rec_balanced': 0.5498854032373586, '_wandb': {'runtime': 65}, 'val_acc': 0.5033604527767952, '_runtime': 65.8042197227478, '_timestamp': 1686364656.9815748, 'val_acc_balanced': 0.54598032321496, 'trainer/global_step': 359}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",neat-voice-882
35,"{'_runtime': 68.98868227005005, 'val_loss': 0.6561030745506287, '_timestamp': 1686364500.7682133, 'train_loss': 0.6922855973243713, 'val_acc_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 68}, 'val_acc': 0.6964980544747081, 'trainer/global_step': 359, '_step': 16, 'val_acc_rec': 0.6964980544747081, 'val_acc_rec_balanced': 0.5}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",devoted-moon-881
36,"{'_step': 16, 'epoch': 9, 'val_acc': 0.6964980544747081, 'val_loss': 0.6285672187805176, '_timestamp': 1686364340.383324, 'val_acc_balanced': 0.5, 'trainer/global_step': 389, '_wandb': {'runtime': 70}, '_runtime': 70.94353985786438, 'train_loss': 0.657469630241394, 'val_acc_rec': 0.6964980544747081, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",fragrant-disco-880
37,"{'train_loss': 0.6608315706253052, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 70}, '_runtime': 70.62160134315491, 'val_loss': 0.6313237547874451, '_timestamp': 1686364178.1326494, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, 'trainer/global_step': 389, '_step': 16, 'val_acc': 0.6964980544747081}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",kind-grass-879
38,"{'val_loss': 0.6264039874076843, '_timestamp': 1686364014.6291423, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, '_runtime': 73.63118052482605, 'train_loss': 0.6874293088912964, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.5, '_step': 15, 'epoch': 9, '_wandb': {'runtime': 73}, 'val_acc': 0.6964980544747081}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",vocal-pine-878
39,"{'val_loss': 0.597893238067627, 'val_acc_rec': 0.708171206225681, '_runtime': 69.7700765132904, '_timestamp': 1686363850.9900026, 'train_loss': 0.6391085982322693, 'val_acc_balanced': 0.5256914872836661, '_step': 15, 'epoch': 9, '_wandb': {'runtime': 69}, 'val_acc': 0.6986204457021578, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.5373155708351239}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",exalted-galaxy-877
40,"{'val_loss': 0.6322495937347412, '_timestamp': 1686363692.3239465, 'train_loss': 0.6558721661567688, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, 'trainer/global_step': 929, '_step': 27, '_wandb': {'runtime': 80}, '_runtime': 80.54826354980469, 'val_acc_rec_balanced': 0.5, 'epoch': 9, 'val_acc': 0.6964980544747081}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",hopeful-shape-876
41,"{'_runtime': 81.41816568374634, 'val_acc_balanced': 0.5, 'train_loss': 0.6558739542961121, 'val_acc_rec': 0.6964980544747081, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 81}, 'val_acc': 0.6964980544747081, 'val_loss': 0.6322548985481262, '_timestamp': 1686363518.9869926, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.5}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",driven-pyramid-875
42,"{'trainer/global_step': 939, 'epoch': 9, '_wandb': {'runtime': 80}, 'val_acc': 0.6964980544747081, 'train_loss': 0.6768916249275208, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 27, '_runtime': 81.06966876983643, 'val_loss': 0.6381238698959351, '_timestamp': 1686363342.969036, 'val_acc_rec': 0.6964980544747081}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",desert-valley-874
43,"{'val_acc_balanced': 0.5637167767056035, 'trainer/global_step': 939, 'epoch': 9, 'val_acc': 0.6929607357622922, '_runtime': 79.4685652256012, 'train_loss': 0.6833928823471069, 'val_acc_rec': 0.6964980544747081, 'val_acc_rec_balanced': 0.5542544048130641, '_step': 27, '_wandb': {'runtime': 79}, 'val_loss': 0.5959263443946838, '_timestamp': 1686363167.3722591}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",fearless-frost-873
44,"{'val_acc_balanced': 0.5, 'trainer/global_step': 909, 'val_acc_rec_balanced': 0.5, '_step': 27, '_wandb': {'runtime': 81}, 'val_acc': 0.6964980544747081, 'val_loss': 0.630756139755249, 'val_acc_rec': 0.6964980544747081, 'epoch': 9, '_runtime': 81.68383383750916, '_timestamp': 1686362993.6192975, 'train_loss': 0.6852648258209229}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",zesty-shadow-872
45,"{'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 81}, 'val_acc_rec': 0.6964980544747081, 'val_acc': 0.6964980544747081, '_runtime': 81.93036103248596, 'val_loss': 0.6312931776046753, '_timestamp': 1686362817.808893, 'train_loss': 0.6690422892570496, 'trainer/global_step': 909, '_step': 27, 'epoch': 9}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",revived-sun-871
46,"{'val_acc_rec_balanced': 0.5, '_step': 27, '_wandb': {'runtime': 80}, 'val_loss': 0.6437833309173584, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, 'trainer/global_step': 929, 'epoch': 9, 'val_acc': 0.6964980544747081, '_runtime': 81.102055311203, '_timestamp': 1686362640.7256603, 'train_loss': 0.6808903813362122}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",ethereal-firebrand-870
47,"{'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, 'trainer/global_step': 929, '_wandb': {'runtime': 82}, 'val_acc': 0.6964980544747081, 'train_loss': 0.6873924136161804, 'val_loss': 0.6097164154052734, '_timestamp': 1686362464.3491952, 'val_acc_rec_balanced': 0.5, '_step': 27, 'epoch': 9, '_runtime': 83.0194091796875}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",splendid-surf-869
48,"{'epoch': 9, 'val_acc': 0.6964980544747081, 'val_loss': 0.6252593994140625, 'train_loss': 0.7003766298294067, 'trainer/global_step': 889, 'val_acc_rec_balanced': 0.5, '_step': 26, '_wandb': {'runtime': 80}, '_runtime': 80.36824822425842, '_timestamp': 1686362286.8518622, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",resilient-brook-868
49,"{'_wandb': {'runtime': 81}, 'train_loss': 0.6802505254745483, 'trainer/global_step': 889, 'val_loss': 0.6002167463302612, '_timestamp': 1686362111.481098, 'val_acc_rec': 0.7159533073929961, 'val_acc_balanced': 0.5745464963342058, '_step': 26, 'epoch': 9, 'val_acc': 0.7078174743544393, '_runtime': 81.27842593193054, 'val_acc_rec_balanced': 0.5754548059017333}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",jolly-pyramid-867
50,"{'train_loss': 0.6563767790794373, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'epoch': 9, '_wandb': {'runtime': 100}, 'val_acc': 0.6964980544747081, '_runtime': 100.31038570404051, 'val_loss': 0.6334945559501648, '_timestamp': 1686361937.2022717, 'val_acc_rec_balanced': 0.5, '_step': 46}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",likely-snowflake-866
51,"{'train_loss': 0.6223272085189819, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_timestamp': 1686361733.7919872, 'val_acc': 0.6964980544747081, '_runtime': 107.38603019714355, 'val_loss': 0.5945403575897217, 'trainer/global_step': 1869, '_step': 46, '_wandb': {'runtime': 107}}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",sparkling-dragon-865
52,"{'_runtime': 101.99684309959412, 'train_loss': 0.6465759873390198, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'epoch': 9, '_wandb': {'runtime': 101}, 'val_loss': 0.6361996531486511, '_timestamp': 1686361524.7966, 'val_acc_rec': 0.6964980544747081, 'val_acc_rec_balanced': 0.5, '_step': 46, 'val_acc': 0.6964980544747081}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",lyric-pyramid-864
53,"{'_step': 46, 'epoch': 9, 'val_loss': 0.5951922535896301, '_timestamp': 1686361320.642221, 'train_loss': 0.6789500713348389, 'trainer/global_step': 1869, '_wandb': {'runtime': 103}, 'val_acc': 0.7101167315175098, '_runtime': 103.26237797737122, 'val_acc_rec': 0.7237354085603113, 'val_acc_balanced': 0.5668258650102225, 'val_acc_rec_balanced': 0.5810413980805043}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",radiant-meadow-863
54,"{'_step': 46, '_wandb': {'runtime': 101}, '_runtime': 101.82007932662964, 'val_loss': 0.6344738006591797, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'epoch': 9, 'val_acc': 0.6964980544747081, '_timestamp': 1686361115.3616474, 'train_loss': 0.6380674839019775, 'val_acc_rec': 0.6964980544747081, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",sweet-cosmos-862
55,"{'_runtime': 97.1962480545044, '_timestamp': 1686360912.709914, 'train_loss': 0.6380553841590881, 'val_acc_rec': 0.6964980544747081, 'val_acc_rec_balanced': 0.5, 'val_acc': 0.6964980544747081, 'epoch': 9, '_wandb': {'runtime': 97}, 'val_loss': 0.6344600915908813, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, '_step': 46}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",sunny-star-861
56,"{'_timestamp': 1686360712.5217342, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, '_wandb': {'runtime': 102}, 'val_acc': 0.6964980544747081, '_runtime': 102.12720918655396, 'val_loss': 0.6339168548583984, 'train_loss': 0.656553328037262, 'val_acc_rec': 0.6964980544747081, 'val_acc_rec_balanced': 0.5, '_step': 46, 'epoch': 9}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",hearty-blaze-860
57,"{'val_acc_rec': 0.7159533073929961, 'val_acc_balanced': 0.5566195908374679, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5537530439765077, '_step': 46, 'val_acc': 0.705518217191369, '_runtime': 99.80695796012878, 'val_loss': 0.598008930683136, 'epoch': 9, '_wandb': {'runtime': 99}, '_timestamp': 1686360508.847301, 'train_loss': 0.59160977602005}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",expert-bee-859
58,"{'_step': 46, 'val_acc': 0.6964980544747081, '_runtime': 99.23903894424438, '_timestamp': 1686360307.084373, 'train_loss': 0.6819789409637451, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 99}, 'val_loss': 0.6334660053253174, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869}","{'weights': 'under_50_task', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",zany-silence-858
59,"{'_wandb': {'runtime': 100}, 'val_acc': 0.7058719490626105, '_runtime': 100.96947741508484, 'val_loss': 0.5994270443916321, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5481664517977367, '_step': 46, 'epoch': 9, '_timestamp': 1686360105.9740293, 'train_loss': 0.67119300365448, 'val_acc_rec': 0.708171206225681, 'val_acc_balanced': 0.564271854774648}","{'weights': 'pathological_task', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",true-water-857
60,"{'epoch': 9, '_runtime': 61.49566078186035, 'val_loss': 0.8965469002723694, 'val_acc_rec': 0.48249027237354086, 'val_acc_balanced': 0.5185076006687259, 'val_acc_rec_balanced': 0.5105955425648521, '_step': 9, '_wandb': {'runtime': 61}, 'val_acc': 0.4978776087725504, '_timestamp': 1686359902.6369758, 'trainer/global_step': 29}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",lucky-forest-856
61,"{'val_acc_rec_balanced': 0.5, '_step': 9, '_wandb': {'runtime': 60}, 'val_acc': 0.46303501945525294, 'val_loss': 1.0352013111114502, 'val_acc_rec': 0.46303501945525294, 'val_acc_balanced': 0.5, 'epoch': 9, '_runtime': 61.25253367424011, '_timestamp': 1686359754.0963917, 'trainer/global_step': 29}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",genial-mountain-855
62,"{'val_loss': 1.397266149520874, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5325782486907806, '_step': 9, 'val_acc': 0.5435090201627166, '_runtime': 60.525649070739746, 'val_acc_rec': 0.556420233463035, 'val_acc_balanced': 0.5224490982163617, 'epoch': 9, '_wandb': {'runtime': 60}, '_timestamp': 1686359605.832353}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",usual-cherry-854
63,"{'val_loss': 0.8642547726631165, '_timestamp': 1686359458.3219612, 'val_acc_rec': 0.5136186770428015, 'val_acc_balanced': 0.5130506748153807, 'trainer/global_step': 29, '_runtime': 59.89559316635132, 'epoch': 9, '_wandb': {'runtime': 59}, 'val_acc': 0.5176865935620799, 'val_acc_rec_balanced': 0.5083424674217514, '_step': 9}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",whole-plant-853
64,"{'_step': 9, '_wandb': {'runtime': 60}, 'val_acc': 0.4527767951892466, '_runtime': 60.736130475997925, 'epoch': 9, 'val_loss': 0.7517086267471313, '_timestamp': 1686359310.4048386, 'val_acc_rec': 0.4591439688715953, 'val_acc_balanced': 0.46099744245524293, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.4668737060041408}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",vague-energy-852
65,"{'epoch': 9, 'trainer/global_step': 29, 'val_acc_rec': 0.5330739299610895, 'val_acc_balanced': 0.5400336023737559, '_step': 9, '_wandb': {'runtime': 60}, 'val_acc': 0.5366112486735055, '_runtime': 61.1235773563385, 'val_loss': 1.6586798429489136, '_timestamp': 1686359162.5169923, 'val_acc_rec_balanced': 0.5368712702472294}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",sunny-microwave-851
66,"{'_step': 9, 'val_acc_rec': 0.5486381322957199, 'val_acc_rec_balanced': 0.535744732675679, '_timestamp': 1686359015.711188, 'val_acc_balanced': 0.5483580784092292, 'trainer/global_step': 29, 'epoch': 9, '_wandb': {'runtime': 57}, 'val_acc': 0.5611956137247966, '_runtime': 57.72350811958313, 'val_loss': 0.7901106476783752}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",fine-sponge-850
67,"{'_wandb': {'runtime': 58}, '_runtime': 58.526084423065186, '_timestamp': 1686358872.6314833, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.48331506515649736, '_step': 9, 'epoch': 9, 'val_acc': 0.5281216837637072, 'val_loss': 0.7312546968460083, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5033090311223304}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",leafy-snowball-849
68,"{'val_loss': 0.7253506183624268, '_timestamp': 1686358727.0956428, 'val_acc_rec': 0.46303501945525294, 'val_acc_rec_balanced': 0.5, '_step': 9, 'val_acc': 0.46303501945525294, '_runtime': 60.91400074958801, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'epoch': 9, '_wandb': {'runtime': 60}}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",azure-thunder-848
69,"{'_step': 9, 'epoch': 9, '_wandb': {'runtime': 61}, '_timestamp': 1686358579.6717224, 'val_acc': 0.4467633533781394, '_runtime': 61.9850435256958, 'val_loss': 0.888275146484375, 'val_acc_rec': 0.42412451361867703, 'val_acc_balanced': 0.4525055081321066, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.4302155644866642}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",revived-wave-847
70,"{'train_loss': 0.5865833163261414, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5318170746559494, '_wandb': {'runtime': 63}, '_runtime': 63.812443017959595, 'val_loss': 0.7106758952140808, '_timestamp': 1686358432.105164, 'val_acc_balanced': 0.527954462417378, '_step': 10, 'epoch': 9, 'val_acc': 0.5350194552529183, 'val_acc_rec': 0.5369649805447471}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",fine-frog-846
71,"{'_step': 10, '_runtime': 62.39013576507568, 'val_loss': 0.766637921333313, 'train_loss': 0.5370994210243225, 'val_acc_balanced': 0.53124411820064, 'epoch': 9, '_wandb': {'runtime': 61}, 'val_acc': 0.5129112133003184, '_timestamp': 1686358281.1274838, 'val_acc_rec': 0.5252918287937743, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5440872000974303}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",fine-salad-845
72,"{'val_acc_rec': 0.5291828793774319, 'val_acc_balanced': 0.5495828766289124, '_wandb': {'runtime': 53}, 'val_acc': 0.5622568093385214, '_runtime': 53.77599811553955, 'val_loss': 0.6868072152137756, '_timestamp': 1686358133.502972, 'train_loss': 0.6744436025619507, 'val_acc_rec_balanced': 0.5141578370478626, '_step': 10, 'epoch': 9, 'trainer/global_step': 79}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",autumn-flower-844
73,"{'val_acc': 0.5286522815705695, '_timestamp': 1686357991.4549594, 'val_acc_rec': 0.5408560311284046, 'val_acc_balanced': 0.5042764141229614, 'train_loss': 0.6606870293617249, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5123005724028742, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 61}, '_runtime': 61.62385940551758, 'val_loss': 0.7130542993545532}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",zesty-tree-843
74,"{'val_acc': 0.4759462327555713, 'val_acc_rec': 0.490272373540856, 'val_loss': 0.8648624420166016, '_timestamp': 1686357842.1297376, 'train_loss': 0.6299455761909485, 'val_acc_balanced': 0.4851488026040456, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 62}, '_runtime': 63.04531168937683, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.4999086591158202}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",lemon-monkey-842
75,"{'val_acc': 0.4778917580474001, '_runtime': 61.521153688430786, 'val_acc_rec': 0.4863813229571984, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.4864511021800024, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 61}, 'val_loss': 0.7095220685005188, '_timestamp': 1686357692.9841456, 'train_loss': 0.6889150738716125, 'val_acc_balanced': 0.48264799991142704}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",colorful-pond-841
76,"{'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5263670685665571, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 59}, '_timestamp': 1686357542.1759622, 'train_loss': 0.6117037534713745, 'val_acc_rec': 0.5603112840466926, 'val_acc_balanced': 0.5320108280466337, 'val_acc': 0.5587194906261054, '_runtime': 59.5496871471405, 'val_loss': 0.7330009937286377}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",summer-monkey-840
77,"{'val_acc_rec': 0.5214007782101168, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 58}, 'val_acc': 0.5281216837637072, '_runtime': 59.122955083847046, 'val_loss': 0.7185975313186646, '_timestamp': 1686357391.0897632, 'val_acc_balanced': 0.501573554322915, 'val_acc_rec_balanced': 0.49302764584094505, 'train_loss': 0.6483662724494934, 'trainer/global_step': 79}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",bumbling-haze-839
78,"{'trainer/global_step': 79, 'val_acc_rec_balanced': 0.4927536231884058, 'epoch': 9, '_wandb': {'runtime': 61}, 'val_acc': 0.5244074991156703, 'train_loss': 0.69101881980896, 'val_acc_balanced': 0.4903317058048516, '_step': 10, '_runtime': 62.386488914489746, 'val_loss': 0.6932640671730042, '_timestamp': 1686357244.7553148, 'val_acc_rec': 0.5291828793774319}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",daily-galaxy-838
79,"{'val_acc': 0.45507605235231696, '_runtime': 63.0559401512146, 'val_acc_balanced': 0.44544181308887193, 'epoch': 9, '_wandb': {'runtime': 62}, '_timestamp': 1686357095.299044, 'train_loss': 0.6479768753051758, 'val_acc_rec': 0.4785992217898833, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.4682133723054439, '_step': 10, 'val_loss': 0.7529575228691101}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",young-morning-837
80,"{'val_acc_rec': 0.556420233463035, '_step': 12, 'val_acc': 0.5433321542270959, '_runtime': 65.07962846755981, 'val_loss': 0.7114410400390625, '_timestamp': 1686356945.6330574, 'train_loss': 0.6544987559318542, 'val_acc_balanced': 0.5400336023737559, 'trainer/global_step': 169, 'epoch': 9, '_wandb': {'runtime': 64}, 'val_acc_rec_balanced': 0.5534039702837656}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",lilac-haze-836
81,"{'_timestamp': 1686356792.3512278, 'val_acc_balanced': 0.5303846835176759, 'val_acc_rec_balanced': 0.5303251735476799, '_step': 12, 'val_acc': 0.5137955429784223, 'val_loss': 0.7273462414741516, 'train_loss': 0.6195957660675049, 'val_acc_rec': 0.5136186770428015, 'trainer/global_step': 169, 'epoch': 9, '_wandb': {'runtime': 66}, '_runtime': 66.53769874572754}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",different-sound-835
82,"{'_wandb': {'runtime': 64}, 'val_acc': 0.5385567739653343, 'val_loss': 0.6949410438537598, '_timestamp': 1686356636.821962, 'train_loss': 0.699981153011322, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5021658861172927, '_step': 12, 'trainer/global_step': 169, '_runtime': 65.02057886123657, 'val_acc_rec_balanced': 0.5, 'epoch': 9}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",stilted-surf-834
83,"{'_step': 12, '_wandb': {'runtime': 64}, 'val_acc': 0.4913335691545808, '_runtime': 64.91063690185547, '_timestamp': 1686356483.090097, 'val_acc_balanced': 0.46676575768647377, 'epoch': 9, 'val_loss': 0.7140569090843201, 'train_loss': 0.6549085378646851, 'val_acc_rec': 0.5019455252918288, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.4714407502131288}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",elated-meadow-833
84,"{'_runtime': 64.94921135902405, 'val_loss': 0.7413272857666016, '_timestamp': 1686356328.630649, 'trainer/global_step': 179, 'val_acc_rec_balanced': 0.5, '_step': 12, 'epoch': 9, 'val_acc': 0.46303501945525294, 'val_acc_balanced': 0.5, '_wandb': {'runtime': 64}, 'train_loss': 0.650532066822052, 'val_acc_rec': 0.46303501945525294}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",trim-capybara-832
85,"{'train_loss': 0.6672903895378113, 'val_acc_rec': 0.46303501945525294, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 67}, 'val_loss': 0.71451336145401, '_timestamp': 1686356177.1735442, '_runtime': 68.06399607658386, 'val_acc_balanced': 0.5, 'trainer/global_step': 179, '_step': 12, 'epoch': 9, 'val_acc': 0.46303501945525294}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",astral-field-831
86,"{'_timestamp': 1686356021.1141264, 'train_loss': 0.5966871380805969, 'val_acc_rec': 0.5369649805447471, '_step': 12, 'epoch': 9, 'val_acc': 0.5369649805447471, 'val_acc_balanced': 0.5, 'trainer/global_step': 189, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 68}, '_runtime': 68.43257546424866, 'val_loss': 0.6981915831565857}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",icy-jazz-830
87,"{'train_loss': 0.6166557669639587, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_timestamp': 1686355864.3436046, 'val_acc': 0.5369649805447471, '_runtime': 66.37929654121399, 'val_loss': 0.7064710259437561, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, 'trainer/global_step': 189, '_step': 12, '_wandb': {'runtime': 65}}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",scarlet-sky-829
88,"{'trainer/global_step': 159, 'val_acc_rec_balanced': 0.5658567774936061, 'epoch': 9, '_runtime': 66.2415235042572, '_timestamp': 1686355711.2768235, 'val_acc_balanced': 0.5503094518439786, 'train_loss': 0.6777390837669373, 'val_acc_rec': 0.5642023346303502, '_step': 12, '_wandb': {'runtime': 65}, 'val_acc': 0.5472232048107535, 'val_loss': 0.6860249638557434}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",absurd-waterfall-828
89,"{'val_acc_balanced': 0.5, 'trainer/global_step': 159, 'val_acc_rec_balanced': 0.5, '_step': 12, '_runtime': 65.4651894569397, '_timestamp': 1686355557.8954804, 'train_loss': 0.6882206797599792, 'val_acc_rec': 0.5369649805447471, 'epoch': 9, '_wandb': {'runtime': 65}, 'val_acc': 0.5369649805447471, 'val_loss': 0.6911497712135315}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",solar-cherry-827
90,"{'_step': 16, 'val_acc': 0.555535903784931, 'train_loss': 0.6967819929122925, 'val_acc_rec': 0.5525291828793775, 'val_acc_balanced': 0.5448760531880736, 'val_acc_rec_balanced': 0.5411033978808915, 'epoch': 9, '_wandb': {'runtime': 70}, '_runtime': 70.7612373828888, 'val_loss': 0.6824405193328857, '_timestamp': 1686355402.5654025, 'trainer/global_step': 369}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",autumn-lion-826
91,"{'trainer/global_step': 369, '_wandb': {'runtime': 69}, '_timestamp': 1686355241.1108153, 'train_loss': 0.689186692237854, 'val_acc_rec': 0.5369649805447471, 'val_loss': 0.6920706033706665, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 16, 'epoch': 9, 'val_acc': 0.5369649805447471, '_runtime': 69.80967330932617}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",volcanic-universe-825
92,"{'val_acc': 0.5592500884329679, 'train_loss': 0.691778302192688, 'val_acc_rec': 0.556420233463035, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.5580319084155402, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 69}, '_runtime': 69.60231184959412, 'val_loss': 0.6859614253044128, '_timestamp': 1686355080.1863089, 'val_acc_balanced': 0.5610876761771902}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",curious-tree-824
93,"{'_wandb': {'runtime': 70}, 'val_acc': 0.527414220021224, '_runtime': 70.80230021476746, 'train_loss': 0.6523210406303406, 'val_acc_balanced': 0.5309963906511221, '_step': 16, 'val_loss': 0.7014445066452026, '_timestamp': 1686354920.3347952, 'val_acc_rec': 0.5408560311284046, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.5435391547923518, 'epoch': 9}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",pious-star-823
94,"{'_wandb': {'runtime': 71}, 'trainer/global_step': 359, '_runtime': 71.41437721252441, 'val_loss': 0.6930903792381287, '_timestamp': 1686354758.8862913, 'train_loss': 0.6930025815963745, 'val_acc_rec': 0.5369649805447471, '_step': 16, 'epoch': 9, 'val_acc': 0.5369649805447471, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",vocal-terrain-822
95,"{'_wandb': {'runtime': 68}, '_timestamp': 1686354597.066626, 'train_loss': 0.6930138468742371, 'val_acc_balanced': 0.5, 'val_acc_rec': 0.5369649805447471, 'trainer/global_step': 359, 'val_acc_rec_balanced': 0.5, '_step': 16, 'epoch': 9, 'val_acc': 0.5369649805447471, '_runtime': 68.52549314498901, 'val_loss': 0.6930946707725525}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",colorful-breeze-821
96,"{'epoch': 9, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, 'trainer/global_step': 389, '_step': 16, '_wandb': {'runtime': 71}, 'val_acc': 0.5369649805447471, '_runtime': 72.15014505386353, 'val_loss': 0.685721755027771, '_timestamp': 1686354439.269786, 'train_loss': 0.6745404601097107, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",treasured-galaxy-820
97,"{'_step': 16, 'epoch': 9, '_wandb': {'runtime': 71}, 'val_acc': 0.5261761584718783, '_runtime': 72.02441334724426, '_timestamp': 1686354278.3082802, 'train_loss': 0.6387494802474976, 'val_loss': 0.710159420967102, 'val_acc_rec': 0.5330739299610895, 'val_acc_balanced': 0.49652766244837854, 'trainer/global_step': 389, 'val_acc_rec_balanced': 0.502161734258921}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",swift-frog-819
98,"{'val_acc_balanced': 0.5612025442588102, 'val_acc_rec_balanced': 0.5749908659115821, '_wandb': {'runtime': 70}, '_runtime': 70.67539596557617, '_timestamp': 1686354115.880035, 'train_loss': 0.6366785168647766, 'val_acc_rec': 0.5758754863813229, '_step': 15, 'epoch': 9, 'val_acc': 0.560841881853555, 'val_loss': 0.6902992725372314, 'trainer/global_step': 329}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",upbeat-moon-818
99,"{'_timestamp': 1686353954.4299858, 'train_loss': 0.6600520610809326, 'val_acc_rec': 0.5680933852140078, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.5480757520399464, 'epoch': 9, 'val_acc': 0.5445702157764415, '_runtime': 65.17086386680603, 'val_loss': 0.6948329210281372, 'val_acc_balanced': 0.52722373534394, '_step': 15, '_wandb': {'runtime': 64}}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",dutiful-firefly-817
100,"{'trainer/global_step': 929, 'val_loss': 0.6971216797828674, '_timestamp': 1686353797.4074764, 'val_acc_rec': 0.46303501945525294, 'val_acc': 0.46303501945525294, '_runtime': 80.69265246391296, 'train_loss': 0.6874793767929077, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 80}}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",ethereal-dragon-816
101,"{'val_acc_balanced': 0.5, 'trainer/global_step': 929, '_wandb': {'runtime': 80}, 'val_acc': 0.46303501945525294, 'val_loss': 0.6971211433410645, 'train_loss': 0.6874792575836182, 'val_acc_rec': 0.46303501945525294, '_step': 27, 'epoch': 9, '_runtime': 80.77149200439453, '_timestamp': 1686353621.490342, 'val_acc_rec_balanced': 0.5}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",swift-moon-815
102,"{'_timestamp': 1686353445.6712692, 'trainer/global_step': 939, 'val_acc': 0.5578351609480015, 'epoch': 9, '_wandb': {'runtime': 82}, '_runtime': 82.85071325302124, 'val_loss': 0.6846076846122742, 'train_loss': 0.6988779902458191, 'val_acc_rec': 0.5642023346303502, 'val_acc_balanced': 0.5567199211700491, '_step': 27, 'val_acc_rec_balanced': 0.5658567774936061}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",treasured-aardvark-814
103,"{'_wandb': {'runtime': 81}, 'val_acc': 0.5341351255748142, 'train_loss': 0.6925135850906372, 'val_acc_rec': 0.5486381322957199, 'val_acc_balanced': 0.5299445865302643, 'trainer/global_step': 939, 'val_acc_rec_balanced': 0.5438436244062843, '_step': 27, 'epoch': 9, '_runtime': 82.21080875396729, 'val_loss': 0.6929623484611511, '_timestamp': 1686353269.5052156}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",major-water-813
104,"{'epoch': 9, '_wandb': {'runtime': 76}, 'val_acc': 0.5482844004244782, '_runtime': 76.49905633926392, 'val_loss': 0.6858043074607849, 'val_acc_rec': 0.5642023346303502, '_step': 27, '_timestamp': 1686353095.2999873, 'train_loss': 0.6887304782867432, 'val_acc_balanced': 0.5545055967050851, 'trainer/global_step': 909, 'val_acc_rec_balanced': 0.5687492388259652}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",misunderstood-violet-812
105,"{'val_acc_rec_balanced': 0.4664778955060285, 'epoch': 9, 'val_acc': 0.5014149274849664, 'val_loss': 0.6953315734863281, '_timestamp': 1686352925.3901088, 'train_loss': 0.6949005722999573, 'val_acc_rec': 0.4785992217898833, '_step': 27, '_wandb': {'runtime': 79}, '_runtime': 80.24678683280945, 'val_acc_balanced': 0.49145685942361134, 'trainer/global_step': 909}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",usual-music-811
106,"{'epoch': 9, '_wandb': {'runtime': 80}, 'val_acc': 0.5369649805447471, 'train_loss': 0.6855962872505188, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, 'trainer/global_step': 929, '_step': 27, '_runtime': 80.42265176773071, 'val_loss': 0.6905139088630676, '_timestamp': 1686352752.8401778, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",winter-music-810
107,"{'epoch': 9, '_wandb': {'runtime': 79}, '_timestamp': 1686352575.8598166, 'val_acc_rec': 0.5369649805447471, 'val_acc_rec_balanced': 0.5, '_step': 27, 'val_acc': 0.5369649805447471, '_runtime': 80.26203060150146, 'val_loss': 0.6905138492584229, 'train_loss': 0.6855951547622681, 'val_acc_balanced': 0.5, 'trainer/global_step': 929}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",lemon-grass-809
108,"{'train_loss': 0.6886630058288574, 'epoch': 9, '_wandb': {'runtime': 80}, 'val_acc': 0.5369649805447471, 'val_loss': 0.6915788650512695, '_timestamp': 1686352400.7831588, 'val_acc_rec_balanced': 0.5, '_step': 26, '_runtime': 80.68750166893005, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, 'trainer/global_step': 889}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",polar-snow-808
109,"{'val_acc_rec': 0.5369649805447471, 'trainer/global_step': 889, '_step': 26, '_wandb': {'runtime': 80}, '_runtime': 80.55796670913696, 'val_loss': 0.6915771961212158, 'train_loss': 0.6886583566665649, 'epoch': 9, 'val_acc': 0.5369649805447471, '_timestamp': 1686352223.5661516, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",polished-grass-807
110,"{'val_loss': 0.6910077333450317, '_timestamp': 1686352048.7131202, 'val_acc_rec': 0.5369649805447471, 'trainer/global_step': 1869, 'val_acc': 0.5369649805447471, '_runtime': 101.21685433387756, '_wandb': {'runtime': 101}, 'train_loss': 0.6988598704338074, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 46, 'epoch': 9}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",distinctive-universe-806
111,"{'val_acc_rec_balanced': 0.5, 'epoch': 9, 'train_loss': 0.6988611817359924, 'val_acc_rec': 0.5369649805447471, 'trainer/global_step': 1869, 'val_loss': 0.6910075545310974, '_timestamp': 1686351847.1902874, 'val_acc_balanced': 0.5, '_step': 46, '_wandb': {'runtime': 99}, 'val_acc': 0.5369649805447471, '_runtime': 99.8893404006958}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",swift-resonance-805
112,"{'_timestamp': 1686351645.524311, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.561259286323225, 'epoch': 9, '_runtime': 102.03390312194824, 'val_acc': 0.568800848956491, 'val_loss': 0.6834425926208496, 'train_loss': 0.6900083422660828, 'val_acc_rec': 0.5797665369649806, 'val_acc_balanced': 0.5527839594335758, '_step': 46, '_wandb': {'runtime': 101}}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",silver-music-804
113,"{'_step': 46, 'epoch': 9, 'val_acc': 0.5357269189954015, 'val_loss': 0.690883457660675, '_timestamp': 1686351439.878419, 'val_acc_rec': 0.5369649805447471, 'trainer/global_step': 1869, '_wandb': {'runtime': 100}, '_runtime': 100.67484593391418, 'train_loss': 0.6974427700042725, 'val_acc_balanced': 0.49926788897377133, 'val_acc_rec_balanced': 0.5}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",leafy-plant-803
114,"{'val_loss': 0.6910197734832764, '_timestamp': 1686351237.6511648, 'train_loss': 0.6975806951522827, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, '_step': 46, 'val_acc': 0.5369649805447471, '_runtime': 101.8447756767273, 'trainer/global_step': 1869, 'epoch': 9, '_wandb': {'runtime': 101}, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",true-violet-802
115,"{'val_acc_rec': 0.5369649805447471, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 101}, 'val_acc': 0.5369649805447471, 'val_loss': 0.6910197734832764, 'train_loss': 0.6975806951522827, '_step': 46, '_runtime': 101.21776056289671, '_timestamp': 1686351032.6976497, 'val_acc_balanced': 0.5}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",volcanic-puddle-801
116,"{'epoch': 9, 'val_loss': 0.6911366581916809, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, 'train_loss': 0.6937922239303589, 'trainer/global_step': 1869, '_step': 46, '_wandb': {'runtime': 105}, 'val_acc': 0.5369649805447471, '_runtime': 105.69654440879822, '_timestamp': 1686350830.8269303}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",rosy-violet-800
117,"{'val_loss': 0.6911364197731018, '_timestamp': 1686350620.6702995, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 99}, 'val_acc': 0.5369649805447471, '_runtime': 99.5566565990448, 'train_loss': 0.6937923431396484, 'val_acc_rec': 0.5369649805447471, 'val_acc_rec_balanced': 0.5}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",earnest-bee-799
118,"{'epoch': 9, '_wandb': {'runtime': 99}, 'val_loss': 0.6867368221282959, '_timestamp': 1686350420.8968766, 'val_acc_balanced': 0.5, '_step': 46, 'val_acc': 0.5369649805447471, '_runtime': 99.91232657432556, 'train_loss': 0.6855211853981018, 'val_acc_rec': 0.5369649805447471, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",divine-glade-798
119,"{'_wandb': {'runtime': 100}, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, '_step': 46, 'epoch': 9, 'val_acc': 0.5369649805447471, '_runtime': 101.16904091835022, 'val_loss': 0.6910918354988098, '_timestamp': 1686350218.808025, 'train_loss': 0.6868765950202942, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5}","{'weights': 'pathological_task', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",sunny-voice-797
120,"{'_runtime': 61.08204483985901, 'val_loss': 1.191083312034607, '_timestamp': 1686350016.657544, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 9, 'epoch': 9, 'trainer/global_step': 29, '_wandb': {'runtime': 60}, 'val_acc': 0.4046692607003891}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",classic-frog-796
121,"{'_step': 9, '_wandb': {'runtime': 55}, 'val_loss': 1.2835506200790403, '_timestamp': 1686349869.9679115, 'val_acc_rec_balanced': 0.5, 'epoch': 9, 'val_acc': 0.4046692607003891, '_runtime': 56.3395037651062, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.5, 'trainer/global_step': 29}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",pious-waterfall-795
122,"{'_step': 9, 'epoch': 9, 'val_acc': 0.5028298549699328, '_runtime': 61.091485023498535, 'val_loss': 0.7062671184539795, '_timestamp': 1686349725.463686, 'val_acc_rec': 0.4863813229571984, '_wandb': {'runtime': 60}, 'val_acc_balanced': 0.5000671305818365, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.488562091503268}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",electric-puddle-794
123,"{'val_acc_rec': 0.41245136186770426, 'val_acc_balanced': 0.5125834133187075, 'trainer/global_step': 29, '_step': 9, 'val_acc': 0.42748496639547223, 'val_loss': 0.7883368134498596, '_timestamp': 1686349579.0506232, 'epoch': 9, '_wandb': {'runtime': 59}, '_runtime': 59.667455196380615, 'val_acc_rec_balanced': 0.503456510809452}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",hopeful-oath-793
124,"{'val_acc_rec_balanced': 0.5226244343891403, '_step': 9, '_wandb': {'runtime': 57}, 'val_acc': 0.4934559603820304, 'trainer/global_step': 29, 'val_acc_rec': 0.490272373540856, 'val_acc_balanced': 0.5196992549933726, 'epoch': 9, '_runtime': 57.860671281814575, 'val_loss': 1.0515981912612915, '_timestamp': 1686349434.162599}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",copper-deluge-792
125,"{'_wandb': {'runtime': 60}, 'val_loss': 0.7183685302734375, 'trainer/global_step': 29, '_step': 9, 'val_acc': 0.42748496639547223, '_runtime': 60.90103602409363, '_timestamp': 1686349288.894442, 'val_acc_rec': 0.4085603112840467, 'val_acc_balanced': 0.5067744755244755, 'val_acc_rec_balanced': 0.5001885369532428, 'epoch': 9}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",amber-glade-791
126,"{'epoch': 9, 'val_acc': 0.5045985143261408, '_timestamp': 1686349140.366115, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5350992961287079, '_step': 9, '_wandb': {'runtime': 54}, '_runtime': 55.3760621547699, 'val_loss': 0.7171886563301086, 'val_acc_rec': 0.5252918287937743, 'val_acc_balanced': 0.5197492458521871}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",eternal-tree-790
127,"{'val_acc_balanced': 0.5089340806252571, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5020424836601307, 'val_loss': 1.120930314064026, 'val_acc_rec': 0.4785992217898833, '_wandb': {'runtime': 61}, 'val_acc': 0.4807216130173329, '_runtime': 62.05136084556579, '_timestamp': 1686348998.7507408, '_step': 9, 'epoch': 9}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",hopeful-frog-789
128,"{'_step': 9, 'epoch': 9, 'val_acc': 0.4676335337813937, '_timestamp': 1686348849.7730317, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5001256913021619, '_wandb': {'runtime': 60}, '_runtime': 60.792155504226685, 'val_loss': 0.7683039903640747, 'val_acc_rec': 0.4708171206225681, 'val_acc_balanced': 0.4926227775492481}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",celestial-wood-788
129,"{'epoch': 9, '_runtime': 60.37948560714722, '_timestamp': 1686348702.0595975, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.48240321769733535, '_step': 9, '_wandb': {'runtime': 59}, 'val_acc': 0.4764768305624337, 'val_loss': 0.8002258539199829, 'val_acc_rec': 0.4863813229571984, 'val_acc_balanced': 0.4808734974176151}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",kind-sponge-787
130,"{'trainer/global_step': 79, '_wandb': {'runtime': 61}, '_runtime': 62.12442636489868, 'train_loss': 0.6715426445007324, 'val_acc_rec': 0.4396887159533074, '_timestamp': 1686348555.214337, 'val_acc_balanced': 0.5013240436034554, 'val_acc_rec_balanced': 0.5001571141277024, '_step': 10, 'epoch': 9, 'val_acc': 0.44499469402193137, 'val_loss': 0.71993488073349}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",stellar-silence-786
131,"{'epoch': 9, 'val_acc': 0.4066147859922179, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.4976447163947164, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.4984602815485169, '_step': 10, '_wandb': {'runtime': 64}, '_runtime': 65.18724131584167, 'val_loss': 0.7227699160575867, '_timestamp': 1686348405.1603234, 'train_loss': 0.6751236915588379}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",true-yogurt-785
132,"{'trainer/global_step': 79, '_step': 10, 'epoch': 9, '_runtime': 63.39840483665466, 'val_loss': 0.7438878417015076, 'train_loss': 0.6932318210601807, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 62}, 'val_acc': 0.4087371772196675, '_timestamp': 1686348251.2815578, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.49977718360071305}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",firm-haze-784
133,"{'val_acc_balanced': 0.4891362722245075, '_step': 10, 'val_loss': 0.7767219543457031, '_timestamp': 1686348099.0264, '_runtime': 58.64423394203186, 'train_loss': 0.6703351140022278, 'val_acc_rec': 0.41245136186770426, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.49421820010055306, 'epoch': 9, '_wandb': {'runtime': 58}, 'val_acc': 0.4115670321896003}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",elated-pine-783
134,"{'epoch': 9, '_wandb': {'runtime': 61}, 'val_acc': 0.5447470817120622, 'val_acc_rec': 0.5719844357976653, 'val_acc_balanced': 0.5202948603683898, 'trainer/global_step': 79, '_step': 10, '_runtime': 62.33139991760254, 'val_loss': 0.8159171938896179, '_timestamp': 1686347954.353988, 'train_loss': 0.6332885026931763, 'val_acc_rec_balanced': 0.5496794871794872}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",wild-plant-782
135,"{'_wandb': {'runtime': 61}, 'val_acc': 0.5953307392996109, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, 'trainer/global_step': 79, '_step': 10, 'epoch': 9, '_timestamp': 1686347804.294542, 'train_loss': 0.6810373663902283, 'val_acc_rec_balanced': 0.5, '_runtime': 61.82190299034119, 'val_loss': 0.6765592098236084}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",zany-bee-781
136,"{'val_acc_balanced': 0.5, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 62}, '_runtime': 62.70394206047058, '_timestamp': 1686347653.088105, 'val_acc': 0.4046692607003891, 'val_loss': 0.7050741910934448, 'train_loss': 0.6847059726715088, 'val_acc_rec': 0.4046692607003891}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",treasured-dragon-780
137,"{'val_loss': 0.8074969053268433, '_timestamp': 1686347502.4701087, 'train_loss': 0.5638909935951233, 'val_acc_balanced': 0.5199563508387037, 'trainer/global_step': 79, '_step': 10, '_wandb': {'runtime': 62}, 'val_acc': 0.524761230986912, 'val_acc_rec_balanced': 0.5423893916540975, 'epoch': 9, '_runtime': 63.15822172164917, 'val_acc_rec': 0.5486381322957199}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",deft-disco-779
138,"{'epoch': 9, 'train_loss': 0.6958362460136414, 'val_acc_balanced': 0.49135300973536267, '_step': 10, '_wandb': {'runtime': 59}, 'val_acc': 0.4342058719490626, '_runtime': 60.09325122833252, 'val_loss': 0.7265843749046326, '_timestamp': 1686347351.2790742, 'val_acc_rec': 0.42023346303501946, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.48843640020110607}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",radiant-violet-778
139,"{'epoch': 9, '_wandb': {'runtime': 62}, 'val_acc': 0.4046692607003891, '_runtime': 63.059568643569946, 'val_acc_rec': 0.4046692607003891, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5, '_step': 10, 'val_loss': 0.697380006313324, '_timestamp': 1686347202.6632967, 'train_loss': 0.6943753957748413, 'val_acc_balanced': 0.5}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",clean-surf-777
140,"{'train_loss': 0.5997310280799866, 'val_acc_balanced': 0.5, '_wandb': {'runtime': 65}, 'val_loss': 0.7817453742027283, 'val_acc': 0.4046692607003891, '_runtime': 65.71622729301453, '_timestamp': 1686347048.9928093, 'val_acc_rec': 0.4046692607003891, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.5, '_step': 12, 'epoch': 9}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",restful-brook-776
141,"{'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.5, '_step': 12, 'val_acc': 0.4046692607003891, '_timestamp': 1686346895.206428, 'train_loss': 0.6198889017105103, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 55}, '_runtime': 55.59052896499634, 'val_loss': 0.7678720951080322}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",driven-planet-775
142,"{'val_loss': 0.7564753890037537, 'train_loss': 0.6700079441070557, 'val_acc_rec': 0.4046692607003891, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 66}, 'val_acc_balanced': 0.5005498994469583, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.5, 'val_acc': 0.4059073222497347, '_runtime': 67.41303086280823, '_timestamp': 1686346750.495209}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",wise-dust-774
143,"{'_wandb': {'runtime': 62}, 'val_loss': 0.8111253380775452, 'train_loss': 0.6415841579437256, 'val_acc_balanced': 0.497746126422597, 'trainer/global_step': 169, '_step': 12, 'val_acc': 0.4101521047046338, '_runtime': 63.349915504455566, '_timestamp': 1686346595.2249694, 'val_acc_rec': 0.4085603112840467, 'val_acc_rec_balanced': 0.5001885369532428, 'epoch': 9}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",tough-snowflake-773
144,"{'_step': 12, '_wandb': {'runtime': 64}, 'val_acc': 0.5583657587548638, 'train_loss': 0.7053061127662659, 'val_acc_balanced': 0.49365973079208375, 'val_acc_rec_balanced': 0.49695198592257417, 'epoch': 9, '_runtime': 65.02522993087769, 'val_loss': 0.6874030232429504, '_timestamp': 1686346443.007102, 'val_acc_rec': 0.5642023346303502, 'trainer/global_step': 179}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",super-moon-772
145,"{'train_loss': 0.6961820721626282, 'val_acc_rec': 0.5953307392996109, 'trainer/global_step': 179, 'epoch': 9, '_wandb': {'runtime': 64}, '_timestamp': 1686346289.0609207, 'val_loss': 0.6813443899154663, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 12, 'val_acc': 0.5953307392996109, '_runtime': 65.3487536907196}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",splendid-galaxy-771
146,"{'val_acc_balanced': 0.5, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 66}, '_runtime': 67.27008867263794, 'val_loss': 0.7162318825721741, 'train_loss': 0.7158549427986145, 'val_acc': 0.4046692607003891, '_timestamp': 1686346134.3371947, 'val_acc_rec': 0.4046692607003891, 'trainer/global_step': 189, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",prime-glitter-770
147,"{'_step': 12, 'epoch': 9, '_wandb': {'runtime': 65}, 'val_loss': 0.72579026222229, 'val_acc_rec': 0.40077821011673154, 'val_acc': 0.4030774672798019, '_runtime': 65.50597834587097, '_timestamp': 1686345978.1400204, 'train_loss': 0.7090767025947571, 'val_acc_balanced': 0.4943938822615293, 'trainer/global_step': 189, 'val_acc_rec_balanced': 0.4936525892408245}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",grateful-valley-769
148,"{'val_acc_balanced': 0.4963720919603273, 'trainer/global_step': 159, 'val_acc_rec_balanced': 0.5035822021116139, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 66}, 'val_acc': 0.4713477184294305, 'val_loss': 0.7440477013587952, '_runtime': 67.09279084205627, '_timestamp': 1686345824.9852178, 'train_loss': 0.6904476881027222, 'val_acc_rec': 0.4785992217898833}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",ancient-gorge-768
149,"{'val_acc_balanced': 0.502895184880479, '_timestamp': 1686345669.3662758, 'train_loss': 0.7203519344329834, '_wandb': {'runtime': 65}, 'val_acc': 0.4571984435797665, '_runtime': 66.44059085845947, 'val_loss': 0.7456371784210205, 'val_acc_rec': 0.4396887159533074, 'trainer/global_step': 159, '_step': 12, 'epoch': 9, 'val_acc_rec_balanced': 0.4801407742584213}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",celestial-yogurt-767
150,"{'epoch': 9, '_wandb': {'runtime': 69}, 'val_loss': 0.6962387561798096, '_timestamp': 1686345513.4848802, 'train_loss': 0.6947289705276489, 'val_acc_rec': 0.4046692607003891, '_step': 16, 'val_acc': 0.4046692607003891, '_runtime': 69.62631011009216, 'val_acc_balanced': 0.5, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",astral-mountain-766
151,"{'val_acc': 0.4046692607003891, '_timestamp': 1686345353.7218666, 'val_acc_rec': 0.4046692607003891, 'val_acc_rec_balanced': 0.5, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 72}, '_runtime': 74.37959671020508, 'val_loss': 0.6962265372276306, 'train_loss': 0.6947227120399475, 'val_acc_balanced': 0.5, 'trainer/global_step': 369}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",blooming-pond-765
152,"{'val_loss': 0.6944171190261841, '_timestamp': 1686345187.7171254, 'train_loss': 0.6928676962852478, 'val_acc_rec': 0.4046692607003891, 'trainer/global_step': 369, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 70}, 'val_acc_rec_balanced': 0.5, 'val_acc': 0.4055535903784931, '_runtime': 70.92990946769714, 'val_acc_balanced': 0.5006727341286165}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",atomic-moon-764
153,"{'val_acc': 0.43226034665723384, 'val_loss': 0.7243919372558594, '_timestamp': 1686345016.655459, 'train_loss': 0.6847434639930725, 'trainer/global_step': 369, 'epoch': 9, '_wandb': {'runtime': 69}, '_runtime': 69.76636791229248, 'val_acc_rec': 0.4085603112840467, 'val_acc_balanced': 0.4871294963206728, 'val_acc_rec_balanced': 0.4693941679235797, '_step': 16}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",happy-snowball-763
154,"{'_wandb': {'runtime': 70}, 'val_acc': 0.4899186416696144, '_runtime': 70.6450252532959, '_timestamp': 1686344858.2609725, 'train_loss': 0.6921637654304504, 'val_acc_rec_balanced': 0.4918300653594771, '_step': 16, 'epoch': 9, 'val_loss': 0.730074942111969, 'val_acc_rec': 0.490272373540856, 'val_acc_balanced': 0.4976918506330271, 'trainer/global_step': 359}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",sage-terrain-762
155,"{'_timestamp': 1686344697.7146404, 'train_loss': 0.6962231993675232, 'val_acc': 0.5953307392996109, 'val_loss': 0.6752845048904419, '_wandb': {'runtime': 70}, '_runtime': 71.31752038002014, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, 'trainer/global_step': 359, 'val_acc_rec_balanced': 0.5, '_step': 16, 'epoch': 9}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",skilled-aardvark-761
156,"{'_runtime': 69.51979470252991, 'val_loss': 0.6962212920188904, '_timestamp': 1686344537.7203426, 'train_loss': 0.6927832365036011, 'val_acc_rec': 0.4046692607003891, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 69}, 'val_acc_rec_balanced': 0.5, 'val_acc': 0.4046692607003891, 'val_acc_balanced': 0.5, 'trainer/global_step': 389}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",fast-firebrand-760
157,"{'_step': 16, '_runtime': 74.53511667251587, 'val_loss': 0.7243576049804688, '_timestamp': 1686344376.3184886, 'val_acc_balanced': 0.48371154988802045, 'trainer/global_step': 389, 'val_acc_rec_balanced': 0.4855455002513826, 'epoch': 9, '_wandb': {'runtime': 74}, 'val_acc': 0.42094092677750267, 'train_loss': 0.6584628820419312, 'val_acc_rec': 0.42412451361867703}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",resilient-flower-759
158,"{'_step': 15, '_runtime': 69.36513566970825, '_timestamp': 1686344211.0527656, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.5262694821518351, 'val_acc_balanced': 0.5008369898075781, 'epoch': 9, '_wandb': {'runtime': 69}, 'val_acc': 0.4768305624336753, 'val_loss': 0.7204939126968384, 'train_loss': 0.6640230417251587, 'val_acc_rec': 0.5019455252918288}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",gallant-field-758
159,"{'val_acc_balanced': 0.49455099638923167, '_step': 15, 'epoch': 9, '_wandb': {'runtime': 69}, 'val_acc': 0.4640962150689777, 'train_loss': 0.6636353731155396, 'val_acc_rec': 0.4474708171206226, '_runtime': 69.55595970153809, 'val_loss': 0.7157989144325256, '_timestamp': 1686344051.6815577, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.4697398190045249}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",divine-donkey-757
160,"{'val_acc_rec_balanced': 0.5, '_step': 27, 'epoch': 9, 'val_acc': 0.5953307392996109, 'train_loss': 0.6925033926963806, 'val_acc_rec': 0.5953307392996109, 'trainer/global_step': 929, '_wandb': {'runtime': 80}, '_runtime': 80.85657501220703, 'val_loss': 0.6910520792007446, '_timestamp': 1686343891.813322, 'val_acc_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",skilled-serenity-756
161,"{'_timestamp': 1686343716.393281, 'val_acc_rec': 0.5953307392996109, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 81}, 'val_loss': 0.6910557746887207, 'val_acc': 0.5953307392996109, '_runtime': 81.59724402427673, 'train_loss': 0.6925045251846313, 'val_acc_balanced': 0.5, '_step': 27, 'epoch': 9}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",clean-serenity-755
162,"{'val_acc_balanced': 0.5, 'trainer/global_step': 939, 'val_acc_rec_balanced': 0.5, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 80}, 'val_acc': 0.5953307392996109, 'val_loss': 0.6835165619850159, '_runtime': 81.2453191280365, '_timestamp': 1686343541.459769, 'train_loss': 0.6911962032318115, 'val_acc_rec': 0.5953307392996109}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",balmy-field-754
163,"{'_timestamp': 1686343365.5376225, 'val_acc_balanced': 0.5, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 82}, 'val_acc': 0.5953307392996109, '_runtime': 82.44000148773193, 'val_loss': 0.6835199594497681, 'train_loss': 0.6911963820457458, 'val_acc_rec': 0.5953307392996109, 'trainer/global_step': 939, 'val_acc_rec_balanced': 0.5}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",hearty-fog-753
164,"{'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 81}, 'val_acc_balanced': 0.5, 'trainer/global_step': 909, '_runtime': 82.21240448951721, 'val_loss': 0.6903457641601562, '_timestamp': 1686343188.3557875, 'train_loss': 0.6908724904060364, 'val_acc_rec': 0.5953307392996109, '_step': 27, 'epoch': 9, 'val_acc': 0.5953307392996109}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",northern-glitter-752
165,"{'val_acc_rec': 0.5953307392996109, 'trainer/global_step': 909, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 81}, 'val_acc': 0.5953307392996109, '_runtime': 81.9853937625885, '_timestamp': 1686343012.801077, 'val_acc_rec_balanced': 0.5, 'val_loss': 0.6904339790344238, 'train_loss': 0.6910279989242554, 'val_acc_balanced': 0.5}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",stoic-cherry-751
166,"{'epoch': 9, '_runtime': 79.53594899177551, 'train_loss': 0.6937960982322693, 'val_acc_balanced': 0.5, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.5, '_step': 27, '_wandb': {'runtime': 79}, 'val_acc': 0.5953307392996109, 'val_loss': 0.6924993395805359, '_timestamp': 1686342837.270084, 'val_acc_rec': 0.5953307392996109}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",spring-dream-750
167,"{'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 82}, '_runtime': 82.72231364250183, '_timestamp': 1686342663.4975686, 'train_loss': 0.6937952041625977, 'val_acc_balanced': 0.5, 'trainer/global_step': 929, '_step': 27, 'epoch': 9, 'val_acc': 0.5953307392996109, 'val_loss': 0.692500114440918, 'val_acc_rec': 0.5953307392996109}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",rare-puddle-749
168,"{'train_loss': 0.6926931142807007, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, 'trainer/global_step': 889, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_runtime': 83.03492331504822, 'val_acc': 0.5953307392996109, 'val_loss': 0.6890295743942261, '_timestamp': 1686342486.8578422, '_step': 26, '_wandb': {'runtime': 82}}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",dutiful-bird-748
169,"{'_step': 26, 'epoch': 9, '_wandb': {'runtime': 79}, '_runtime': 79.92379927635193, 'val_acc_balanced': 0.5, 'trainer/global_step': 889, 'val_acc_rec_balanced': 0.5, 'val_acc': 0.5953307392996109, 'val_loss': 0.6890287399291992, '_timestamp': 1686342309.3525405, 'train_loss': 0.6926929950714111, 'val_acc_rec': 0.5953307392996109}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",true-cloud-747
170,"{'train_loss': 0.6928708553314209, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 101}, 'epoch': 9, 'val_acc': 0.5953307392996109, '_runtime': 101.13813185691832, 'val_loss': 0.6846030354499817, '_timestamp': 1686342133.7248778, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, '_step': 46}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",icy-dragon-746
171,"{'_wandb': {'runtime': 105}, 'val_acc': 0.5953307392996109, 'val_loss': 0.684602677822113, 'train_loss': 0.6928708553314209, 'val_acc_rec': 0.5953307392996109, 'val_acc_rec_balanced': 0.5, '_step': 46, 'epoch': 9, '_runtime': 106.1166844367981, '_timestamp': 1686341928.0300446, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",fanciful-pine-745
172,"{'_step': 46, 'val_acc': 0.5953307392996109, '_runtime': 101.3145236968994, 'epoch': 9, '_wandb': {'runtime': 101}, 'val_loss': 0.6849503517150879, '_timestamp': 1686341714.4327338, 'train_loss': 0.6959095597267151, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",spring-thunder-744
173,"{'val_acc': 0.5953307392996109, 'val_loss': 0.6849485039710999, 'train_loss': 0.6959100365638733, 'val_acc_rec_balanced': 0.5, '_timestamp': 1686341511.9119625, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 120}, '_runtime': 120.33045840263368}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",wandering-microwave-743
174,"{'epoch': 9, '_wandb': {'runtime': 96}, 'val_acc': 0.5953307392996109, '_runtime': 96.89924168586732, 'val_loss': 0.6842241883277893, '_timestamp': 1686341288.6774297, '_step': 46, 'train_loss': 0.6980517506599426, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",serene-microwave-742
175,"{'val_loss': 0.6842241883277893, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, '_step': 46, '_wandb': {'runtime': 98}, 'val_acc': 0.5953307392996109, '_runtime': 99.20503330230711, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_timestamp': 1686341089.2379854, 'train_loss': 0.6980518102645874, 'trainer/global_step': 1869}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",lilac-durian-741
176,"{'train_loss': 0.7013829350471497, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, '_step': 46, 'epoch': 9, '_timestamp': 1686340887.4763558, 'val_loss': 0.6842871904373169, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, '_wandb': {'runtime': 100}, 'val_acc': 0.5953307392996109, '_runtime': 100.29714775085448}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",youthful-snow-740
177,"{'_wandb': {'runtime': 101}, 'val_loss': 0.6920404434204102, '_timestamp': 1686340686.979869, 'train_loss': 0.7019032835960388, 'val_acc_balanced': 0.48493561177384703, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, 'val_acc': 0.5256455606650159, '_runtime': 102.25903058052064, 'val_acc_rec': 0.5097276264591439, 'val_acc_rec_balanced': 0.4650578179989945}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",sleek-feather-739
178,"{'_timestamp': 1686340481.6694484, 'train_loss': 0.6880384087562561, 'val_acc_rec_balanced': 0.5, '_step': 46, '_runtime': 102.68620538711548, 'val_acc': 0.5953307392996109, 'val_loss': 0.6846597790718079, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'epoch': 9, '_wandb': {'runtime': 102}}","{'weights': 'under_50_task', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",quiet-surf-738
179,"{'val_acc': 0.5953307392996109, 'val_loss': 0.6846596002578735, '_timestamp': 1686340277.471882, 'train_loss': 0.6880383491516113, 'val_acc_rec_balanced': 0.5, '_step': 46, '_wandb': {'runtime': 129}, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'epoch': 9, '_runtime': 129.58160090446472}","{'weights': 'pathological_task', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",dandy-sunset-737
180,"{'_step': 9, 'val_acc': 0.7509727626459144, 'val_acc_rec': 0.7859922178988327, 'trainer/global_step': 29, 'val_acc_balanced': 0.7523677110291284, 'val_acc_rec_balanced': 0.787007874015748, 'epoch': 9, '_wandb': {'runtime': 61}, '_runtime': 61.90894174575806, 'val_loss': 0.5110476613044739, '_timestamp': 1686340044.8285067}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",swept-firebrand-736
181,"{'_step': 9, '_runtime': 61.64871263504028, 'val_acc_balanced': 0.660446836627939, 'val_acc_rec': 0.6731517509727627, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.6705632949727438, 'epoch': 9, '_wandb': {'runtime': 61}, 'val_acc': 0.6627166607711354, 'val_loss': 0.9967283010482788, '_timestamp': 1686339895.2440577}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",dandy-sunset-735
182,"{'_step': 9, '_wandb': {'runtime': 59}, 'val_acc': 0.7209055535903784, '_runtime': 60.43436861038208, 'val_loss': 0.5655015707015991, '_timestamp': 1686339748.5457456, 'val_acc_rec': 0.7431906614785992, 'val_acc_balanced': 0.7203719508837619, 'epoch': 9, 'val_acc_rec_balanced': 0.7424288310115081, 'trainer/global_step': 29}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",happy-donkey-734
183,"{'val_loss': 0.8506593704223633, 'val_acc_rec_balanced': 0.5, 'val_acc': 0.5058365758754864, 'epoch': 9, '_wandb': {'runtime': 60}, '_runtime': 60.61267900466919, '_timestamp': 1686339601.388244, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, '_step': 9}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",fiery-silence-733
184,"{'epoch': 9, '_runtime': 62.060508012771606, 'val_acc_rec': 0.7782101167315175, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.7795881284070261, '_step': 9, 'val_acc': 0.7437212592854616, 'val_loss': 0.5349863171577454, '_timestamp': 1686339453.471459, 'val_acc_balanced': 0.7450594680909641, '_wandb': {'runtime': 61}}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",polished-valley-732
185,"{'epoch': 9, '_wandb': {'runtime': 60}, '_timestamp': 1686339304.7894747, 'val_acc_rec': 0.642023346303502, 'val_acc_balanced': 0.6348425196850394, 'trainer/global_step': 29, '_step': 9, 'val_acc': 0.6351255748142908, '_runtime': 61.56710362434387, 'val_loss': 0.6592987179756165, 'val_acc_rec_balanced': 0.641701998788613}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",frosty-night-731
186,"{'_step': 9, 'epoch': 9, '_wandb': {'runtime': 62}, 'val_loss': 0.5043350458145142, '_timestamp': 1686339155.9879155, 'val_acc_rec': 0.7587548638132295, 'val_acc_balanced': 0.7361433841748802, 'trainer/global_step': 29, 'val_acc': 0.7373540856031129, '_runtime': 62.840919494628906, 'val_acc_rec_balanced': 0.757359176256814}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",whole-dew-730
187,"{'_timestamp': 1686339007.0503025, 'val_acc_rec': 0.4980544747081712, 'epoch': 9, '_wandb': {'runtime': 61}, '_runtime': 62.4102623462677, 'val_loss': 0.8297707438468933, 'val_acc_rec_balanced': 0.4941247728649304, '_step': 9, 'val_acc': 0.4775380261761585, 'val_acc_balanced': 0.474728814492594, 'trainer/global_step': 29}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",royal-hill-729
188,"{'epoch': 9, '_runtime': 62.00947999954224, 'val_acc_rec': 0.7354085603112841, 'val_acc_balanced': 0.7177826110896977, 'val_acc_rec_balanced': 0.7330102967898243, '_step': 9, '_wandb': {'runtime': 61}, 'val_acc': 0.7198443579766537, 'val_loss': 0.5726413130760193, '_timestamp': 1686338857.491584, 'trainer/global_step': 29}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",atomic-night-728
189,"{'_step': 9, 'val_acc': 0.5058365758754864, '_timestamp': 1686338711.0571065, 'val_acc_rec': 0.5058365758754864, 'epoch': 9, '_wandb': {'runtime': 63}, '_runtime': 63.81333041191101, 'val_loss': 0.8684008717536926, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",vibrant-plasma-727
190,"{'trainer/global_step': 79, 'val_acc_rec_balanced': 0.7978195033313144, '_step': 10, 'epoch': 9, 'val_loss': 0.533151388168335, 'val_acc_rec': 0.7976653696498055, 'train_loss': 0.29856470227241516, 'val_acc_balanced': 0.7502849512692031, '_wandb': {'runtime': 63}, 'val_acc': 0.7497347010965688, '_runtime': 64.0569257736206, '_timestamp': 1686338560.9413898}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",youthful-snowball-726
191,"{'val_acc_balanced': 0.6537979736798634, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.6646880678376741, '_step': 10, 'epoch': 9, '_runtime': 64.84314012527466, 'train_loss': 0.39638519287109375, 'val_acc_rec': 0.6653696498054474, '_wandb': {'runtime': 64}, 'val_acc': 0.6542270958613371, 'val_loss': 0.794227123260498, '_timestamp': 1686338408.6987731}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",grateful-yogurt-725
192,"{'epoch': 9, 'val_acc': 0.7200212239122745, '_runtime': 63.029879331588745, 'val_loss': 0.5291116833686829, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.7493943064809206, '_step': 10, '_wandb': {'runtime': 62}, '_timestamp': 1686338255.481743, 'train_loss': 0.5241669416427612, 'val_acc_rec': 0.7509727626459144, 'val_acc_balanced': 0.7187007874015747}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",neat-glade-724
193,"{'epoch': 9, 'val_acc': 0.5058365758754864, '_runtime': 63.94478273391723, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 10, '_wandb': {'runtime': 63}, 'val_loss': 0.7449541687965393, '_timestamp': 1686338104.3054516, 'train_loss': 0.6172566413879395, 'val_acc_rec': 0.5058365758754864, 'trainer/global_step': 79}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",glad-river-723
194,"{'_wandb': {'runtime': 64}, 'val_acc': 0.7644145737530952, '_runtime': 64.58383631706238, '_timestamp': 1686337950.2694223, 'val_acc_balanced': 0.764799570508232, '_step': 10, 'epoch': 9, 'val_loss': 0.5350397229194641, 'train_loss': 0.5720837712287903, 'val_acc_rec': 0.7665369649805448, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.7670502725620836}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",proud-salad-722
195,"{'_step': 10, '_wandb': {'runtime': 63}, '_timestamp': 1686337797.539605, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.6475772259236826, 'epoch': 9, 'val_acc': 0.6489211177927131, '_runtime': 64.07644581794739, 'val_loss': 0.6524062156677246, 'train_loss': 0.6503543853759766, 'val_acc_rec': 0.6498054474708171, 'val_acc_balanced': 0.6467650459776444}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",revived-brook-721
196,"{'_step': 10, '_wandb': {'runtime': 65}, 'val_acc_rec': 0.7782101167315175, 'val_acc_rec_balanced': 0.7773167777104785, '_timestamp': 1686337645.453344, 'train_loss': 0.4329303205013275, 'val_acc_balanced': 0.762364407246297, 'trainer/global_step': 79, 'epoch': 9, 'val_acc': 0.7629996462681288, '_runtime': 65.42782402038574, 'val_loss': 0.5052130818367004}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",devout-brook-720
197,"{'epoch': 9, '_wandb': {'runtime': 64}, 'val_acc': 0.5021223912274496, 'val_loss': 0.7170484066009521, 'train_loss': 0.6399392485618591, 'trainer/global_step': 79, '_step': 10, '_runtime': 64.71502447128296, '_timestamp': 1686337490.5122526, 'val_acc_rec': 0.5175097276264592, 'val_acc_balanced': 0.5004790485105446, 'val_acc_rec_balanced': 0.51480920654149}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",misunderstood-wildflower-719
198,"{'val_acc_rec_balanced': 0.7142337976983646, 'epoch': 9, '_wandb': {'runtime': 62}, 'val_acc': 0.7175451008135834, 'val_acc_balanced': 0.7160632674412202, 'trainer/global_step': 79, 'val_acc_rec': 0.7159533073929961, '_step': 10, '_runtime': 63.318633794784546, 'val_loss': 0.5338866710662842, '_timestamp': 1686337338.1071749, 'train_loss': 0.5929772853851318}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",swift-disco-718
199,"{'epoch': 9, '_runtime': 64.02215671539307, 'train_loss': 0.5720462799072266, 'val_acc_balanced': 0.5, 'trainer/global_step': 79, 'val_acc_rec': 0.5058365758754864, 'val_acc_rec_balanced': 0.5, '_step': 10, '_wandb': {'runtime': 63}, 'val_acc': 0.5058365758754864, 'val_loss': 0.8957719802856445, '_timestamp': 1686337187.7684698}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",devoted-field-717
200,"{'_timestamp': 1686337037.235517, 'val_acc_rec': 0.754863813229572, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.7539672925499697, 'epoch': 9, 'val_loss': 0.5338279008865356, 'val_acc': 0.7430137955429784, '_runtime': 67.82539200782776, 'train_loss': 0.46161317825317383, 'val_acc_balanced': 0.7423448598645449, '_step': 12, '_wandb': {'runtime': 67}}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",comfy-moon-716
201,"{'_timestamp': 1686336878.654979, 'train_loss': 0.5208589434623718, 'val_acc_rec': 0.6498054474708171, 'val_acc_rec_balanced': 0.6483949121744397, 'val_loss': 0.6402461528778076, 'epoch': 9, '_wandb': {'runtime': 59}, 'val_acc': 0.6551114255394411, '_runtime': 60.515464067459106, 'val_acc_balanced': 0.6539906943450251, 'trainer/global_step': 169, '_step': 12}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",trim-water-715
202,"{'_runtime': 63.98657131195069, 'val_acc_rec': 0.7937743190661478, 'trainer/global_step': 169, '_step': 12, 'val_acc': 0.7516802263883976, 'val_loss': 0.5277271866798401, '_timestamp': 1686336729.5773425, 'train_loss': 0.3955129384994507, 'val_acc_balanced': 0.7519189471945378, 'val_acc_rec_balanced': 0.7936099333737129, 'epoch': 9, '_wandb': {'runtime': 63}}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",silver-brook-714
203,"{'val_acc': 0.6335337813937035, 'val_loss': 0.6458050012588501, '_timestamp': 1686336578.3023489, 'train_loss': 0.6448018550872803, 'val_acc_rec': 0.6381322957198443, 'val_acc_balanced': 0.6328189527008425, 'val_acc_rec_balanced': 0.6371290127195639, '_wandb': {'runtime': 66}, 'epoch': 9, '_runtime': 66.47294473648071, 'trainer/global_step': 169, '_step': 12}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",valiant-paper-713
204,"{'val_loss': 0.5522193908691406, 'train_loss': 0.6671543717384338, 'val_acc_rec': 0.7587548638132295, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 66}, 'val_acc': 0.7304563141139017, '_runtime': 66.86612367630005, '_timestamp': 1686336423.3614638, 'val_acc_balanced': 0.7316584989813336, 'trainer/global_step': 179, 'val_acc_rec_balanced': 0.7599030890369474}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",magic-durian-712
205,"{'val_acc_rec_balanced': 0.6321926105390673, '_step': 12, 'epoch': 9, 'val_acc': 0.6347718429430491, 'val_loss': 0.6504883766174316, 'val_acc_rec': 0.6342412451361867, 'trainer/global_step': 179, '_wandb': {'runtime': 67}, '_runtime': 67.73565936088562, '_timestamp': 1686336267.9324114, 'train_loss': 0.6939886808395386, 'val_acc_balanced': 0.632700567149386}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",vocal-donkey-711
206,"{'_runtime': 65.2459499835968, 'val_loss': 0.5308688282966614, 'val_acc_rec': 0.7782101167315175, '_step': 12, '_wandb': {'runtime': 64}, 'val_acc': 0.7564556066501592, 'val_acc_balanced': 0.7559660811629316, 'trainer/global_step': 189, 'val_acc_rec_balanced': 0.7775893397940643, 'epoch': 9, '_timestamp': 1686336111.299671, 'train_loss': 0.45870956778526306}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",stellar-thunder-710
207,"{'_runtime': 66.85311388969421, '_timestamp': 1686335956.858943, 'val_acc_rec_balanced': 0.6470926711084191, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 66}, 'val_acc_rec': 0.6459143968871596, 'val_acc_balanced': 0.6368495677550796, 'trainer/global_step': 189, 'val_acc': 0.6356561726211531, 'val_loss': 0.6438935399055481, 'train_loss': 0.6098514199256897}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",light-haze-709
208,"{'_wandb': {'runtime': 68}, '_runtime': 68.78126358985901, '_timestamp': 1686335798.8486867, 'val_acc_rec_balanced': 0.765778316172017, '_step': 12, 'epoch': 9, 'train_loss': 0.385416567325592, 'val_acc_rec': 0.7665369649805448, 'val_acc_balanced': 0.7462006497439568, 'trainer/global_step': 159, 'val_acc': 0.7470817120622568, 'val_loss': 0.5143319964408875}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",radiant-jazz-708
209,"{'val_acc_balanced': 0.64894554264633, 'epoch': 9, '_wandb': {'runtime': 67}, '_runtime': 67.63884925842285, 'val_loss': 0.6412767767906189, 'train_loss': 0.5483729839324951, 'val_acc_rec': 0.6614785992217899, '_step': 12, 'val_acc': 0.6510435090201627, '_timestamp': 1686335641.5900412, 'trainer/global_step': 159, 'val_acc_rec_balanced': 0.6593882495457298}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",earthy-sun-707
210,"{'_runtime': 70.00634384155273, 'val_loss': 0.5029436945915222, '_timestamp': 1686335486.603514, 'train_loss': 0.42515283823013306, 'val_acc_balanced': 0.7658251197621276, 'val_acc_rec_balanced': 0.7858267716535433, '_step': 16, '_wandb': {'runtime': 69}, 'val_acc_rec': 0.7859922178988327, 'trainer/global_step': 369, 'epoch': 9, 'val_acc': 0.7656526353024408}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",curious-yogurt-706
211,"{'_wandb': {'runtime': 72}, 'val_loss': 0.6420534253120422, 'trainer/global_step': 369, '_runtime': 72.74319553375244, '_timestamp': 1686335326.4866185, 'train_loss': 0.5825648903846741, 'val_acc_rec': 0.6536964980544747, 'val_acc_balanced': 0.6574651726226529, '_step': 16, 'epoch': 9, 'val_acc': 0.6590024761230987, 'val_acc_rec_balanced': 0.651877649909146}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",fancy-vortex-705
212,"{'_step': 16, 'val_acc': 0.754863813229572, 'val_loss': 0.5028201937675476, '_timestamp': 1686335165.0098035, 'train_loss': 0.408692866563797, 'trainer/global_step': 369, 'epoch': 9, '_wandb': {'runtime': 67}, '_runtime': 68.1121735572815, 'val_acc_rec': 0.7821011673151751, 'val_acc_balanced': 0.75406640603491, 'val_acc_rec_balanced': 0.7811629315566324}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",eternal-lake-704
213,"{'epoch': 9, '_wandb': {'runtime': 69}, 'val_acc': 0.6471524584365052, 'val_loss': 0.6702688932418823, 'val_acc_balanced': 0.6444923187049172, 'trainer/global_step': 369, '_step': 16, '_timestamp': 1686335007.6142795, 'train_loss': 0.612887442111969, 'val_acc_rec': 0.6459143968871596, 'val_acc_rec_balanced': 0.6428225317989098, '_runtime': 69.36081886291504}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",neat-snow-703
214,"{'train_loss': 0.4074113368988037, 'val_acc_balanced': 0.7435273388029293, '_wandb': {'runtime': 71}, 'val_acc': 0.7447824548991864, '_runtime': 71.8589780330658, 'val_loss': 0.5175250768661499, 'trainer/global_step': 359, 'val_acc_rec_balanced': 0.7496668685645064, '_step': 16, 'epoch': 9, '_timestamp': 1686334848.565096, 'val_acc_rec': 0.7509727626459144}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",woven-mountain-702
215,"{'val_acc_rec': 0.6653696498054474, 'trainer/global_step': 359, '_step': 16, 'epoch': 9, 'val_acc': 0.657941280509374, '_runtime': 73.55907249450684, 'val_acc_balanced': 0.6561519189471945, 'val_acc_rec_balanced': 0.6634161114476076, '_wandb': {'runtime': 73}, 'val_loss': 0.6584315299987793, '_timestamp': 1686334687.2747984, 'train_loss': 0.4216241240501404}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",deep-field-701
216,"{'val_acc_balanced': 0.7607662023016353, '_step': 16, '_wandb': {'runtime': 70}, 'val_acc': 0.7608772550406792, '_runtime': 71.15384936332703, 'val_loss': 0.5036015510559082, 'train_loss': 0.5573301315307617, 'val_acc_rec': 0.7859922178988327, 'trainer/global_step': 389, 'epoch': 9, '_timestamp': 1686334523.3754733, 'val_acc_rec_balanced': 0.7857359176256814}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",dry-night-700
217,"{'_wandb': {'runtime': 71}, 'val_acc': 0.6641315882561019, '_runtime': 72.24572706222534, 'val_acc_rec': 0.6653696498054474, 'val_acc_balanced': 0.6640465833379219, 'trainer/global_step': 389, '_step': 16, 'epoch': 9, 'val_acc_rec_balanced': 0.6654149000605694, 'train_loss': 0.6376100778579712, 'val_loss': 0.6202359199523926, '_timestamp': 1686334362.119162}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",iconic-forest-699
218,"{'val_loss': 0.5329262018203735, 'val_acc_rec': 0.7392996108949417, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.7373107207752877, '_wandb': {'runtime': 70}, '_runtime': 70.80798172950745, 'val_acc': 0.7285107888220729, '_timestamp': 1686334199.1017938, 'train_loss': 0.39679035544395447, 'val_acc_balanced': 0.7267372391388139, '_step': 15, 'epoch': 9}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",dauntless-wind-698
219,"{'_wandb': {'runtime': 70}, 'val_acc': 0.6443226034665723, 'val_acc_balanced': 0.6412408457684048, '_step': 15, 'epoch': 9, '_runtime': 70.7182023525238, 'val_loss': 0.6456905007362366, '_timestamp': 1686334038.023039, 'train_loss': 0.5768676400184631, 'val_acc_rec': 0.6614785992217899, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.658388855239249}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",giddy-wood-697
220,"{'val_acc': 0.5565970993986559, '_runtime': 85.57511520385742, '_timestamp': 1686333876.0655632, 'trainer/global_step': 929, '_step': 27, '_wandb': {'runtime': 85}, 'train_loss': 0.5019149780273438, 'val_acc_rec': 0.5408560311284046, 'val_acc_balanced': 0.5616100434998073, 'val_acc_rec_balanced': 0.5461538461538462, 'epoch': 9, 'val_loss': 0.7076901197433472}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",breezy-elevator-696
221,"{'_timestamp': 1686333693.0560875, 'val_acc_balanced': 0.6561519189471945, 'val_acc_rec_balanced': 0.6556329497274379, 'epoch': 9, '_wandb': {'runtime': 84}, '_runtime': 85.18620038032532, 'val_loss': 0.6407022476196289, 'trainer/global_step': 929, '_step': 27, 'val_acc': 0.657941280509374, 'train_loss': 0.6176539659500122, 'val_acc_rec': 0.6575875486381323}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",charmed-valley-695
222,"{'_wandb': {'runtime': 83}, '_runtime': 83.54575872421265, 'val_loss': 0.48503655195236206, 'val_acc_rec': 0.7976653696498055, 'trainer/global_step': 939, 'val_acc_rec_balanced': 0.7973652331920048, '_step': 27, 'epoch': 9, 'val_acc': 0.7679518924655111, '_timestamp': 1686333512.5364716, 'train_loss': 0.4801460206508637, 'val_acc_balanced': 0.7678376741368867}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",playful-brook-694
223,"{'val_acc_rec': 0.6809338521400778, 'val_acc_balanced': 0.66490969660261, 'trainer/global_step': 939, '_step': 27, 'val_acc': 0.6667845772904138, 'val_loss': 0.6339604258537292, 'train_loss': 0.6935850381851196, 'val_acc_rec_balanced': 0.6788007268322229, 'epoch': 9, '_wandb': {'runtime': 83}, '_runtime': 83.92327499389648, '_timestamp': 1686333335.54042}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",floral-salad-693
224,"{'_wandb': {'runtime': 81}, 'val_loss': 0.4974636733531952, 'train_loss': 0.5493320822715759, 'val_acc_balanced': 0.7599292439843621, 'epoch': 9, 'val_acc': 0.7605235231694376, '_runtime': 82.16866326332092, '_timestamp': 1686333157.6268492, 'val_acc_rec': 0.7898832684824902, 'trainer/global_step': 909, 'val_acc_rec_balanced': 0.7892186553603877, '_step': 27}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",iconic-surf-692
225,"{'_timestamp': 1686332980.3846557, 'val_acc_rec': 0.6848249027237354, 'trainer/global_step': 909, 'val_acc_rec_balanced': 0.6830102967898244, '_step': 27, 'epoch': 9, '_runtime': 81.36159873008728, 'train_loss': 0.6413835287094116, 'val_acc_balanced': 0.6647403777325037, '_wandb': {'runtime': 81}, 'val_acc': 0.6662539794835515, 'val_loss': 0.6207729578018188}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",light-dream-691
226,"{'_step': 27, '_wandb': {'runtime': 83}, '_runtime': 84.16064405441284, 'val_loss': 0.4928838610649109, '_timestamp': 1686332806.534836, 'val_acc_rec': 0.7898832684824902, 'epoch': 9, 'val_acc': 0.7667138309161655, 'train_loss': 0.5093138813972473, 'val_acc_balanced': 0.7671507626232036, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.789763779527559}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",skilled-wind-690
227,"{'train_loss': 0.6492380499839783, 'val_acc_balanced': 0.6731746599856836, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.6865838885523925, '_step': 27, 'val_acc': 0.6749204103289707, 'val_loss': 0.619986355304718, '_timestamp': 1686332627.0236557, 'val_acc_rec': 0.688715953307393, 'epoch': 9, '_wandb': {'runtime': 82}, '_runtime': 82.91511654853821}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",radiant-water-689
228,"{'_wandb': {'runtime': 80}, 'val_acc': 0.7603466572338168, 'val_loss': 0.4969499707221985, 'train_loss': 0.485533595085144, 'val_acc_balanced': 0.7598824403942515, 'trainer/global_step': 889, '_step': 26, 'epoch': 9, '_runtime': 80.89080739021301, '_timestamp': 1686332449.6436894, 'val_acc_rec': 0.7937743190661478, 'val_acc_rec_balanced': 0.7931556632344035}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",curious-firefly-688
229,"{'_timestamp': 1686332272.8834198, 'train_loss': 0.6107965111732483, 'trainer/global_step': 889, 'epoch': 9, 'val_acc': 0.6600636717368235, '_runtime': 84.58852863311768, 'val_loss': 0.640635073184967, 'val_acc_rec': 0.6653696498054474, 'val_acc_balanced': 0.6580226859754419, 'val_acc_rec_balanced': 0.6631435493640218, '_step': 26, '_wandb': {'runtime': 84}}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",glorious-armadillo-687
230,"{'_wandb': {'runtime': 100}, 'val_acc': 0.661301733286169, '_runtime': 100.53844666481018, 'val_acc_rec': 0.6653696498054474, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.6691399152029074, '_step': 46, 'epoch': 9, 'train_loss': 0.5254166126251221, 'val_acc_balanced': 0.6647679092560982, 'val_loss': 0.6164863705635071, '_timestamp': 1686332092.3261747}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",gallant-frog-686
231,"{'val_acc_balanced': 0.6482696437420847, 'val_acc_rec_balanced': 0.6420654149000605, 'epoch': 9, 'val_acc': 0.6480367881146091, '_runtime': 102.61386013031006, 'val_loss': 0.6242913007736206, '_timestamp': 1686331888.2909012, 'train_loss': 0.5461522340774536, '_step': 46, '_wandb': {'runtime': 102}, 'val_acc_rec': 0.642023346303502, 'trainer/global_step': 1869}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",zesty-frog-685
232,"{'_wandb': {'runtime': 102}, '_runtime': 102.55487585067748, 'train_loss': 0.5418298840522766, 'val_acc_rec': 0.6809338521400778, 'val_acc_rec_balanced': 0.684251968503937, '_step': 46, 'epoch': 9, '_timestamp': 1686331683.7169747, 'val_acc_balanced': 0.6739978525411596, 'trainer/global_step': 1869, 'val_acc': 0.6708524938096923, 'val_loss': 0.5819441080093384}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",celestial-darkness-684
233,"{'val_acc': 0.667492041032897, 'val_loss': 0.6147745251655579, '_timestamp': 1686331477.8125093, 'train_loss': 0.665459394454956, 'val_acc_rec': 0.669260700389105, 'val_acc_balanced': 0.6670296239193877, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 107}, '_runtime': 107.5338032245636, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.6688976377952756}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",rich-snowflake-683
234,"{'_step': 46, 'epoch': 9, 'val_loss': 0.6820902824401855, '_timestamp': 1686331264.912635, '_wandb': {'runtime': 106}, 'val_acc': 0.5778210116731517, '_runtime': 106.87789821624756, 'train_loss': 0.425626665353775, 'val_acc_rec': 0.5642023346303502, 'val_acc_balanced': 0.5825436374648973, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5692307692307692}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",solar-morning-682
235,"{'_wandb': {'runtime': 101}, 'val_acc': 0.6651927838698267, '_runtime': 101.91631531715392, 'val_loss': 0.6149430871009827, 'val_acc_rec': 0.6614785992217899, 'trainer/global_step': 1869, '_step': 46, '_timestamp': 1686331058.2238674, 'train_loss': 0.570562481880188, 'val_acc_balanced': 0.6646577831617202, 'val_acc_rec_balanced': 0.6608419139915203, 'epoch': 9}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",genial-silence-681
236,"{'_wandb': {'runtime': 101}, 'val_acc': 0.6747435443933498, '_runtime': 101.82060837745668, 'val_acc_rec': 0.688715953307393, 'trainer/global_step': 1869, 'epoch': 9, 'val_loss': 0.5880245566368103, '_timestamp': 1686330854.5821884, 'train_loss': 0.4461153745651245, 'val_acc_balanced': 0.6778935631297835, 'val_acc_rec_balanced': 0.6919442761962447, '_step': 46}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",devout-wind-680
237,"{'trainer/global_step': 1869, 'epoch': 9, 'val_acc': 0.666607711354793, '_runtime': 102.63765573501588, 'train_loss': 0.6301706433296204, 'val_acc_rec': 0.6770428015564203, 'val_acc_rec_balanced': 0.6756814052089642, '_step': 46, '_wandb': {'runtime': 102}, 'val_loss': 0.6167057156562805, '_timestamp': 1686330650.0232086, 'val_acc_balanced': 0.665531909035846}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",vivid-eon-679
238,"{'val_acc': 0.554651574106827, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, 'val_loss': 0.7044222950935364, '_timestamp': 1686330441.024021, 'train_loss': 0.6317043900489807, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5596993557623479, 'val_acc_rec_balanced': 0.5423076923076923, '_wandb': {'runtime': 98}, '_runtime': 98.9303297996521}","{'weights': 'under_50_task', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",curious-hill-678
239,"{'_step': 46, 'epoch': 9, '_wandb': {'runtime': 3124}, '_runtime': 3124.556868314743, 'val_acc_rec': 0.6731517509727627, 'val_acc_rec_balanced': 0.6715626892792248, 'val_acc': 0.6713830916165546, 'val_loss': 0.6183336973190308, '_timestamp': 1686330233.1409214, 'train_loss': 0.6465255618095398, 'val_acc_balanced': 0.670025053686471, 'trainer/global_step': 1869}","{'weights': 'pathological_task', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",hardy-resonance-677
240,"{'val_loss': 0.958900272846222, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 59}, 'val_acc': 0.490272373540856, '_runtime': 60.11674737930298, '_timestamp': 1686327008.981151, 'val_acc_rec': 0.490272373540856, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, '_step': 9}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",smart-lake-676
241,"{'_wandb': {'runtime': 60}, 'val_loss': 1.235896110534668, '_timestamp': 1686326859.3068664, 'trainer/global_step': 29, '_step': 9, 'epoch': 9, 'val_acc': 0.490272373540856, '_runtime': 60.943886518478394, 'val_acc_rec': 0.490272373540856, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",royal-pyramid-675
242,"{'val_acc_balanced': 0.529567760483791, 'val_acc_rec_balanced': 0.5300193868896159, '_step': 9, '_wandb': {'runtime': 59}, '_runtime': 59.93473768234253, 'val_loss': 0.6961946487426758, '_timestamp': 1686326710.9141915, 'epoch': 9, 'val_acc': 0.5325433321542271, 'val_acc_rec': 0.5330739299610895, 'trainer/global_step': 29}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",glamorous-shadow-674
243,"{'trainer/global_step': 29, 'val_acc_rec_balanced': 0.518114624984854, '_step': 9, '_wandb': {'runtime': 61}, 'val_loss': 0.7830063700675964, '_timestamp': 1686326564.8977902, 'val_acc_rec': 0.5214007782101168, 'val_acc_balanced': 0.4940173270325942, 'epoch': 9, 'val_acc': 0.497347010965688, '_runtime': 62.01302623748779}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",rose-planet-673
244,"{'trainer/global_step': 29, 'epoch': 9, '_wandb': {'runtime': 60}, 'val_loss': 0.9409158825874328, '_timestamp': 1686326415.0078945, 'val_acc_balanced': 0.5, '_step': 9, 'val_acc': 0.5097276264591439, '_runtime': 61.29337430000305, 'val_acc_rec': 0.5097276264591439, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",misunderstood-monkey-672
245,"{'val_acc_balanced': 0.46982639921571223, 'val_acc_rec_balanced': 0.4704955773658064, '_step': 9, 'epoch': 9, 'val_acc': 0.4736469755925009, '_runtime': 61.068928480148315, '_timestamp': 1686326265.5490255, 'val_acc_rec': 0.47470817120622566, '_wandb': {'runtime': 60}, 'val_loss': 1.405280351638794, 'trainer/global_step': 29}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",effortless-snowflake-671
246,"{'val_acc_rec_balanced': 0.5, '_step': 9, 'epoch': 9, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, '_timestamp': 1686326116.9760878, 'val_acc_rec': 0.5097276264591439, '_wandb': {'runtime': 59}, 'val_acc': 0.5097276264591439, '_runtime': 60.15009689331055, 'val_loss': 2.2500627040863037}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",driven-violet-670
247,"{'_step': 9, 'val_acc': 0.5127343473646976, '_runtime': 60.99738049507141, '_timestamp': 1686325968.1275604, 'val_acc_rec_balanced': 0.4925178722888646, 'epoch': 9, '_wandb': {'runtime': 60}, 'val_loss': 2.2828989028930664, 'val_acc_rec': 0.5019455252918288, 'val_acc_balanced': 0.5036722183668748, 'trainer/global_step': 29}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",leafy-galaxy-669
248,"{'val_loss': 0.8010715246200562, 'val_acc_rec': 0.4941634241245136, 'trainer/global_step': 29, '_step': 9, '_wandb': {'runtime': 58}, 'val_acc': 0.4702865228157057, '_runtime': 58.85467481613159, 'epoch': 9, '_timestamp': 1686325816.956982, 'val_acc_balanced': 0.4656282013152242, 'val_acc_rec_balanced': 0.4876105658548407}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",laced-galaxy-668
249,"{'val_acc_balanced': 0.5658300562880716, 'trainer/global_step': 29, 'epoch': 9, '_wandb': {'runtime': 60}, '_runtime': 60.830815076828, 'val_loss': 0.7420453429222107, '_timestamp': 1686325670.70761, '_step': 9, 'val_acc': 0.5654403961796958, 'val_acc_rec': 0.5875486381322957, 'val_acc_rec_balanced': 0.5876953834969102}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",generous-frog-667
250,"{'train_loss': 0.5881140828132629, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 62}, 'val_acc': 0.490272373540856, 'val_loss': 0.7700031995773315, '_timestamp': 1686325520.746056, '_step': 10, '_runtime': 62.93242406845093, 'val_acc_rec': 0.490272373540856, 'val_acc_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",skilled-dream-666
251,"{'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5671876893250939, 'epoch': 9, 'val_acc': 0.5373187124159887, '_runtime': 63.856468200683594, 'val_loss': 0.8090144991874695, 'val_acc_rec': 0.5603112840466926, 'val_acc_balanced': 0.5436838394853662, '_step': 10, '_wandb': {'runtime': 63}, '_timestamp': 1686325368.447945, 'train_loss': 0.5055592060089111}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",dark-oath-665
252,"{'_step': 10, '_wandb': {'runtime': 59}, 'val_acc': 0.5067209055535904, 'val_acc_rec': 0.5214007782101168, 'val_acc_balanced': 0.5035083661037859, 'val_acc_rec_balanced': 0.5175087846843571, 'epoch': 9, '_runtime': 60.08869814872742, 'val_loss': 0.7394588589668274, '_timestamp': 1686325217.9091392, 'train_loss': 0.7278701663017273, 'trainer/global_step': 79}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",vivid-tree-664
253,"{'val_loss': 0.71401047706604, 'val_acc_balanced': 0.5130283202802287, 'val_acc_rec_balanced': 0.5170544044589847, '_step': 10, 'epoch': 9, 'val_acc': 0.5171559957552175, '_runtime': 61.68973898887634, 'trainer/global_step': 79, '_wandb': {'runtime': 61}, '_timestamp': 1686325070.497487, 'train_loss': 0.5800461173057556, 'val_acc_rec': 0.5214007782101168}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",trim-bird-663
254,"{'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 10, '_wandb': {'runtime': 175}, '_runtime': 175.91402888298035, '_timestamp': 1686324919.42163, 'train_loss': 0.6049458980560303, 'val_acc_rec': 0.5097276264591439, 'epoch': 9, 'val_acc': 0.5097276264591439, 'val_loss': 0.8525642156600952, 'trainer/global_step': 79}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",azure-microwave-662
255,"{'val_acc_rec_balanced': 0.4932751726644856, 'epoch': 9, 'val_acc': 0.503537318712416, '_timestamp': 1686324656.9641893, 'trainer/global_step': 79, 'train_loss': 0.5898477435112, 'val_acc_rec': 0.5019455252918288, 'val_acc_balanced': 0.4950844321073329, '_step': 10, '_wandb': {'runtime': 59}, '_runtime': 60.25747632980347, 'val_loss': 0.8033940196037292}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",warm-violet-661
256,"{'val_loss': 0.8318338990211487, 'val_acc_rec': 0.5097276264591439, 'val_acc_rec_balanced': 0.5, '_step': 10, 'val_acc': 0.5097276264591439, '_runtime': 54.00721979141235, '_timestamp': 1686324509.3970127, 'train_loss': 0.6066669225692749, 'val_acc_balanced': 0.5, 'trainer/global_step': 79, 'epoch': 9, '_wandb': {'runtime': 53}}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",expert-shadow-660
257,"{'trainer/global_step': 79, 'val_acc_rec_balanced': 0.4961832061068702, '_step': 10, 'epoch': 9, '_timestamp': 1686324367.261474, 'train_loss': 0.5917330980300903, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5007256314508223, '_wandb': {'runtime': 61}, 'val_acc': 0.5100813583303856, '_runtime': 61.753296852111816, 'val_loss': 0.981001317501068}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",worthy-microwave-659
258,"{'val_acc': 0.5097276264591439, 'val_loss': 0.6947228908538818, 'train_loss': 0.6753361225128174, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 49}, '_runtime': 50.32976222038269, '_timestamp': 1686324217.6170485, 'trainer/global_step': 79, '_step': 10}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",fiery-serenity-658
259,"{'val_acc_rec_balanced': 0.5837271295286562, 'val_acc': 0.5527060488149982, '_timestamp': 1686324080.366644, 'train_loss': 0.6035945415496826, 'val_acc_balanced': 0.5529945033761827, 'val_loss': 0.7203390598297119, 'val_acc_rec': 0.5836575875486382, 'trainer/global_step': 79, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 61}, '_runtime': 61.69920802116394}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",cool-haze-657
260,"{'_timestamp': 1686323929.5119286, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.5664303889494728, 'epoch': 9, '_wandb': {'runtime': 67}, '_runtime': 67.69471859931946, 'val_loss': 0.7130988240242004, 'val_acc_balanced': 0.5636201161010321, '_step': 12, 'val_acc': 0.5583657587548638, 'train_loss': 0.5164047479629517, 'val_acc_rec': 0.5603112840466926}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",absurd-vortex-656
261,"{'val_acc': 0.5753448885744605, '_runtime': 66.52954483032227, 'val_loss': 0.7091779708862305, '_timestamp': 1686323774.456431, 'train_loss': 0.5850632786750793, 'val_acc_balanced': 0.5776797418018792, 'epoch': 9, '_wandb': {'runtime': 65}, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.5707318550830002, '_step': 12, 'val_acc_rec': 0.5680933852140078}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",lunar-totem-655
262,"{'val_acc_balanced': 0.5666754788892193, 'val_acc_rec_balanced': 0.565279292378529, '_step': 12, '_wandb': {'runtime': 64}, 'val_acc': 0.568800848956491, '_timestamp': 1686323618.5098386, 'val_acc_rec': 0.5680933852140078, 'epoch': 9, '_runtime': 64.79362463951111, 'val_loss': 0.6957792639732361, 'train_loss': 0.6482470631599426, 'trainer/global_step': 169}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",drawn-puddle-654
263,"{'_wandb': {'runtime': 64}, 'train_loss': 0.6583173274993896, 'val_acc_rec': 0.490272373540856, 'val_acc_balanced': 0.4960455151294846, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.4848539924875803, '_step': 12, 'epoch': 9, 'val_acc': 0.5014149274849664, '_runtime': 65.37823295593262, 'val_loss': 0.7063683271408081, '_timestamp': 1686323464.4910378}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",amber-wind-653
264,"{'epoch': 9, '_wandb': {'runtime': 65}, '_timestamp': 1686323309.0009432, 'train_loss': 0.6713509559631348, 'val_acc_balanced': 0.5, 'trainer/global_step': 179, '_step': 12, 'val_acc': 0.5097276264591439, '_runtime': 65.7497501373291, 'val_loss': 0.6972585320472717, 'val_acc_rec': 0.5097276264591439, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",twilight-universe-652
265,"{'epoch': 9, '_wandb': {'runtime': 66}, '_runtime': 66.65868759155273, '_timestamp': 1686323153.1549315, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, 'trainer/global_step': 179, '_step': 12, 'val_acc': 0.5097276264591439, 'val_loss': 0.6998523473739624, 'train_loss': 0.668192982673645, 'val_acc_rec_balanced': 0.5}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",peach-river-651
266,"{'val_acc_rec': 0.5097276264591439, 'trainer/global_step': 189, 'epoch': 9, 'val_acc': 0.5097276264591439, '_runtime': 64.61078810691833, 'val_loss': 0.7410964369773865, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 12, '_wandb': {'runtime': 64}, '_timestamp': 1686322996.060019, 'train_loss': 0.574063777923584}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",fiery-capybara-650
267,"{'val_loss': 0.8203235268592834, 'val_acc_balanced': 0.4896249848539925, 'trainer/global_step': 189, '_step': 12, '_wandb': {'runtime': 63}, '_runtime': 63.85823607444763, '_timestamp': 1686322844.450298, 'train_loss': 0.5893941521644592, 'val_acc_rec': 0.5058365758754864, 'val_acc_rec_balanced': 0.4987580273839816, 'epoch': 9, 'val_acc': 0.496462681287584}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",faithful-rain-649
268,"{'val_acc_balanced': 0.5838441668594341, 'trainer/global_step': 159, 'val_acc': 0.5863105765829502, '_runtime': 64.88307523727417, 'val_loss': 0.690962553024292, '_timestamp': 1686322690.402069, 'val_acc_rec': 0.5992217898832685, 'val_acc_rec_balanced': 0.5958136435235671, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 64}, 'train_loss': 0.6304301023483276}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",cosmic-music-648
269,"{'_runtime': 49.93242621421814, 'train_loss': 0.567561149597168, 'val_acc_rec': 0.5525291828793775, 'val_acc_balanced': 0.5591313902382604, 'trainer/global_step': 159, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 49}, 'val_acc_rec_balanced': 0.547740215679147, 'val_acc': 0.5636717368234878, 'val_loss': 0.7488542795181274, '_timestamp': 1686322536.8296533}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",fancy-resonance-647
270,"{'_timestamp': 1686322400.159082, 'train_loss': 0.6872332692146301, 'val_acc_rec': 0.5369649805447471, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.537925602811099, '_wandb': {'runtime': 67}, 'epoch': 9, 'val_acc': 0.530067209055536, '_runtime': 68.12158703804016, 'val_loss': 0.701181948184967, 'val_acc_balanced': 0.5312283136710618, '_step': 16}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",sage-sun-646
271,"{'epoch': 9, 'val_acc': 0.5838344534842589, 'val_loss': 0.6833327412605286, '_timestamp': 1686322240.832205, 'train_loss': 0.6681647300720215, 'val_acc_balanced': 0.5818146018527698, '_step': 16, '_wandb': {'runtime': 69}, '_runtime': 69.6337571144104, 'val_acc_rec': 0.5953307392996109, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.5922997697806858}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",sage-sea-645
272,"{'_runtime': 65.58952593803406, 'val_acc_rec': 0.490272373540856, 'val_acc_balanced': 0.5, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 65}, 'val_acc': 0.490272373540856, 'val_loss': 0.6971991658210754, '_timestamp': 1686322081.2081668, 'train_loss': 0.6753465533256531, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",woven-wind-644
273,"{'val_acc_rec': 0.5369649805447471, '_wandb': {'runtime': 70}, 'val_acc': 0.5403254333215423, '_runtime': 70.78859901428223, '_timestamp': 1686321922.935936, 'train_loss': 0.6048441529273987, 'val_acc_rec_balanced': 0.5353507815339876, '_step': 16, 'epoch': 9, 'val_loss': 0.717195451259613, 'val_acc_balanced': 0.538702179923554, 'trainer/global_step': 369}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",cosmic-music-643
274,"{'val_acc_rec': 0.5097276264591439, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 69}, 'val_acc': 0.5097276264591439, '_runtime': 69.34168839454651, 'val_loss': 0.7009085416793823, '_step': 16, '_timestamp': 1686321759.9123244, 'train_loss': 0.6399356722831726, 'val_acc_balanced': 0.5, 'trainer/global_step': 359}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",quiet-blaze-642
275,"{'epoch': 9, 'val_acc': 0.5097276264591439, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, 'train_loss': 0.6249659061431885, 'trainer/global_step': 359, 'val_acc_rec_balanced': 0.5, '_step': 16, '_wandb': {'runtime': 68}, '_runtime': 69.21729755401611, 'val_loss': 0.6954938769340515, '_timestamp': 1686321600.2167685}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",skilled-dawn-641
276,"{'_wandb': {'runtime': 70}, '_runtime': 70.65196824073792, 'val_loss': 0.6873325705528259, '_timestamp': 1686321441.0903573, 'val_acc_rec_balanced': 0.5732460923300617, '_step': 16, 'val_acc': 0.5834807216130173, 'train_loss': 0.6590187549591064, 'val_acc_rec': 0.5797665369649806, 'val_acc_balanced': 0.5780046925085094, 'trainer/global_step': 389, 'epoch': 9}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",rose-wildflower-640
277,"{'val_acc_rec_balanced': 0.5605840300496789, '_wandb': {'runtime': 65}, 'val_acc': 0.5748142907675982, 'val_loss': 0.7110516428947449, '_timestamp': 1686321280.7320385, 'train_loss': 0.6075230836868286, 'trainer/global_step': 389, '_step': 16, 'epoch': 9, '_runtime': 66.1155104637146, 'val_acc_rec': 0.5680933852140078, 'val_acc_balanced': 0.5680441272044325}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",vocal-armadillo-639
278,"{'_step': 15, '_wandb': {'runtime': 68}, 'val_acc': 0.6016979129819596, '_runtime': 69.06434607505798, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.6069308130376833, 'epoch': 9, 'val_loss': 0.665449321269989, '_timestamp': 1686321125.290729, 'train_loss': 0.6322862505912781, 'val_acc_rec': 0.6070038910505836, 'val_acc_balanced': 0.6016021722128593}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",lively-donkey-638
279,"{'_step': 15, '_wandb': {'runtime': 67}, 'train_loss': 0.6324453353881836, 'val_acc_rec': 0.5408560311284046, 'trainer/global_step': 329, 'epoch': 9, 'val_acc': 0.5521754510081358, '_runtime': 67.95285773277283, 'val_loss': 0.706169068813324, '_timestamp': 1686320966.7782607, 'val_acc_balanced': 0.5498854411068151, 'val_acc_rec_balanced': 0.5382588149763723}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",proud-thunder-637
280,"{'val_acc_rec': 0.5097276264591439, 'val_acc_rec_balanced': 0.5, '_step': 27, '_wandb': {'runtime': 76}, 'val_acc': 0.5097276264591439, '_runtime': 77.00043892860413, 'train_loss': 0.6996304392814636, 'epoch': 9, 'val_loss': 0.6974128484725952, '_timestamp': 1686320808.004355, 'val_acc_balanced': 0.5, 'trainer/global_step': 929}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",hopeful-brook-636
281,"{'epoch': 9, 'val_acc': 0.5753448885744605, '_timestamp': 1686320634.218142, 'train_loss': 0.6631152629852295, 'val_acc_rec': 0.5525291828793775, 'trainer/global_step': 929, '_step': 27, '_runtime': 78.71148014068604, 'val_loss': 0.6799110174179077, 'val_acc_balanced': 0.5702031217298393, 'val_acc_rec_balanced': 0.5451653944020356, '_wandb': {'runtime': 78}}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",comfy-planet-635
282,"{'_step': 27, 'val_acc': 0.603112840466926, '_runtime': 79.15151238441467, 'val_loss': 0.6626390814781189, 'train_loss': 0.6431893706321716, 'trainer/global_step': 939, 'epoch': 9, '_wandb': {'runtime': 78}, '_timestamp': 1686320461.9092274, 'val_acc_rec': 0.642023346303502, 'val_acc_balanced': 0.6009866935439454, 'val_acc_rec_balanced': 0.640373197625106}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",still-morning-634
283,"{'epoch': 9, 'val_loss': 0.694597065448761, 'train_loss': 0.5924402475357056, 'val_acc_balanced': 0.5706451097672471, 'trainer/global_step': 939, '_step': 27, '_wandb': {'runtime': 77}, 'val_acc': 0.5728687654757694, '_runtime': 77.3355803489685, '_timestamp': 1686320289.5148404, 'val_acc_rec': 0.5642023346303502, 'val_acc_rec_balanced': 0.5619168787107718}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",fine-bee-633
284,"{'val_loss': 0.6950161457061768, '_timestamp': 1686320115.8080175, 'train_loss': 0.6783396601676941, 'val_acc_rec': 0.5097276264591439, 'trainer/global_step': 909, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 79}, 'val_acc': 0.5097276264591439, '_runtime': 79.80355978012085, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",quiet-violet-632
285,"{'_timestamp': 1686319942.1034088, 'val_acc_balanced': 0.5, 'trainer/global_step': 909, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 79}, 'val_acc': 0.5097276264591439, '_runtime': 79.66668677330017, 'val_loss': 0.6950141787528992, 'train_loss': 0.678343653678894, 'val_acc_rec': 0.5097276264591439, '_step': 27}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",glowing-darkness-631
286,"{'_step': 27, 'epoch': 9, '_wandb': {'runtime': 79}, 'val_acc': 0.5871949062610541, '_runtime': 80.11238193511963, '_timestamp': 1686319767.3390188, 'val_acc_rec': 0.5914396887159533, 'val_loss': 0.6754956245422363, 'train_loss': 0.7007783651351929, 'val_acc_balanced': 0.5876719760307547, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.5913607173149158}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",solar-oath-630
287,"{'_wandb': {'runtime': 78}, 'val_loss': 0.6781070828437805, 'train_loss': 0.7159444689750671, 'val_acc_balanced': 0.5830937510326824, '_step': 27, 'epoch': 9, '_timestamp': 1686319591.6810844, 'val_acc_rec': 0.5914396887159533, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.5872712952865624, 'val_acc': 0.5863105765829502, '_runtime': 78.65933132171631}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",lively-bee-629
288,"{'train_loss': 0.5917506217956543, 'val_acc_rec': 0.6108949416342413, '_step': 26, 'epoch': 9, '_wandb': {'runtime': 77}, 'val_acc': 0.6038203042094092, 'val_loss': 0.6682419776916504, '_timestamp': 1686319419.1082268, 'trainer/global_step': 889, '_runtime': 78.11989974975586, 'val_acc_balanced': 0.603243448663296, 'val_acc_rec_balanced': 0.6108990670059373}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",youthful-snowflake-628
289,"{'val_acc_balanced': 0.559231904651752, 'val_acc_rec_balanced': 0.5454380225372591, '_step': 26, 'epoch': 9, '_wandb': {'runtime': 69}, 'val_acc': 0.5624336752741422, 'val_acc_rec': 0.5486381322957199, 'trainer/global_step': 889, '_runtime': 70.26158833503723, 'val_loss': 0.7045421004295349, '_timestamp': 1686319243.1329534, 'train_loss': 0.5873494744300842}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",lively-eon-627
290,"{'epoch': 9, 'val_acc': 0.5772904138662893, '_runtime': 91.47959566116332, '_timestamp': 1686319066.8722217, 'train_loss': 0.6829697489738464, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5795055241620127, 'trainer/global_step': 1869, '_step': 46, '_wandb': {'runtime': 91}, 'val_loss': 0.6796239018440247, 'val_acc_rec_balanced': 0.5971464921846601}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",stoic-paper-626
291,"{'_timestamp': 1686318870.7146285, 'train_loss': 0.6771419048309326, 'val_acc_rec': 0.5603112840466926, 'val_acc_balanced': 0.5761279644867431, 'trainer/global_step': 1869, 'epoch': 9, '_wandb': {'runtime': 93}, 'val_loss': 0.6750152111053467, 'val_acc_rec_balanced': 0.5571913243668969, '_step': 46, 'val_acc': 0.5785284754156349, '_runtime': 93.25051736831664}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",deft-serenity-625
292,"{'epoch': 9, '_runtime': 91.66627764701845, 'train_loss': 0.6618081331253052, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.576214709802496, '_step': 46, 'val_acc': 0.562610541209763, 'val_loss': 0.6808432340621948, '_timestamp': 1686318676.466745, 'val_acc_rec': 0.5719844357976653, 'val_acc_balanced': 0.5667718625733893, '_wandb': {'runtime': 91}}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",daily-deluge-624
293,"{'train_loss': 0.6817818880081177, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, 'epoch': 9, 'val_loss': 0.6937886476516724, 'val_acc': 0.5097276264591439, '_runtime': 95.27544736862184, '_timestamp': 1686318482.8095243, '_step': 46, '_wandb': {'runtime': 95}}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",treasured-puddle-623
294,"{'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 90}, 'val_loss': 0.6937980651855469, 'train_loss': 0.6893053650856018, 'val_acc': 0.5097276264591439, '_runtime': 90.80975723266602, '_timestamp': 1686318285.1887383, 'val_acc_rec': 0.5097276264591439}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",light-bird-622
295,"{'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 92}, 'val_acc': 0.5097276264591439, '_runtime': 92.6350691318512, 'val_loss': 0.6937980651855469, '_timestamp': 1686318089.4885132, 'val_acc_balanced': 0.5, '_step': 46, 'epoch': 9, 'train_loss': 0.6893051862716675, 'val_acc_rec': 0.5097276264591439, 'trainer/global_step': 1869}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",sage-blaze-621
296,"{'val_loss': 0.6810190081596375, '_timestamp': 1686317892.4819963, 'trainer/global_step': 1869, 'epoch': 9, 'val_acc': 0.5656172621153166, '_runtime': 318.3604323863983, 'train_loss': 0.665469765663147, 'val_acc_rec': 0.5992217898832685, 'val_acc_balanced': 0.5703270436094863, 'val_acc_rec_balanced': 0.6039924875802738, '_step': 46, '_wandb': {'runtime': 318}}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",worthy-sun-620
297,"{'_wandb': {'runtime': 93}, 'val_acc': 0.5758754863813229, '_timestamp': 1686317472.4870694, 'val_acc_rec': 0.5797665369649806, 'val_acc_balanced': 0.5755358932839849, '_step': 46, '_runtime': 93.26682543754578, 'val_loss': 0.6724898815155029, 'train_loss': 0.6753359436988831, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.579607415485278, 'epoch': 9}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",quiet-star-619
298,"{'train_loss': 0.6894465684890747, 'epoch': 9, '_wandb': {'runtime': 88}, '_runtime': 88.89174795150757, '_timestamp': 1686317272.478919, 'val_acc_balanced': 0.5028102728484408, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.4990912395492548, '_step': 46, 'val_acc': 0.4939865581888928, 'val_loss': 0.7043251395225525, 'val_acc_rec': 0.490272373540856}","{'weights': 'under_50_task', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",dutiful-river-618
299,"{'val_acc_balanced': 0.5756487998854412, '_step': 46, 'epoch': 9, '_runtime': 92.99564027786256, '_timestamp': 1686317069.7828572, 'train_loss': 0.678149938583374, 'val_acc_rec': 0.5914396887159533, '_wandb': {'runtime': 92}, 'val_acc': 0.5744605588963566, 'val_loss': 0.6786254644393921, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5924209378407852}","{'weights': 'pathological_task', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",smart-pine-617
300,"{'_step': 9, 'epoch': 9, 'val_acc': 0.6147859922178989, '_runtime': 61.1771194934845, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 60}, 'val_loss': 0.7704058289527893, '_timestamp': 1686316874.5625546, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5, 'trainer/global_step': 29}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",charmed-thunder-616
301,"{'_timestamp': 1686316722.6441996, 'val_acc_rec': 0.6147859922178989, '_step': 9, 'val_acc': 0.6147859922178989, '_runtime': 59.09199571609497, 'val_loss': 0.5878946185112, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 58}}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",iconic-energy-615
302,"{'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, '_step': 9, 'val_acc': 0.6147859922178989, '_runtime': 51.713844776153564, '_timestamp': 1686316571.0006847, 'epoch': 9, '_wandb': {'runtime': 51}, 'val_loss': 0.6831826567649841, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",lucky-elevator-614
303,"{'epoch': 9, '_timestamp': 1686316422.3381274, 'val_acc_rec': 0.8171206225680934, 'val_acc_balanced': 0.7744955306808011, 'val_acc_rec_balanced': 0.7645122107147424, '_step': 9, '_wandb': {'runtime': 60}, 'val_acc': 0.8190661478599222, '_runtime': 60.66935348510742, 'val_loss': 0.5339059829711914, 'trainer/global_step': 29}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",icy-bee-613
304,"{'_timestamp': 1686316274.159793, 'val_acc_rec': 0.7276264591439688, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 58}, 'val_acc': 0.6945525291828794, '_runtime': 59.135432958602905, 'val_loss': 0.8350798487663269, 'val_acc_balanced': 0.690374690518534, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.7105868814729575}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",vital-pond-612
305,"{'epoch': 9, '_wandb': {'runtime': 56}, 'val_acc_rec': 0.7665369649805448, 'val_acc_balanced': 0.7321459706384907, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.6988556450581767, '_step': 9, 'val_acc': 0.7822780332507959, '_runtime': 57.2853741645813, 'val_loss': 0.5918741226196289, '_timestamp': 1686316125.9156992}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",bumbling-armadillo-611
306,"{'val_acc_rec_balanced': 0.5, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 60}, 'val_acc': 0.6147859922178989, '_runtime': 61.47346329689026, 'val_acc_rec': 0.6147859922178989, 'val_loss': 0.8370465636253357, '_timestamp': 1686315979.9629402, 'val_acc_balanced': 0.5, 'trainer/global_step': 29}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",zesty-gorge-610
307,"{'_step': 9, '_wandb': {'runtime': 60}, '_runtime': 60.716538429260254, 'val_loss': 0.5474437475204468, 'val_acc_rec': 0.8482490272373541, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.8256616800920599, 'epoch': 9, 'val_acc': 0.8142907675981605, '_timestamp': 1686315830.0289905, 'val_acc_balanced': 0.8026728737315618}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",frosty-resonance-609
308,"{'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 61}, 'val_acc': 0.6147859922178989, 'val_loss': 0.6296240091323853, 'val_acc_balanced': 0.5, '_step': 9, '_runtime': 61.879603147506714, '_timestamp': 1686315680.916829, 'val_acc_rec': 0.6147859922178989}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",stellar-disco-608
309,"{'_timestamp': 1686315530.5210445, 'val_acc_balanced': 0.787979042438191, 'val_acc_rec_balanced': 0.8186612965093978, 'epoch': 9, '_wandb': {'runtime': 60}, '_runtime': 61.10171937942505, 'val_loss': 0.5295641422271729, '_step': 9, 'val_acc': 0.7974885037141847, 'val_acc_rec': 0.8326848249027238, 'trainer/global_step': 29}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",magic-firefly-607
310,"{'epoch': 9, 'val_acc': 0.7085249380969225, 'val_loss': 0.5905271768569946, 'train_loss': 0.4581366181373596, 'val_acc_rec': 0.7276264591439688, 'trainer/global_step': 79, '_step': 10, '_wandb': {'runtime': 61}, '_runtime': 61.98388314247131, '_timestamp': 1686315382.612822, 'val_acc_balanced': 0.6727633062500726, 'val_acc_rec_balanced': 0.6879555044112007}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",vibrant-bird-606
311,"{'val_acc_balanced': 0.8195083748881218, 'epoch': 9, '_wandb': {'runtime': 62}, 'val_acc': 0.836045277679519, 'val_loss': 0.5176490545272827, 'train_loss': 0.30459827184677124, 'val_acc_rec_balanced': 0.823168392788646, '_step': 10, '_runtime': 63.032856464385986, '_timestamp': 1686315231.1622036, 'val_acc_rec': 0.8521400778210116, 'trainer/global_step': 79}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",usual-disco-605
312,"{'_timestamp': 1686315079.621116, 'train_loss': 0.5947871208190918, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 62}, 'val_acc': 0.6147859922178989, '_runtime': 63.31002187728882, 'val_loss': 0.6862272024154663, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",stilted-firefly-604
313,"{'_timestamp': 1686314926.474264, 'val_acc_rec': 0.7976653696498055, 'val_acc_balanced': 0.7540566772442492, 'val_acc_rec_balanced': 0.7373737373737373, '_wandb': {'runtime': 62}, 'epoch': 9, 'val_acc': 0.8072161301733286, '_runtime': 63.13741588592529, 'val_loss': 0.7276063561439514, 'train_loss': 0.33852654695510864, 'trainer/global_step': 79, '_step': 10}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",comfy-wood-603
314,"{'epoch': 9, '_wandb': {'runtime': 61}, 'train_loss': 0.5724402070045471, 'val_acc_balanced': 0.6549514128628052, '_step': 10, 'val_acc': 0.6128404669260701, '_runtime': 62.04206442832947, 'val_loss': 0.8439521789550781, '_timestamp': 1686314771.5381484, 'val_acc_rec': 0.6498054474708171, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.6887866001790053}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",elated-dream-602
315,"{'epoch': 9, 'val_loss': 0.47930845618247986, 'train_loss': 0.1879727691411972, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.8395345863700294, '_step': 10, '_wandb': {'runtime': 62}, 'val_acc': 0.8372833392288646, '_runtime': 62.89871287345886, '_timestamp': 1686314616.2348528, 'val_acc_rec': 0.8560311284046692, 'val_acc_balanced': 0.8270303727726052}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",golden-energy-601
316,"{'epoch': 9, '_wandb': {'runtime': 62}, 'val_acc': 0.6986204457021578, '_timestamp': 1686314462.9504828, 'val_acc_balanced': 0.6402764701096116, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.6268060350338831, '_step': 10, '_runtime': 63.45256280899048, 'val_loss': 0.6684104800224304, 'train_loss': 0.3866092562675476, 'val_acc_rec': 0.6964980544747081}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",confused-bee-600
317,"{'_wandb': {'runtime': 59}, '_runtime': 60.35193490982056, '_timestamp': 1686314309.4765449, 'val_acc_rec': 0.8443579766536965, 'val_acc_rec_balanced': 0.8036376422452371, '_step': 10, 'epoch': 9, 'train_loss': 0.30139079689979553, 'val_acc_balanced': 0.7921969987562623, 'trainer/global_step': 79, 'val_acc': 0.8319773611602406, 'val_loss': 0.48570382595062256}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",pretty-haze-599
318,"{'_runtime': 63.40727615356445, '_timestamp': 1686314160.487986, 'train_loss': 0.4457250237464905, 'val_acc_balanced': 0.5, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 62}, 'val_loss': 0.7561104893684387, 'val_acc_rec': 0.6147859922178989, '_step': 10, 'val_acc': 0.6147859922178989}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",polar-terrain-598
319,"{'_timestamp': 1686314011.0195436, 'val_acc_rec': 0.6147859922178989, 'trainer/global_step': 79, '_wandb': {'runtime': 61}, 'val_acc': 0.6147859922178989, '_runtime': 61.50334358215332, 'val_loss': 0.5290043950080872, 'train_loss': 0.4180987775325775, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 10, 'epoch': 9}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",true-darkness-597
320,"{'_runtime': 65.85824346542358, 'val_loss': 0.5810520648956299, '_timestamp': 1686313863.2156565, 'val_acc_rec': 0.7237354085603113, 'val_acc_balanced': 0.6472158291778545, 'epoch': 9, '_wandb': {'runtime': 65}, 'val_acc': 0.7030420940926777, 'train_loss': 0.5263475775718689, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.6640455184758982, '_step': 12}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",major-sponge-596
321,"{'_step': 12, '_wandb': {'runtime': 63}, 'val_acc': 0.842058719490626, 'val_loss': 0.4439573884010315, '_timestamp': 1686313709.0067284, 'trainer/global_step': 169, 'epoch': 9, '_runtime': 63.84129738807678, 'train_loss': 0.3779288232326507, 'val_acc_rec': 0.8599221789883269, 'val_acc_balanced': 0.8284281247457312, 'val_acc_rec_balanced': 0.833269402889656}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",pious-donkey-595
322,"{'val_acc_rec_balanced': 0.686676895537655, '_step': 12, 'val_acc': 0.7032189600282985, '_runtime': 64.02399563789368, 'val_loss': 0.5971121191978455, '_timestamp': 1686313556.7024896, 'val_acc_rec': 0.7237354085603113, 'trainer/global_step': 169, 'epoch': 9, '_wandb': {'runtime': 63}, 'train_loss': 0.5876997113227844, 'val_acc_balanced': 0.6677622019969546}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",azure-smoke-594
323,"{'val_acc_rec': 0.867704280155642, 'val_acc_balanced': 0.8293536632144227, 'val_acc_rec_balanced': 0.8433704129906661, '_step': 12, 'val_acc': 0.8493102228510789, '_runtime': 50.765528440475464, '_timestamp': 1686313402.8472044, 'train_loss': 0.3823436200618744, 'trainer/global_step': 169, 'epoch': 9, '_wandb': {'runtime': 50}, 'val_loss': 0.40507930517196655}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",misunderstood-hill-593
324,"{'epoch': 9, 'val_acc_balanced': 0.678530122862689, 'val_acc_rec_balanced': 0.7230533179900268, 'val_acc_rec': 0.708171206225681, '_step': 12, '_wandb': {'runtime': 65}, 'val_acc': 0.6605942695436859, '_runtime': 66.284423828125, 'val_loss': 0.622646689414978, '_timestamp': 1686313263.3739948, 'train_loss': 0.43733078241348267, 'trainer/global_step': 179}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",devout-cloud-592
325,"{'val_acc_balanced': 0.8058069184363776, 'trainer/global_step': 179, 'val_acc_rec_balanced': 0.8439138217619231, 'epoch': 9, '_wandb': {'runtime': 64}, '_runtime': 65.35542392730713, '_timestamp': 1686313107.5850508, 'val_acc_rec': 0.8521400778210116, '_step': 12, 'val_acc': 0.8073929961089494, 'val_loss': 0.4438592791557312, 'train_loss': 0.25203269720077515}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",charmed-grass-591
326,"{'_timestamp': 1686312953.1370134, 'trainer/global_step': 189, 'val_acc_rec_balanced': 0.6960427055363765, 'epoch': 9, '_wandb': {'runtime': 64}, 'val_acc': 0.6970286522815705, 'train_loss': 0.6108002066612244, 'val_acc_rec': 0.7120622568093385, 'val_acc_balanced': 0.6910169008845648, '_step': 12, '_runtime': 64.75577139854431, 'val_loss': 0.6171349287033081}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",sage-fog-590
327,"{'_timestamp': 1686312799.2102208, 'train_loss': 0.27031633257865906, 'val_acc_rec': 0.8871595330739299, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 67}, 'val_loss': 0.3654738664627075, 'val_acc_rec_balanced': 0.8742807825086306, 'val_acc': 0.8509020162716661, '_runtime': 68.02179074287415, 'val_acc_balanced': 0.8393921958363846, 'trainer/global_step': 189}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",pious-sunset-589
328,"{'val_acc_rec': 0.7276264591439688, 'val_acc_balanced': 0.6677505782799223, 'val_acc_rec_balanced': 0.6747538677918425, 'epoch': 9, '_runtime': 67.51694893836975, 'val_acc': 0.7115316590024762, 'val_loss': 0.5790143609046936, '_timestamp': 1686312642.4210448, 'train_loss': 0.568990170955658, 'trainer/global_step': 159, '_step': 12, '_wandb': {'runtime': 67}}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",rosy-cloud-588
329,"{'val_acc_balanced': 0.8131937906103613, 'trainer/global_step': 159, 'val_acc_rec_balanced': 0.8257256105357371, '_runtime': 66.31939792633057, '_timestamp': 1686312487.242681, 'val_acc_rec': 0.8599221789883269, 'val_acc': 0.841351255748143, 'val_loss': 0.3936886191368103, 'train_loss': 0.37829282879829407, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 65}}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",silver-moon-587
330,"{'val_acc': 0.7163070392642377, '_runtime': 65.24025964736938, 'val_loss': 0.5741815567016602, 'train_loss': 0.5388331413269043, 'val_acc_rec': 0.7315175097276264, '_step': 16, '_wandb': {'runtime': 64}, 'val_acc_balanced': 0.6875791865722821, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.6986638537271449, 'epoch': 9, '_timestamp': 1686312330.4489458}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",swift-pyramid-586
331,"{'_runtime': 67.14042091369629, 'train_loss': 0.3211502730846405, 'val_acc_rec_balanced': 0.8074095384221966, '_step': 16, 'epoch': 9, 'val_loss': 0.394228458404541, '_timestamp': 1686312173.344334, 'val_acc_rec': 0.8443579766536965, 'val_acc_balanced': 0.79878474038428, 'trainer/global_step': 369, '_wandb': {'runtime': 66}, 'val_acc': 0.8348072161301733}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",frosty-breeze-585
332,"{'_wandb': {'runtime': 63}, '_runtime': 63.31521797180176, '_timestamp': 1686312010.001921, 'train_loss': 0.6416609883308411, 'val_acc_rec': 0.688715953307393, 'val_acc_rec_balanced': 0.7053445850914206, '_step': 16, 'epoch': 9, 'val_acc': 0.6512203749557836, 'val_loss': 0.6112837195396423, 'val_acc_balanced': 0.6745925887180203, 'trainer/global_step': 369}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",ethereal-puddle-584
333,"{'_wandb': {'runtime': 70}, 'train_loss': 0.473855197429657, 'val_acc_rec': 0.8599221789883269, 'val_acc_balanced': 0.8112831421231882, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.8351553509781358, '_step': 16, 'epoch': 9, 'val_acc': 0.842058719490626, '_runtime': 70.86086130142212, 'val_loss': 0.3806027472019195, '_timestamp': 1686311845.0850112}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",fanciful-sponge-583
334,"{'val_acc_rec': 0.7315175097276264, 'trainer/global_step': 359, 'epoch': 9, '_wandb': {'runtime': 68}, 'val_acc': 0.7056950831269897, 'val_loss': 0.6230067014694214, 'val_acc_balanced': 0.6873496181608955, 'val_acc_rec_balanced': 0.7024357499041043, '_step': 16, '_runtime': 68.79655385017395, '_timestamp': 1686311684.315153, 'train_loss': 0.3046543002128601}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",sweet-deluge-582
335,"{'val_acc_rec': 0.8482490272373541, 'val_acc_rec_balanced': 0.8369773686229383, '_step': 16, 'epoch': 9, 'val_acc': 0.8167668906968518, 'val_loss': 0.41107335686683655, '_timestamp': 1686311523.2866168, '_wandb': {'runtime': 70}, '_runtime': 70.56754875183105, 'train_loss': 0.2403892278671265, 'val_acc_balanced': 0.8116304006695261, 'trainer/global_step': 359}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",cerulean-plasma-581
336,"{'_timestamp': 1686311361.5253427, 'val_acc_rec': 0.7120622568093385, 'val_acc_balanced': 0.690325289721147, 'val_acc_rec_balanced': 0.699814601713336, '_step': 16, 'epoch': 9, '_runtime': 68.91296863555908, 'train_loss': 0.5152024030685425, 'trainer/global_step': 389, '_wandb': {'runtime': 68}, 'val_acc': 0.6959674566678458, 'val_loss': 0.5882174372673035}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",sandy-cloud-580
337,"{'epoch': 9, 'val_acc': 0.8556773965334277, 'val_acc_rec': 0.8599221789883269, 'trainer/global_step': 389, 'val_acc_rec_balanced': 0.833269402889656, 'train_loss': 0.24896615743637085, 'val_acc_balanced': 0.8317888319326754, '_step': 16, '_wandb': {'runtime': 70}, '_runtime': 70.72946453094482, 'val_loss': 0.3811919093132019, '_timestamp': 1686311200.4093466}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",confused-sun-579
338,"{'epoch': 9, 'val_acc': 0.7237354085603113, '_runtime': 68.69447302818298, '_timestamp': 1686311039.300086, 'val_acc_rec_balanced': 0.6949558879938627, '_step': 15, '_wandb': {'runtime': 68}, 'val_loss': 0.5656318664550781, 'train_loss': 0.603150486946106, 'val_acc_rec': 0.7431906614785992, 'val_acc_balanced': 0.6817048505771175, 'trainer/global_step': 329}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",trim-breeze-578
339,"{'_runtime': 70.54804587364197, '_timestamp': 1686310880.1393049, 'val_acc_rec': 0.8521400778210116, 'val_acc_balanced': 0.8112061349978497, '_step': 15, '_wandb': {'runtime': 70}, 'val_acc': 0.8436505129112133, 'val_loss': 0.3756900131702423, 'train_loss': 0.3351062834262848, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.8212824447001662, 'epoch': 9}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",major-eon-577
340,"{'val_acc_rec': 0.5875486381322957, 'val_acc_balanced': 0.6205887993862678, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.6419255849635597, '_step': 27, 'val_acc': 0.5682702511496286, '_runtime': 79.14278292655945, 'val_loss': 0.6677701473236084, 'epoch': 9, '_wandb': {'runtime': 78}, '_timestamp': 1686310719.028501, 'train_loss': 0.5995495319366455}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",gentle-night-576
341,"{'epoch': 9, 'val_acc': 0.8374602051644853, '_runtime': 78.71735978126526, 'val_acc_rec': 0.8404669260700389, 'val_acc_rec_balanced': 0.8042449814601713, '_step': 27, '_wandb': {'runtime': 78}, 'val_loss': 0.37948715686798096, '_timestamp': 1686310546.3857307, 'train_loss': 0.2850581705570221, 'val_acc_balanced': 0.8017996419895155, 'trainer/global_step': 929}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",stellar-valley-575
342,"{'val_acc_balanced': 0.7029893294277645, '_step': 27, 'epoch': 9, 'val_acc': 0.6917226742129465, '_runtime': 79.17529082298279, 'val_loss': 0.5860292315483093, 'train_loss': 0.6375588178634644, 'val_acc_rec': 0.708171206225681, 'val_acc_rec_balanced': 0.7136235775476282, '_wandb': {'runtime': 78}, '_timestamp': 1686310371.698346, 'trainer/global_step': 939}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",worldly-frost-574
343,"{'train_loss': 0.39017370343208313, 'val_acc_balanced': 0.7987048273296835, 'trainer/global_step': 939, 'val_acc_rec_balanced': 0.8137386523462473, 'epoch': 9, '_wandb': {'runtime': 77}, '_runtime': 78.04473567008972, '_timestamp': 1686310199.4748316, '_step': 27, 'val_acc': 0.8358684117438981, 'val_loss': 0.3888731002807617, 'val_acc_rec': 0.8521400778210116}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",fanciful-lion-573
344,"{'val_acc_balanced': 0.6998029779963036, '_wandb': {'runtime': 77}, 'val_loss': 0.5700092911720276, '_timestamp': 1686310028.7998445, 'train_loss': 0.5528482794761658, 'val_acc_rec': 0.7470817120622568, 'trainer/global_step': 909, 'val_acc_rec_balanced': 0.7207518220176448, '_step': 27, 'epoch': 9, 'val_acc': 0.7203749557835161, '_runtime': 77.77836728096008}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",super-brook-572
345,"{'_step': 27, 'train_loss': 0.39485540986061096, 'trainer/global_step': 909, 'val_acc_rec_balanced': 0.833269402889656, 'val_acc_rec': 0.8599221789883269, 'val_acc_balanced': 0.8308560286408387, 'epoch': 9, '_wandb': {'runtime': 78}, 'val_acc': 0.8523169437566325, '_runtime': 78.60711669921875, 'val_loss': 0.35872772336006165, '_timestamp': 1686309854.9533236}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",divine-bush-571
346,"{'_step': 27, 'val_acc': 0.7405376724442873, 'val_loss': 0.5487068891525269, '_timestamp': 1686309682.6796043, 'val_acc_balanced': 0.7062570468784508, 'val_acc_rec_balanced': 0.706271576524741, 'epoch': 9, '_wandb': {'runtime': 78}, '_runtime': 78.61224150657654, 'train_loss': 0.4853380024433136, 'val_acc_rec': 0.7431906614785992, 'trainer/global_step': 929}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",astral-donkey-570
347,"{'_step': 27, 'epoch': 9, '_wandb': {'runtime': 78}, '_runtime': 78.94701838493347, 'val_acc_balanced': 0.8100582929409166, 'val_acc': 0.8427661832331093, 'val_loss': 0.371667355298996, '_timestamp': 1686309507.9598365, 'train_loss': 0.3679414987564087, 'val_acc_rec': 0.8560311284046692, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.8225610535737118}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",sweet-salad-569
348,"{'val_acc': 0.7396533427661832, '_runtime': 79.08844757080078, 'val_loss': 0.5430564284324646, 'val_acc_balanced': 0.7051949297346305, 'val_acc_rec_balanced': 0.6999424626006905, '_wandb': {'runtime': 78}, 'epoch': 9, '_timestamp': 1686309335.4037795, 'train_loss': 0.5500141978263855, 'val_acc_rec': 0.7354085603112841, 'trainer/global_step': 889, '_step': 26}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",dulcet-sun-568
349,"{'_step': 26, 'epoch': 9, '_wandb': {'runtime': 77}, 'val_acc': 0.8496639547223205, 'train_loss': 0.33157995343208313, 'val_acc_rec': 0.867704280155642, 'val_acc_balanced': 0.8275839522962654, '_runtime': 78.01321005821228, 'val_loss': 0.34955352544784546, '_timestamp': 1686309163.326245, 'trainer/global_step': 889, 'val_acc_rec_balanced': 0.8452563610791459}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",dainty-brook-567
350,"{'val_acc_rec_balanced': 0.7755721774709117, '_step': 46, '_wandb': {'runtime': 93}, 'val_loss': 0.5468357801437378, 'val_acc_rec': 0.7704280155642024, 'val_acc_balanced': 0.7434732828864015, 'trainer/global_step': 1869, 'epoch': 9, 'val_acc': 0.7322249734701096, '_runtime': 94.1192557811737, '_timestamp': 1686308990.4086428, 'train_loss': 0.5893682837486267}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",neat-planet-566
351,"{'val_acc_balanced': 0.8355534632864898, 'val_acc_rec_balanced': 0.880545965989004, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 92}, '_timestamp': 1686308794.4979317, 'train_loss': 0.4537258446216583, 'val_acc_rec': 0.8832684824902723, 'val_acc': 0.838698266713831, '_runtime': 92.81517362594604, 'val_loss': 0.3679197132587433, 'trainer/global_step': 1869}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",tough-puddle-565
352,"{'_timestamp': 1686308598.7487104, 'train_loss': 0.4572182595729828, 'val_acc_balanced': 0.7165760016738152, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.7441183991816904, 'epoch': 9, '_wandb': {'runtime': 92}, 'val_loss': 0.5467473864555359, 'val_acc_rec': 0.7665369649805448, '_step': 46, 'val_acc': 0.7359391581181465, '_runtime': 92.25097727775574}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",chocolate-haze-564
353,"{'val_acc_balanced': 0.8403395287745115, 'epoch': 9, 'val_acc': 0.8569154580827733, 'val_loss': 0.3435688614845276, '_timestamp': 1686308405.1307902, 'train_loss': 0.3444572687149048, 'val_acc_rec_balanced': 0.8705088863316712, '_step': 46, '_wandb': {'runtime': 93}, '_runtime': 94.11521315574646, 'val_acc_rec': 0.8871595330739299, 'trainer/global_step': 1869}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",glamorous-blaze-563
354,"{'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.7301815624600434, '_step': 46, 'epoch': 9, 'val_acc': 0.7316943756632472, '_runtime': 91.78890562057497, 'val_loss': 0.5610827803611755, 'val_acc_rec': 0.7470817120622568, '_wandb': {'runtime': 91}, '_timestamp': 1686308209.4608085, 'train_loss': 0.5507749915122986, 'val_acc_balanced': 0.7201532005904848}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",celestial-cherry-562
355,"{'train_loss': 0.4749535322189331, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.842763073775732, '_step': 46, 'val_acc': 0.8470109656880085, '_runtime': 92.53933525085448, '_timestamp': 1686308016.0414832, 'val_acc_balanced': 0.8192541060780416, 'epoch': 9, '_wandb': {'runtime': 92}, 'val_loss': 0.3642296195030213, 'val_acc_rec': 0.8715953307392996}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",helpful-glade-561
356,"{'_runtime': 92.21539449691772, 'val_loss': 0.5539172887802124, '_timestamp': 1686307819.9169116, 'val_acc_rec': 0.7665369649805448, 'val_acc_rec_balanced': 0.7592059838895282, '_step': 46, 'val_acc': 0.7288645206933144, 'train_loss': 0.5121397376060486, 'val_acc_balanced': 0.7241953481884437, 'trainer/global_step': 1869, 'epoch': 9, '_wandb': {'runtime': 92}}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",celestial-smoke-560
357,"{'val_loss': 0.35343673825263977, '_timestamp': 1686307625.0106118, 'val_acc_balanced': 0.8362421685206496, '_step': 46, '_wandb': {'runtime': 92}, '_runtime': 92.80671572685242, 'train_loss': 0.3554413914680481, 'val_acc_rec': 0.8638132295719845, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.8496355964710395, 'epoch': 9, 'val_acc': 0.8496639547223205}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",worldly-vortex-559
358,"{'_runtime': 92.29675388336182, 'val_loss': 0.5694777369499207, 'train_loss': 0.5836241245269775, 'val_acc_balanced': 0.7172748776603782, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 92}, 'val_acc': 0.7226742129465865, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.7295102928014321, '_timestamp': 1686307430.345433, 'val_acc_rec': 0.7392996108949417}","{'weights': 'under_50_task', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",stellar-plasma-558
359,"{'val_loss': 0.4068686366081238, '_timestamp': 1686307237.1239078, 'val_acc_rec': 0.8054474708171206, '_step': 46, '_wandb': {'runtime': 92}, 'val_acc': 0.8100459851432614, '_runtime': 92.42095589637756, 'val_acc_rec_balanced': 0.7512466436517069, 'epoch': 9, 'train_loss': 0.5322572588920593, 'val_acc_balanced': 0.760044344480478, 'trainer/global_step': 1869}","{'weights': 'pathological_task', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",iconic-bird-557
360,"{'_step': 151, '_wandb': {'runtime': 381}, 'val_acc': 0.8457183686929757, '_timestamp': 1686306702.485458, 'trainer/global_step': 7099, 'val_acc_rec_balanced': 0.8628298425867091, 'epoch': 9, '_runtime': 380.53255105018616, 'val_loss': 0.3796630799770355, 'train_loss': 0.22562630474567413, 'val_acc_rec': 0.871825876662636, 'val_acc_balanced': 0.8369060490168618}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': False}",legendary-sea-556
361,"{'_wandb': {'runtime': 411}, 'val_acc': 0.756128393976036, '_runtime': 410.79708886146545, 'val_loss': 0.5305731296539307, 'val_acc_balanced': 0.756841420501849, 'trainer/global_step': 7099, '_step': 151, '_timestamp': 1686306126.8977628, 'train_loss': 0.4602844417095184, 'val_acc_rec': 0.7847642079806529, 'val_acc_rec_balanced': 0.7855559855067366, 'epoch': 9}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': False}",upbeat-surf-555
362,"{'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 55}, 'val_acc': 0.6964980544747081, '_timestamp': 1686289675.446805, '_runtime': 56.05696105957031, 'val_loss': 0.6141924262046814, 'val_acc_rec': 0.6964980544747081}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",fluent-water-545
363,"{'val_acc_rec_balanced': 0.4876808480160435, 'epoch': 9, 'val_acc_rec': 0.669260700389105, 'val_acc_balanced': 0.4545698714693128, 'trainer/global_step': 29, 'val_loss': 0.860494077205658, '_timestamp': 1686289530.224898, '_step': 9, '_wandb': {'runtime': 56}, 'val_acc': 0.5926777502652989, '_runtime': 56.69349718093872}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': False}",hardy-shape-544
364,"{'_runtime': 58.934601068496704, 'val_loss': 0.8808112144470215, '_timestamp': 1686289385.713753, 'val_acc_rec': 0.6964980544747081, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 58}, 'val_acc_balanced': 0.4906206456485786, 'trainer/global_step': 29, '_step': 9, 'val_acc': 0.6802263883975946}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",devoted-dust-543
365,"{'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 9, 'val_acc': 0.6964980544747081, 'val_loss': 0.6598994731903076, '_timestamp': 1686289239.7759588, 'val_acc_rec': 0.6964980544747081, 'trainer/global_step': 29, 'epoch': 9, '_wandb': {'runtime': 53}, '_runtime': 53.89013576507568}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",glorious-deluge-542
366,"{'_wandb': {'runtime': 57}, 'val_acc': 0.510611956137248, '_runtime': 58.13269257545471, '_timestamp': 1686289100.7432437, 'val_acc_rec': 0.6303501945525292, 'val_acc_rec_balanced': 0.5103853316143819, '_step': 9, 'epoch': 9, 'val_loss': 0.7363210320472717, 'val_acc_balanced': 0.5207706632287638, 'trainer/global_step': 29}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': False}",grateful-valley-541
367,"{'epoch': 9, '_timestamp': 1686288955.3987782, 'val_acc_rec': 0.5447470817120622, 'val_acc_balanced': 0.5298944537771353, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5176550637444493, '_step': 9, '_wandb': {'runtime': 57}, 'val_acc': 0.5482844004244782, '_runtime': 57.87610411643982, 'val_loss': 1.0352654457092283}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",decent-shape-540
368,"{'_step': 9, '_wandb': {'runtime': 59}, '_runtime': 60.301615715026855, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, 'epoch': 9, 'val_acc': 0.3035019455252918, 'val_loss': 0.7940239906311035, '_timestamp': 1686288809.0138588, 'val_acc_rec': 0.3035019455252918, 'trainer/global_step': 29}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",stellar-water-539
369,"{'val_acc_balanced': 0.552242450287143, 'trainer/global_step': 29, '_wandb': {'runtime': 54}, 'val_acc': 0.5397948355146799, '_timestamp': 1686288661.0844254, 'val_acc_rec': 0.5525291828793775, 'val_acc_rec_balanced': 0.5774960607362842, '_step': 9, 'epoch': 9, '_runtime': 55.18437933921814, 'val_loss': 1.773354411125183}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': False}",wandering-plasma-538
370,"{'val_acc_rec': 0.5797665369649806, 'val_acc_balanced': 0.621946256722793, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.6476865778541756, 'val_acc': 0.5514679872656526, '_runtime': 58.36246490478515, '_wandb': {'runtime': 57}, 'val_loss': 1.1676908731460571, '_timestamp': 1686288520.775159, '_step': 9, 'epoch': 9}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",worthy-cloud-537
371,"{'val_loss': 0.6375473141670227, '_timestamp': 1686288373.6490302, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, '_runtime': 57.36144423484802, 'epoch': 9, '_wandb': {'runtime': 56}, 'val_acc': 0.6964980544747081, 'val_acc_rec': 0.6964980544747081, 'val_acc_rec_balanced': 0.5, '_step': 9}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",twilight-sponge-536
372,"{'val_acc_rec': 0.6770428015564203, 'val_acc_balanced': 0.49740203930148064, 'trainer/global_step': 29, '_step': 9, 'val_acc': 0.6544039617969579, 'val_loss': 2.4242396354675293, '_timestamp': 1686288228.5274503, 'epoch': 9, '_wandb': {'runtime': 52}, '_runtime': 53.15606641769409, 'val_acc_rec_balanced': 0.5077352814782983}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': False}",splendid-sky-535
373,"{'val_acc': 0.6232755571276972, '_runtime': 53.886465549468994, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5493482309124768, 'val_acc_rec': 0.6342412451361867, 'val_acc_balanced': 0.540325363649386, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 53}, 'val_loss': 0.8152694702148438, '_timestamp': 1686288089.9855375}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",divine-silence-534
374,"{'val_acc_rec_balanced': 0.5, '_step': 9, '_wandb': {'runtime': 53}, '_runtime': 54.10767412185669, 'val_loss': 0.615220308303833, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, 'epoch': 9, 'val_acc': 0.6964980544747081, '_timestamp': 1686287948.4862652, 'trainer/global_step': 29}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",glowing-snowball-533
375,"{'trainer/global_step': 29, 'epoch': 9, 'val_acc': 0.6317651220374956, 'val_loss': 1.795228123664856, 'val_acc_rec': 0.6614785992217899, 'val_acc_balanced': 0.4890416845724108, '_step': 9, '_wandb': {'runtime': 54}, '_runtime': 55.23945713043213, '_timestamp': 1686287806.180958, 'val_acc_rec_balanced': 0.5037960177624983}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': False}",exalted-dream-532
376,"{'val_acc': 0.666607711354793, '_runtime': 62.08220553398132, '_timestamp': 1686287664.6648514, 'val_acc_rec_balanced': 0.5374946282767512, 'val_acc_rec': 0.6731517509727627, 'val_acc_balanced': 0.5400307327681629, 'trainer/global_step': 29, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 61}, 'val_loss': 0.6545988321304321}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 50, 'freeze_encoder': True}",genial-moon-531
377,"{'epoch': 9, '_wandb': {'runtime': 59}, 'val_acc': 0.6964980544747081, 'val_loss': 0.625654935836792, '_timestamp': 1686287513.4572568, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, '_step': 10, 'val_acc_rec_balanced': 0.5, 'train_loss': 0.6663552522659302, 'trainer/global_step': 79, '_runtime': 59.64291977882385}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",fanciful-paper-530
378,"{'_runtime': 60.047430753707886, 'val_loss': 1.086645245552063, '_timestamp': 1686287366.746462, 'train_loss': 0.4194684326648712, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.45996275605214154, '_wandb': {'runtime': 59}, 'epoch': 9, 'val_acc': 0.4968164131588256, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.47239585368076986, '_step': 10}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': False}",rosy-leaf-529
379,"{'val_acc_rec_balanced': 0.4777252542615671, '_wandb': {'runtime': 62}, '_runtime': 62.57192039489746, 'val_loss': 0.8368545770645142, 'val_acc_rec': 0.4941634241245136, 'val_acc_balanced': 0.4662932505111276, 'trainer/global_step': 79, '_step': 10, 'epoch': 9, 'val_acc': 0.47205518217191367, '_timestamp': 1686287220.2326293, 'train_loss': 0.5918352603912354}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",sleek-plant-528
380,"{'epoch': 9, 'val_acc': 0.6964980544747081, '_timestamp': 1686287068.5146356, '_step': 10, '_runtime': 52.64831566810608, 'val_loss': 0.6234948039054871, 'train_loss': 0.6470341682434082, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 52}}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",bumbling-energy-527
381,"{'train_loss': 0.3271218240261078, 'trainer/global_step': 79, 'val_acc_rec': 0.6964980544747081, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 56}, 'val_acc': 0.677219667492041, '_runtime': 56.72988438606262, 'val_loss': 1.2284166812896729, '_timestamp': 1686286927.2218273, 'val_acc_balanced': 0.5029300308629917, 'val_acc_rec_balanced': 0.5036169603208709}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': False}",firm-plasma-526
382,"{'_timestamp': 1686286781.0160165, 'val_acc_rec': 0.5797665369649806, 'val_acc_rec_balanced': 0.4993912046984673, 'epoch': 9, 'val_acc': 0.5879023700035373, '_runtime': 60.70989942550659, 'val_loss': 0.7961946725845337, 'train_loss': 0.5217421650886536, 'val_acc_balanced': 0.5141097264002292, 'trainer/global_step': 79, '_step': 10, '_wandb': {'runtime': 60}}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",warm-bee-525
383,"{'epoch': 9, 'val_acc': 0.3035019455252918, 'val_loss': 0.7478950619697571, 'train_loss': 0.7037503123283386, '_step': 10, '_runtime': 57.4127676486969, '_timestamp': 1686286630.8195777, 'val_acc_rec': 0.3035019455252918, 'val_acc_balanced': 0.5, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 56}}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",hearty-universe-524
384,"{'_timestamp': 1686286485.113435, '_step': 10, '_wandb': {'runtime': 55}, 'val_loss': 2.06509780883789, 'train_loss': 0.1680552065372467, 'val_acc_rec': 0.4669260700389105, 'val_acc_balanced': 0.5236160487557135, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5449792293367712, 'epoch': 9, 'val_acc': 0.4614432260346657, '_runtime': 56.42582607269287}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': False}",rich-disco-523
385,"{'train_loss': 0.2904907464981079, 'val_acc_rec': 0.40077821011673154, 'trainer/global_step': 79, '_step': 10, '_wandb': {'runtime': 66}, 'val_acc': 0.45507605235231696, '_runtime': 66.56910467147827, '_timestamp': 1686286340.1422837, 'val_acc_rec_balanced': 0.5191949577424437, 'epoch': 9, 'val_loss': 1.2971476316452026, 'val_acc_balanced': 0.5476520686017893}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",polar-lion-522
386,"{'_step': 10, 'epoch': 9, 'val_acc': 0.6964980544747081, '_runtime': 63.3024468421936, 'val_acc_rec_balanced': 0.5, 'trainer/global_step': 79, '_wandb': {'runtime': 62}, 'val_loss': 0.6553783416748047, '_timestamp': 1686286179.189401, 'train_loss': 0.6559647917747498, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",swift-flower-521
387,"{'val_acc_balanced': 0.5107597244468753, 'trainer/global_step': 79, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 60}, 'val_acc': 0.6107180756986205, 'val_loss': 0.863678514957428, 'train_loss': 0.4696810245513916, '_runtime': 61.03034853935242, '_timestamp': 1686286020.9919815, 'val_acc_rec': 0.6809338521400778, 'val_acc_rec_balanced': 0.5177624982094255}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': False}",fallen-waterfall-520
388,"{'val_acc_rec': 0.5992217898832685, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 62}, 'val_acc': 0.5657941280509374, '_runtime': 63.18884897232056, '_timestamp': 1686285865.889498, 'train_loss': 0.5341833829879761, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5422933677123621, 'val_loss': 0.7296516299247742, 'val_acc_balanced': 0.5187896368063966}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",lunar-bird-519
389,"{'val_loss': 0.7036069631576538, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, 'epoch': 9, '_runtime': 47.670878648757935, 'val_acc': 0.6964980544747081, '_timestamp': 1686285708.1980257, 'train_loss': 0.5840513706207275, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5, '_step': 10, '_wandb': {'runtime': 47}}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",electric-blaze-518
390,"{'train_loss': 0.23389634490013125, 'val_acc_rec': 0.6848249027237354, 'val_acc_balanced': 0.5055833365889232, 'val_acc_rec_balanced': 0.5024709926944564, 'val_loss': 1.954662084579468, 'epoch': 9, '_wandb': {'runtime': 49}, 'val_acc': 0.6763353378139371, '_runtime': 50.47704601287842, '_timestamp': 1686285569.595347, 'trainer/global_step': 79, '_step': 10}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': False}",glorious-water-517
391,"{'val_acc': 0.6570569508312699, '_timestamp': 1686285429.6280885, 'val_acc_rec': 0.6731517509727627, 'val_acc_rec_balanced': 0.606216874373299, 'epoch': 9, '_wandb': {'runtime': 53}, '_runtime': 54.4515745639801, 'val_loss': 0.7485203146934509, 'train_loss': 0.30695098638534546, 'val_acc_balanced': 0.5979509317498144, 'trainer/global_step': 79, '_step': 10}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 20, 'freeze_encoder': True}",misty-snowflake-516
392,"{'_timestamp': 1686285284.3833218, 'train_loss': 0.6714589595794678, 'val_acc_rec': 0.6964980544747081, '_step': 12, 'epoch': 9, 'val_acc': 0.6964980544747081, '_runtime': 52.75746273994446, 'val_loss': 0.6193252801895142, 'val_acc_balanced': 0.5, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 52}}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",faithful-firefly-515
393,"{'_wandb': {'runtime': 51}, 'val_acc': 0.5346657233816767, 'val_loss': 1.5194562673568726, '_timestamp': 1686285140.2390711, 'train_loss': 0.2689226269721985, 'val_acc_rec': 0.5486381322957199, 'val_acc_balanced': 0.4668499563750961, '_step': 12, 'val_acc_rec_balanced': 0.46619395502077066, 'trainer/global_step': 169, '_runtime': 52.497321128845215, 'epoch': 9}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': False}",electric-wildflower-514
394,"{'epoch': 9, '_wandb': {'runtime': 50}, 'val_acc': 0.5313052706048815, '_runtime': 51.13759350776672, '_timestamp': 1686284996.9322536, 'val_acc_rec': 0.5525291828793775, 'val_acc_balanced': 0.4752884452605123, '_step': 12, 'train_loss': 0.4917896091938019, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.476221171751898, 'val_loss': 0.8039490580558777}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",avid-shape-513
395,"{'_step': 12, 'epoch': 9, '_runtime': 49.772427558898926, 'val_loss': 0.6126323938369751, 'val_acc_rec': 0.6964980544747081, '_wandb': {'runtime': 49}, 'val_acc': 0.6964980544747081, '_timestamp': 1686284851.769019, 'train_loss': 0.6385385394096375, 'val_acc_balanced': 0.5, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",hardy-wave-512
396,"{'train_loss': 0.4021429717540741, 'val_acc_rec': 0.688715953307393, 'trainer/global_step': 169, '_step': 12, 'epoch': 9, 'val_loss': 0.9644290208816528, '_timestamp': 1686284709.5122707, 'val_acc_balanced': 0.5239953249729786, 'val_acc_rec_balanced': 0.5305830110299383, '_wandb': {'runtime': 61}, 'val_acc': 0.6802263883975946, '_runtime': 61.80868363380432}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': False}",lucky-monkey-511
397,"{'_step': 12, 'val_acc': 0.6156703218960028, '_timestamp': 1686284555.4025133, 'val_acc_rec': 0.6342412451361867, 'val_acc_rec_balanced': 0.5276464689872511, 'trainer/global_step': 169, 'epoch': 9, '_wandb': {'runtime': 65}, '_runtime': 66.35133028030396, 'val_loss': 0.6916720271110535, 'train_loss': 0.3977886438369751, 'val_acc_balanced': 0.522370785638942}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",stellar-water-510
398,"{'train_loss': 0.6862322092056274, 'val_acc_rec': 0.30739299610894943, 'val_acc_balanced': 0.49428806761208993, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 58}, '_runtime': 59.00759196281433, 'val_loss': 0.6961477994918823, 'val_acc': 0.309515387336399, '_timestamp': 1686284394.209303, 'trainer/global_step': 179, 'val_acc_rec_balanced': 0.4955593754476436}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",lucky-monkey-509
399,"{'val_acc_rec': 0.6186770428015564, 'val_acc_balanced': 0.5651622585980127, 'trainer/global_step': 179, '_wandb': {'runtime': 65}, 'val_acc': 0.5781747435443934, 'val_loss': 1.666297197341919, '_timestamp': 1686284244.9419212, 'val_acc_rec_balanced': 0.5960464116888697, '_step': 12, 'epoch': 9, '_runtime': 66.03747630119324, 'train_loss': 0.09124810993671416}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': False}",bright-moon-508
400,"{'trainer/global_step': 179, 'val_acc_rec_balanced': 0.6590030081650193, '_wandb': {'runtime': 68}, 'val_acc': 0.6103643438273788, '_runtime': 69.2183473110199, 'val_acc_rec': 0.6459143968871596, 'train_loss': 0.38333582878112793, 'val_acc_balanced': 0.6208230782253128, '_step': 12, 'epoch': 9, 'val_loss': 0.8467293381690979, '_timestamp': 1686284086.8758552}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",noble-hill-507
401,"{'_runtime': 62.36981558799744, 'val_acc_rec': 0.6964980544747081, 'val_acc_rec_balanced': 0.5, 'epoch': 9, 'val_acc': 0.6964980544747081, 'val_loss': 0.6375564932823181, '_timestamp': 1686283923.3339205, 'train_loss': 0.6725112199783325, 'val_acc_balanced': 0.5, 'trainer/global_step': 189, '_step': 12, '_wandb': {'runtime': 61}}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",cool-deluge-506
402,"{'val_acc_rec_balanced': 0.5782839134794442, '_runtime': 62.66080188751221, '_timestamp': 1686283763.081952, 'train_loss': 0.3935962617397308, 'val_acc_balanced': 0.5484708494485031, 'val_loss': 0.851610541343689, 'val_acc_rec': 0.6342412451361867, 'trainer/global_step': 189, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 62}, 'val_acc': 0.5521754510081358}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': False}",quiet-glitter-505
403,"{'train_loss': 0.5769843459129333, 'val_acc_rec': 0.6108949416342413, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 52}, 'val_acc': 0.5863105765829502, 'trainer/global_step': 189, 'val_acc_rec_balanced': 0.5940767798309698, '_runtime': 52.47467756271362, 'val_loss': 0.6619542241096497, '_timestamp': 1686283609.1568274, 'val_acc_balanced': 0.5973079527548801}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",golden-moon-504
404,"{'val_acc': 0.6964980544747081, 'train_loss': 0.5123963952064514, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 12, 'epoch': 9, 'val_loss': 0.6151717305183411, '_timestamp': 1686283463.5658455, 'val_acc_rec': 0.6964980544747081, 'trainer/global_step': 159, '_wandb': {'runtime': 61}, '_runtime': 61.795270681381226}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",laced-eon-503
405,"{'_step': 12, 'val_acc': 0.6756278740714539, 'val_acc_balanced': 0.5271060410725216, 'trainer/global_step': 159, 'val_acc_rec_balanced': 0.5317289786563529, 'epoch': 9, '_wandb': {'runtime': 64}, '_runtime': 65.29834747314453, 'val_loss': 1.983522653579712, '_timestamp': 1686283309.8880825, 'train_loss': 0.28614553809165955, 'val_acc_rec': 0.7003891050583657}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': False}",daily-capybara-502
406,"{'_step': 12, 'val_acc': 0.6192076406084188, 'val_loss': 0.7846601009368896, 'train_loss': 0.5375698804855347, 'trainer/global_step': 159, 'val_acc_rec_balanced': 0.4878599054576708, 'epoch': 9, '_wandb': {'runtime': 66}, '_runtime': 66.539377450943, '_timestamp': 1686283154.9967494, 'val_acc_rec': 0.6342412451361867, 'val_acc_balanced': 0.49301513198161245}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 10, 'freeze_encoder': True}",noble-wood-501
407,"{'val_acc_rec': 0.6964980544747081, '_timestamp': 1686282999.2997367, 'epoch': 9, '_wandb': {'runtime': 63}, 'val_acc': 0.6964980544747081, '_runtime': 64.2142448425293, 'val_loss': 0.6146041750907898, 'train_loss': 0.6093069314956665, 'val_acc_balanced': 0.5, '_step': 16, 'val_acc_rec_balanced': 0.5, 'trainer/global_step': 369}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",volcanic-cosmos-500
408,"{'_step': 16, '_runtime': 77.97373962402344, 'train_loss': 0.28809016942977905, 'val_acc_rec': 0.669260700389105, 'val_acc_balanced': 0.489588623666836, 'val_acc_rec_balanced': 0.5057656496203982, 'epoch': 9, '_wandb': {'runtime': 77}, 'val_acc': 0.6284046692607004, 'val_loss': 1.157727837562561, '_timestamp': 1686282845.6752727, 'trainer/global_step': 369}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': False}",giddy-cherry-499
409,"{'val_acc': 0.6597099398655819, 'val_loss': 0.6478677988052368, 'train_loss': 0.4988359808921814, 'val_acc_rec': 0.6614785992217899, 'val_acc_balanced': 0.49101782761000634, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.4820942558372726, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 72}, '_runtime': 72.90056109428406, '_timestamp': 1686282675.4271922}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",decent-firefly-498
410,"{'val_loss': 0.6607042551040649, 'val_acc_rec': 0.6964980544747081, 'val_acc_rec_balanced': 0.5, '_step': 16, '_wandb': {'runtime': 70}, '_runtime': 71.18578886985779, 'train_loss': 0.6706972718238831, 'val_acc_balanced': 0.5, 'trainer/global_step': 369, 'epoch': 9, 'val_acc': 0.6964980544747081, '_timestamp': 1686282509.1635308}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",confused-glade-497
411,"{'_step': 16, 'epoch': 9, '_timestamp': 1686282348.534283, 'val_acc_balanced': 0.5822622442734174, 'val_acc_rec_balanced': 0.608186506231199, '_wandb': {'runtime': 75}, 'val_acc': 0.6457375309515387, '_runtime': 75.4211368560791, 'val_loss': 0.9203536510467528, 'train_loss': 0.33545559644699097, 'val_acc_rec': 0.6809338521400778, 'trainer/global_step': 369}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': False}",daily-water-496
412,"{'_step': 16, 'epoch': 9, '_wandb': {'runtime': 69}, 'train_loss': 0.609486997127533, 'val_acc_balanced': 0.5655203734812674, 'val_acc_rec_balanced': 0.576457527574846, 'val_acc': 0.6689069685178635, '_runtime': 70.29139995574951, 'val_loss': 0.6499302387237549, '_timestamp': 1686282183.166506, 'val_acc_rec': 0.6770428015564203, 'trainer/global_step': 369}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",bumbling-music-495
413,"{'val_acc_balanced': 0.5, 'trainer/global_step': 359, 'val_acc': 0.6964980544747081, '_runtime': 69.91620421409607, 'val_loss': 0.6618689298629761, '_timestamp': 1686282017.6901183, 'train_loss': 0.7046498656272888, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 69}, 'val_acc_rec': 0.6964980544747081, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",clear-oath-494
414,"{'val_acc': 0.5810045985143262, 'val_acc_balanced': 0.5316817726035603, 'trainer/global_step': 359, 'val_acc_rec_balanced': 0.551819223606933, 'val_loss': 1.3888368606567385, '_timestamp': 1686281848.3514268, 'train_loss': 0.17286165058612823, 'val_acc_rec': 0.622568093385214, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 74}, '_runtime': 74.8614559173584}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': False}",laced-mountain-493
415,"{'_wandb': {'runtime': 70}, '_runtime': 70.64277172088623, 'val_acc_balanced': 0.5754075998489406, 'val_loss': 0.7028082013130188, '_timestamp': 1686281682.9374807, 'train_loss': 0.6233062148094177, 'val_acc_rec': 0.5836575875486382, 'trainer/global_step': 359, '_step': 16, 'epoch': 9, 'val_acc': 0.560841881853555, 'val_acc_rec_balanced': 0.5962254691304971}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",feasible-bee-492
416,"{'val_acc_rec_balanced': 0.5, 'epoch': 9, '_runtime': 65.99596309661865, 'val_loss': 0.6360483765602112, 'trainer/global_step': 389, 'train_loss': 0.6122652292251587, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, '_step': 16, '_wandb': {'runtime': 65}, 'val_acc': 0.6964980544747081, '_timestamp': 1686281522.145375}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",true-sun-491
417,"{'epoch': 9, '_wandb': {'runtime': 79}, 'val_acc': 0.6011673151750972, '_runtime': 79.86976671218872, '_timestamp': 1686281364.8642087, 'val_acc_balanced': 0.5316882837832558, 'trainer/global_step': 389, '_step': 16, 'val_loss': 0.849240243434906, 'train_loss': 0.4084542989730835, 'val_acc_rec': 0.642023346303502, 'val_acc_rec_balanced': 0.5006804182781837}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': False}",noble-music-490
418,"{'trainer/global_step': 389, 'val_acc_rec_balanced': 0.5187652198825383, '_step': 16, 'epoch': 9, 'val_acc': 0.6517509727626459, '_runtime': 71.0688169002533, '_timestamp': 1686281192.63619, 'val_acc_rec': 0.642023346303502, '_wandb': {'runtime': 70}, 'val_loss': 0.6033264398574829, 'train_loss': 0.5749255418777466, 'val_acc_balanced': 0.5541909208110325}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",still-silence-489
419,"{'trainer/global_step': 329, '_step': 15, '_wandb': {'runtime': 66}, 'val_acc': 0.6964980544747081, '_timestamp': 1686281029.8336017, 'train_loss': 0.6266536116600037, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_runtime': 66.50676035881042, 'val_loss': 0.6138015985488892}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",fast-shadow-488
420,"{'val_loss': 1.0433170795440674, '_timestamp': 1686280874.7346957, 'val_acc_rec': 0.6614785992217899, 'val_acc_balanced': 0.5487296688414007, 'epoch': 9, '_wandb': {'runtime': 73}, 'val_acc': 0.6480367881146091, '_runtime': 74.23860478401184, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.5508165019338204, '_step': 15, 'train_loss': 0.20020391047000885}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': False}",silver-shadow-487
421,"{'_step': 15, 'epoch': 9, 'train_loss': 0.5987167954444885, 'val_acc_rec': 0.6653696498054474, 'val_acc_balanced': 0.5127863291271112, 'trainer/global_step': 329, '_wandb': {'runtime': 66}, 'val_acc': 0.6430845419172267, '_runtime': 67.04798436164856, 'val_loss': 0.8033463954925537, '_timestamp': 1686280712.031613, 'val_acc_rec_balanced': 0.5210571551353674}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 5, 'freeze_encoder': True}",colorful-aardvark-486
422,"{'val_loss': 0.6264867782592773, '_timestamp': 1686280553.8353894, 'train_loss': 0.6239128708839417, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, '_step': 27, '_wandb': {'runtime': 81}, '_runtime': 81.55248045921326, 'trainer/global_step': 929, 'epoch': 9, 'val_acc': 0.6964980544747081, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",cosmic-butterfly-485
423,"{'_step': 27, '_runtime': 98.14417004585266, 'train_loss': 0.4939170777797699, 'trainer/global_step': 929, 'epoch': 9, '_wandb': {'runtime': 97}, 'val_acc': 0.6740360806508666, 'val_loss': 0.7362217307090759, '_timestamp': 1686280380.1274672, 'val_acc_rec': 0.6809338521400778, 'val_acc_balanced': 0.49489035173392715, 'val_acc_rec_balanced': 0.492443775963329}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': False}",devoted-frog-484
424,"{'val_acc': 0.6811107180756987, 'val_loss': 0.6295452117919922, 'train_loss': 0.6864158511161804, 'val_acc_rec': 0.6731517509727627, 'val_acc_rec_balanced': 0.5049419853889128, '_step': 27, '_wandb': {'runtime': 74}, '_timestamp': 1686280187.3773403, 'val_acc_balanced': 0.5226572775455457, 'trainer/global_step': 929, 'epoch': 9, '_runtime': 74.60014033317566}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",pleasant-shape-483
425,"{'_step': 27, '_wandb': {'runtime': 80}, 'val_acc': 0.6964980544747081, '_runtime': 80.94435214996338, '_timestamp': 1686280015.83992, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, 'epoch': 9, 'val_loss': 0.6384559869766235, 'train_loss': 0.6637194156646729, 'val_acc_rec': 0.6964980544747081, 'trainer/global_step': 939}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",wise-star-482
426,"{'val_acc': 0.6683763707110011, '_timestamp': 1686279840.606161, 'train_loss': 0.5336518883705139, 'val_acc_rec': 0.7276264591439688, 'val_acc_balanced': 0.5962124467711061, 'trainer/global_step': 939, 'val_acc_rec_balanced': 0.6127703767368572, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 97}, '_runtime': 97.60263109207152, 'val_loss': 0.656973123550415}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': False}",chocolate-brook-481
427,"{'_step': 27, '_runtime': 84.28502321243286, 'val_loss': 0.5970751643180847, '_timestamp': 1686279648.3150103, 'train_loss': 0.5649096965789795, 'val_acc_rec': 0.6731517509727627, 'val_acc_balanced': 0.582888945319113, 'trainer/global_step': 939, 'epoch': 9, '_wandb': {'runtime': 83}, 'val_acc': 0.6839405730456314, 'val_acc_rec_balanced': 0.5628133505228478}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",dulcet-fire-480
428,"{'trainer/global_step': 909, 'val_acc_rec_balanced': 0.5, '_step': 27, 'epoch': 9, 'val_loss': 0.6369534134864807, '_timestamp': 1686279467.8290312, 'val_acc_rec': 0.6964980544747081, '_wandb': {'runtime': 82}, 'val_acc': 0.6964980544747081, '_runtime': 82.75387215614319, 'train_loss': 0.6475796103477478, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",floral-smoke-479
429,"{'epoch': 9, 'val_loss': 0.8210739493370056, '_timestamp': 1686279289.628956, 'val_acc_rec': 0.6459143968871596, 'val_acc_balanced': 0.5358912502767251, 'trainer/global_step': 909, '_step': 27, '_wandb': {'runtime': 96}, 'val_acc': 0.6294658648744252, '_runtime': 97.20131015777588, 'train_loss': 0.447102427482605, 'val_acc_rec_balanced': 0.5324093969345366}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': False}",silver-puddle-478
430,"{'val_acc_rec': 0.6575875486381323, 'trainer/global_step': 909, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 85}, 'val_loss': 0.6084483861923218, 'train_loss': 0.5869371891021729, 'val_acc': 0.6496285815351963, '_runtime': 85.46014213562012, '_timestamp': 1686279091.641015, 'val_acc_balanced': 0.6110318917581488, 'val_acc_rec_balanced': 0.5914267296948861}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",lucky-sunset-477
431,"{'_wandb': {'runtime': 77}, 'val_acc': 0.6964980544747081, '_runtime': 77.9834213256836, 'val_loss': 0.6416868567466736, '_timestamp': 1686278899.8037124, 'val_acc_rec': 0.6964980544747081, 'val_acc_rec_balanced': 0.5, '_step': 27, 'epoch': 9, 'train_loss': 0.6708624958992004, 'val_acc_balanced': 0.5, 'trainer/global_step': 929}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",daily-haze-476
432,"{'_wandb': {'runtime': 97}, 'val_acc': 0.6174389812522109, 'val_acc_balanced': 0.5323540519071246, 'trainer/global_step': 929, '_step': 27, '_runtime': 98.23839592933656, 'val_loss': 0.8701034784317017, '_timestamp': 1686278730.0133889, 'train_loss': 0.5534194111824036, 'val_acc_rec': 0.6731517509727627, 'val_acc_rec_balanced': 0.5338776679558802, 'epoch': 9}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': False}",dainty-pond-475
433,"{'epoch': 9, '_wandb': {'runtime': 84}, 'val_acc': 0.6844711708524938, '_runtime': 84.96400213241577, 'trainer/global_step': 929, '_step': 27, '_timestamp': 1686278537.028233, 'train_loss': 0.6216365098953247, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.6357157739839304, 'val_acc_rec_balanced': 0.6338275318722246, 'val_loss': 0.6073663830757141}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",woven-eon-474
434,"{'val_acc_balanced': 0.5, 'trainer/global_step': 889, 'val_acc_rec_balanced': 0.5, 'val_acc': 0.6964980544747081, 'val_loss': 0.627655565738678, '_wandb': {'runtime': 77}, '_runtime': 77.75375270843506, '_timestamp': 1686278355.4413698, 'train_loss': 0.6096175909042358, 'val_acc_rec': 0.6964980544747081, '_step': 26, 'epoch': 9}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",whole-thunder-473
435,"{'val_acc_rec_balanced': 0.5, '_step': 26, '_wandb': {'runtime': 98}, 'val_acc': 0.6964980544747081, 'val_loss': 0.627655565738678, 'val_acc_rec': 0.6964980544747081, 'val_acc_balanced': 0.5, 'trainer/global_step': 889, 'epoch': 9, '_runtime': 98.59154152870178, '_timestamp': 1686278181.4126775, 'train_loss': 0.6096175909042358}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': False}",brisk-bird-472
436,"{'epoch': 9, 'val_acc': 0.655288291475062, '_runtime': 69.98700666427612, '_timestamp': 1686277988.1173947, 'val_acc_rec': 0.6653696498054474, 'trainer/global_step': 889, '_step': 26, 'val_loss': 0.6307096481323242, 'train_loss': 0.5678793787956238, 'val_acc_balanced': 0.5845151124480734, 'val_acc_rec_balanced': 0.5825454805901733, '_wandb': {'runtime': 69}}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 2, 'freeze_encoder': True}",denim-star-471
437,"{'epoch': 9, 'val_acc_balanced': 0.5, 'val_acc_rec': 0.6964980544747081, '_step': 46, '_wandb': {'runtime': 100}, 'val_acc': 0.6964980544747081, '_runtime': 100.31746768951416, 'val_loss': 0.6339190602302551, '_timestamp': 1686277824.7946656, 'train_loss': 0.656554102897644, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",splendid-shadow-470
438,"{'train_loss': 0.5643447637557983, '_step': 46, '_wandb': {'runtime': 138}, 'val_acc': 0.6835868411743898, '_runtime': 138.66017079353333, 'val_loss': 0.6705297231674194, '_timestamp': 1686277623.164884, 'epoch': 9, 'val_acc_rec': 0.6926070038910506, 'val_acc_balanced': 0.5377518198747249, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5369932674401948}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': False}",zany-sun-469
439,"{'_wandb': {'runtime': 103}, 'val_acc': 0.6865935620799434, '_runtime': 104.0258765220642, '_timestamp': 1686277382.4261734, 'trainer/global_step': 1869, 'epoch': 9, 'val_loss': 0.5822679400444031, 'train_loss': 0.650298535823822, 'val_acc_rec': 0.6926070038910506, 'val_acc_balanced': 0.5831493925069344, 'val_acc_rec_balanced': 0.5876307119323879, '_step': 46}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",northern-meadow-468
440,"{'val_acc': 0.6964980544747081, '_runtime': 100.95894813537598, 'val_loss': 0.6340630054473877, 'train_loss': 0.6440013647079468, 'val_acc_rec': 0.6964980544747081, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 100}, '_timestamp': 1686277174.5632772, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",wobbly-butterfly-467
441,"{'_timestamp': 1686276968.7818956, 'val_acc_balanced': 0.5284945501425948, 'trainer/global_step': 1869, 'val_acc': 0.6970286522815705, 'epoch': 9, '_wandb': {'runtime': 137}, '_runtime': 137.7484905719757, 'val_loss': 0.6494975686073303, 'train_loss': 0.5080232620239258, 'val_acc_rec': 0.7042801556420234, 'val_acc_rec_balanced': 0.5309053144248674, '_step': 46}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': False}",swift-firefly-466
442,"{'epoch': 9, '_wandb': {'runtime': 107}, 'val_acc': 0.6897771489211177, '_runtime': 108.17252564430235, 'val_acc_rec': 0.6848249027237354, 'val_acc_balanced': 0.594312810094933, '_step': 46, 'val_loss': 0.5793272852897644, '_timestamp': 1686276728.9159937, 'train_loss': 0.7211564779281616, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5856610800744879}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",cerulean-breeze-465
443,"{'_timestamp': 1686276515.8437164, 'train_loss': 0.6566243767738342, 'val_acc_rec': 0.6964980544747081, 'epoch': 9, '_wandb': {'runtime': 101}, 'val_loss': 0.6340855956077576, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, '_step': 46, 'val_acc': 0.6964980544747081, '_runtime': 101.3563814163208}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",bright-frog-464
444,"{'val_acc': 0.686947293951185, '_runtime': 135.589528799057, 'val_loss': 0.6363959908485413, '_timestamp': 1686276309.4923089, 'train_loss': 0.41856151819229126, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 135}, 'val_acc_rec': 0.6809338521400778, 'trainer/global_step': 1869, 'val_acc_balanced': 0.5229014467841283, 'val_acc_rec_balanced': 0.5069116172468128}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': False}",ethereal-wood-463
445,"{'_wandb': {'runtime': 104}, 'val_acc': 0.6987973116377786, 'val_loss': 0.5843196511268616, '_timestamp': 1686276070.4760523, 'val_acc_rec': 0.708171206225681, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, 'val_acc_rec_balanced': 0.5626342930812205, 'val_acc_balanced': 0.5511371775338256, '_runtime': 104.30159044265748, 'train_loss': 0.6281424760818481}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",super-deluge-462
446,"{'val_acc': 0.6964980544747081, 'val_loss': 0.6335766315460205, '_timestamp': 1686275862.498249, 'val_acc_rec': 0.6964980544747081, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 99}, '_runtime': 99.84168696403503, 'train_loss': 0.6691815257072449, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",smooth-galaxy-461
447,"{'val_acc_balanced': 0.5798791525048509, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 135}, '_timestamp': 1686275658.8581452, 'val_acc_rec': 0.669260700389105, 'val_acc': 0.64379200565971, '_runtime': 135.85974311828613, 'val_loss': 0.6327841877937317, 'train_loss': 0.4873290061950683, 'val_acc_rec_balanced': 0.5817218163586879}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': False}",fast-fog-460
448,"{'_wandb': {'runtime': 108}, 'val_acc': 0.6964980544747081, '_timestamp': 1686275418.665136, 'train_loss': 0.6090126037597656, 'val_acc_balanced': 0.5823680509434699, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, '_runtime': 108.68973517417908, 'val_loss': 0.5777257084846497, 'val_acc_rec': 0.7042801556420234, 'val_acc_rec_balanced': 0.5887766795588024}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",resilient-music-459
449,"{'_runtime': 95.1131148338318, 'val_loss': 0.6341705322265625, 'train_loss': 0.6692391633987427, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 94}, 'val_acc': 0.6964980544747081, '_timestamp': 1686275201.568034, 'val_acc_rec': 0.6964980544747081, '_step': 46}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",light-music-458
450,"{'_timestamp': 1686274992.790856, 'train_loss': 0.6692392826080322, 'val_acc_rec': 0.6964980544747081, '_step': 46, 'val_acc': 0.6964980544747081, '_runtime': 136.1091980934143, 'val_loss': 0.6341707110404968, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 135}}","{'weights': 'random', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': False}",peach-morning-457
451,"{'val_loss': 0.5772644877433777, 'val_acc_balanced': 0.5771639905718118, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, 'val_acc': 0.7032189600282985, 'train_loss': 0.6674293279647827, 'val_acc_rec': 0.7042801556420234, 'val_acc_rec_balanced': 0.5706918779544478, '_wandb': {'runtime': 101}, '_runtime': 101.87586116790771, '_timestamp': 1686274753.8919792}","{'weights': 'eegclip', 'task_name': 'medication', 'train_frac': 1, 'freeze_encoder': True}",faithful-dew-456
452,"{'_step': 9, 'val_acc': 0.46303501945525294, 'val_loss': 1.0418001413345337, 'val_acc_rec': 0.46303501945525294, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 57}, '_runtime': 58.27864360809326, '_timestamp': 1686274550.3198297}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",stilted-butterfly-455
453,"{'val_acc': 0.4651574106827025, '_timestamp': 1686274405.1011052, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.49884301546705634, 'val_acc_balanced': 0.4993730693858571, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 57}, '_runtime': 58.41675615310669, 'val_loss': 4.593939304351807, 'val_acc_rec': 0.46303501945525294}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': False}",rose-resonance-454
454,"{'_step': 9, 'epoch': 9, 'val_loss': 1.2862310409545898, 'val_acc_balanced': 0.48536054738100776, 'val_acc_rec': 0.45136186770428016, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.4839240043843625, '_wandb': {'runtime': 62}, 'val_acc': 0.4531305270604882, '_runtime': 62.50146675109863, '_timestamp': 1686274258.3620818}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",leafy-cloud-453
455,"{'epoch': 9, '_wandb': {'runtime': 55}, '_runtime': 55.91573143005371, 'val_loss': 0.7216654419898987, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, '_step': 9, 'val_acc': 0.5369649805447471, '_timestamp': 1686274106.5907974, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",fine-durian-452
456,"{'epoch': 9, '_wandb': {'runtime': 54}, 'val_loss': 1.6880626678466797, 'val_acc_rec': 0.5252918287937743, 'val_acc_balanced': 0.4990243132826253, '_step': 9, 'val_acc': 0.5288291475061903, '_runtime': 54.83183431625366, '_timestamp': 1686273963.3046513, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.4902874193155523}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': False}",royal-shape-451
457,"{'val_acc_rec_balanced': 0.557179393496529, '_step': 9, 'val_acc': 0.5721613017332862, '_runtime': 61.799299001693726, '_timestamp': 1686273821.675648, 'val_acc_rec': 0.5642023346303502, 'val_acc_balanced': 0.5654056088838697, 'trainer/global_step': 29, 'epoch': 9, '_wandb': {'runtime': 61}, 'val_loss': 1.2011353969573977}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",solar-sound-450
458,"{'val_loss': 0.6993124485015869, 'val_acc_rec': 0.45136186770428016, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.4845024966508343, 'val_acc': 0.45772904138662895, 'epoch': 9, '_wandb': {'runtime': 57}, '_runtime': 57.815367698669434, '_timestamp': 1686273670.9986286, 'val_acc_balanced': 0.4902998748906678, '_step': 9}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",worldly-bush-449
459,"{'_step': 9, 'epoch': 9, 'val_acc': 0.4908029713477184, '_timestamp': 1686273525.2053096, 'val_acc_rec_balanced': 0.4910485933503836, '_wandb': {'runtime': 58}, '_runtime': 58.62316465377808, 'val_loss': 1.1718097925186155, 'val_acc_rec': 0.4708171206225681, 'val_acc_balanced': 0.5126299531670375, 'trainer/global_step': 29}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': False}",earnest-eon-448
460,"{'_wandb': {'runtime': 59}, 'val_acc': 0.5281216837637072, '_runtime': 59.898531436920166, 'val_loss': 0.921229124069214, '_timestamp': 1686273379.3861244, 'trainer/global_step': 29, '_step': 9, 'epoch': 9, 'val_acc_rec_balanced': 0.5175983436853002, 'val_acc_rec': 0.5136186770428015, 'val_acc_balanced': 0.5314973815613202}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",worthy-plant-447
461,"{'_runtime': 52.65154004096985, 'val_acc_rec': 0.5369649805447471, '_wandb': {'runtime': 52}, 'val_acc': 0.5369649805447471, 'val_loss': 0.6989738345146179, '_timestamp': 1686273229.637979, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, '_step': 9, 'epoch': 9}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",misty-sunset-446
462,"{'epoch': 9, 'val_acc': 0.5321896002829855, 'val_loss': 2.6268551349639893, '_timestamp': 1686273089.4911466, 'val_acc_rec': 0.5330739299610895, 'val_acc_rec_balanced': 0.4969553038606747, '_step': 9, '_wandb': {'runtime': 60}, '_runtime': 61.40603446960449, 'val_acc_balanced': 0.49949762513701135, 'trainer/global_step': 29}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': False}",stilted-brook-445
463,"{'_step': 9, 'epoch': 9, '_runtime': 51.64398980140686, '_timestamp': 1686272941.8442917, 'val_acc_rec': 0.5603112840466926, 'val_acc_balanced': 0.5342099290309009, '_wandb': {'runtime': 50}, 'val_acc': 0.5571276972055182, 'val_loss': 0.8025579452514648, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5362014370965777}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",dainty-cloud-444
464,"{'_step': 9, '_wandb': {'runtime': 60}, 'val_acc': 0.5458082773257871, '_runtime': 61.0452721118927, 'val_loss': 0.692903995513916, 'val_acc_rec': 0.5642023346303502, 'epoch': 9, '_timestamp': 1686272798.3231902, 'val_acc_balanced': 0.5239326956078874, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5404031177688466}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",resilient-galaxy-443
465,"{'val_acc_rec_balanced': 0.47074047010108394, '_step': 9, '_runtime': 54.774187088012695, 'val_loss': 1.0858149528503418, 'val_acc_rec': 0.4396887159533074, 'val_acc_balanced': 0.4855598365828545, 'trainer/global_step': 29, 'epoch': 9, '_wandb': {'runtime': 54}, 'val_acc': 0.4609126282278033, '_timestamp': 1686272647.1000311}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': False}",dazzling-serenity-442
466,"{'_runtime': 61.43569612503052, 'val_loss': 0.8396422863006592, '_timestamp': 1686272503.035094, 'val_acc_balanced': 0.466350571849293, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.4679089026915114, '_wandb': {'runtime': 60}, 'epoch': 9, 'val_acc': 0.4706402546869473, 'val_acc_rec': 0.4708171206225681, '_step': 9}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 50, 'freeze_encoder': True}",stoic-mountain-441
467,"{'val_acc_rec': 0.4669260700389105, 'trainer/global_step': 79, '_wandb': {'runtime': 59}, 'val_acc': 0.4605588963565617, '_runtime': 59.84740948677063, 'val_loss': 0.6951947808265686, '_timestamp': 1686272350.1235914, 'train_loss': 0.6917641162872314, 'val_acc_rec_balanced': 0.5036231884057971, '_step': 10, 'epoch': 9, 'val_acc_balanced': 0.49622180888165546}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",sandy-meadow-440
468,"{'val_loss': 1.5635905265808103, '_timestamp': 1686272196.937287, 'train_loss': 0.24795550107955933, 'val_acc_rec': 0.4708171206225681, 'val_acc_balanced': 0.494047619047619, 'trainer/global_step': 79, '_wandb': {'runtime': 61}, 'val_acc': 0.4715245843650513, '_runtime': 62.049546241760254, 'val_acc_rec_balanced': 0.4974120082815735, '_step': 10, 'epoch': 9}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': False}",upbeat-bush-439
469,"{'_step': 10, 'val_acc': 0.4672798019101521, '_runtime': 62.46290850639343, 'val_loss': 0.999459147453308, 'val_acc_rec': 0.46303501945525294, 'val_acc_balanced': 0.4915412805438381, 'epoch': 9, '_wandb': {'runtime': 61}, '_timestamp': 1686272047.1574397, 'train_loss': 0.4982081651687622, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.4866946778711485}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",bright-pond-438
470,"{'epoch': 9, '_wandb': {'runtime': 60}, '_runtime': 60.752840757369995, 'val_loss': 0.699241042137146, '_timestamp': 1686271893.2491128, 'train_loss': 0.6374918222427368, 'val_acc_balanced': 0.5, '_step': 10, 'val_acc': 0.5369649805447471, 'val_acc_rec': 0.5369649805447471, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",grateful-morning-437
471,"{'_step': 10, 'val_acc': 0.5281216837637072, '_runtime': 60.92784166336059, '_timestamp': 1686271744.4013116, 'val_acc_rec': 0.5214007782101168, 'val_acc_rec_balanced': 0.4993910607721349, 'epoch': 9, '_wandb': {'runtime': 60}, 'val_loss': 1.3614028692245483, 'train_loss': 0.26157841086387634, 'val_acc_balanced': 0.5083313957994265, 'trainer/global_step': 79}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': False}",serene-resonance-436
472,"{'val_acc_rec_balanced': 0.5602545365972476, '_step': 10, 'epoch': 9, 'val_acc': 0.5728687654757694, '_runtime': 60.20363163948059, '_timestamp': 1686271595.6701016, 'train_loss': 0.5089617967605591, 'val_acc_rec': 0.5836575875486382, '_wandb': {'runtime': 59}, 'val_loss': 0.8220555186271667, 'val_acc_balanced': 0.5496562261268143, 'trainer/global_step': 79}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",light-planet-435
473,"{'val_acc_rec': 0.4669260700389105, 'val_acc_balanced': 0.49622180888165546, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5036231884057971, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 58}, '_runtime': 58.99276041984558, 'val_acc': 0.4605588963565617, 'val_loss': 0.6937236189842224, '_timestamp': 1686271445.3137145, 'train_loss': 0.6914312243461609}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",helpful-music-434
474,"{'val_acc_rec_balanced': 0.5117525270977956, '_wandb': {'runtime': 62}, 'val_acc': 0.48142907675981605, '_runtime': 62.50266098976135, 'val_loss': 1.4847428798675537, 'train_loss': 0.40207672119140625, 'val_acc_rec': 0.48249027237354086, '_step': 10, 'epoch': 9, '_timestamp': 1686271297.187445, 'val_acc_balanced': 0.5072671361034532, 'trainer/global_step': 79}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': False}",upbeat-aardvark-433
475,"{'_step': 10, 'epoch': 9, '_wandb': {'runtime': 52}, 'val_loss': 0.8728410601615906, '_timestamp': 1686271146.231488, 'val_acc_balanced': 0.4931868004118643, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.47354159054926315, 'val_acc': 0.48514326140785285, '_runtime': 52.948036909103394, 'train_loss': 0.5884363651275635, 'val_acc_rec': 0.4669260700389105}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",fearless-snowball-432
476,"{'val_acc_balanced': 0.5, 'trainer/global_step': 79, 'epoch': 9, '_wandb': {'runtime': 53}, 'val_loss': 0.6920615434646606, 'val_acc_rec': 0.5369649805447471, 'train_loss': 0.686749279499054, 'val_acc_rec_balanced': 0.5, '_step': 10, 'val_acc': 0.5369649805447471, '_runtime': 53.61355018615723, '_timestamp': 1686271003.0980625}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",easy-oath-431
477,"{'val_acc': 0.492041032897064, '_runtime': 59.945929288864136, '_timestamp': 1686270862.7717233, 'train_loss': 0.5532416105270386, 'val_acc_balanced': 0.4809139070648022, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 59}, 'val_loss': 0.9124602675437928, 'val_acc_rec': 0.5136186770428015, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.48520277676287904}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': False}",prime-valley-430
478,"{'_runtime': 53.5397412776947, 'val_loss': 0.8030380010604858, 'val_acc_rec': 0.5097276264591439, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 53}, 'val_acc': 0.5153873363990096, '_timestamp': 1686270715.2567382, 'train_loss': 0.5896481871604919, 'val_acc_balanced': 0.5045725800201504, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5000913408841798}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",still-violet-429
479,"{'_runtime': 60.590583086013794, 'val_loss': 0.7004523873329163, 'val_acc_balanced': 0.49848041983591856, 'val_acc_rec_balanced': 0.5036231884057971, '_wandb': {'runtime': 60}, 'epoch': 9, 'val_acc': 0.4625044216483905, '_timestamp': 1686270571.0741992, 'train_loss': 0.6882482767105103, 'val_acc_rec': 0.4669260700389105, 'trainer/global_step': 79, '_step': 10}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",sandy-planet-428
480,"{'_step': 10, '_wandb': {'runtime': 57}, 'val_acc': 0.4527767951892466, '_runtime': 58.131017208099365, 'val_loss': 1.302960753440857, 'train_loss': 0.4423796832561493, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.45161977834612105, 'epoch': 9, '_timestamp': 1686270423.1129825, 'val_acc_rec': 0.42412451361867703, 'val_acc_balanced': 0.4795880802914051}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': False}",laced-snowflake-427
481,"{'_step': 10, '_wandb': {'runtime': 60}, 'val_acc': 0.4678103997170145, 'val_loss': 0.8124988675117493, '_timestamp': 1686270276.7996895, 'trainer/global_step': 79, 'epoch': 9, '_runtime': 60.73229646682739, 'train_loss': 0.657322883605957, 'val_acc_rec': 0.4863813229571984, 'val_acc_balanced': 0.46510916619612275, 'val_acc_rec_balanced': 0.481823164048228}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 20, 'freeze_encoder': True}",soft-flower-426
482,"{'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 59}, '_runtime': 59.4425151348114, 'val_acc_rec': 0.5369649805447471, 'val_loss': 0.6917927265167236, '_timestamp': 1686270125.2189302, 'train_loss': 0.6954318881034851, 'val_acc_balanced': 0.5, 'trainer/global_step': 169, '_step': 12, 'epoch': 9, 'val_acc': 0.5369649805447471}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",morning-moon-425
483,"{'_step': 12, '_wandb': {'runtime': 66}, 'val_acc': 0.4936328263176512, '_timestamp': 1686269977.1504483, 'val_acc_rec_balanced': 0.4978687127024723, 'epoch': 9, '_runtime': 66.7489264011383, 'val_loss': 1.6393057107925415, 'train_loss': 0.20779627561569217, 'val_acc_rec': 0.48249027237354086, 'val_acc_balanced': 0.5057724670896027, 'trainer/global_step': 169}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': False}",dauntless-sun-424
484,"{'train_loss': 0.6364794373512268, 'epoch': 9, '_wandb': {'runtime': 60}, 'val_acc': 0.5054828440042448, '_runtime': 61.25327110290527, 'val_loss': 0.794218122959137, '_timestamp': 1686269822.645889, '_step': 12, 'val_acc_rec': 0.4980544747081712, 'val_acc_balanced': 0.5173326247495045, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.5094690049933017}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",sandy-capybara-423
485,"{'val_loss': 0.691325843334198, 'train_loss': 0.6881512403488159, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, 'epoch': 9, 'val_acc': 0.5369649805447471, '_runtime': 62.97544455528259, 'val_acc_rec': 0.5369649805447471, 'trainer/global_step': 169, '_step': 12, '_wandb': {'runtime': 62}, '_timestamp': 1686269670.8864317}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",rich-frog-422
486,"{'_timestamp': 1686269522.0552213, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 70}, '_runtime': 70.68252229690552, 'val_loss': 0.6914132237434387, 'train_loss': 0.6853359341621399, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, 'trainer/global_step': 169, '_step': 12, 'val_acc': 0.5369649805447471}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': False}",honest-firefly-421
487,"{'val_loss': 0.8388294577598572, '_timestamp': 1686269363.4119718, 'val_acc_rec': 0.5758754863813229, 'epoch': 9, '_wandb': {'runtime': 65}, 'val_acc': 0.5541209762999646, '_runtime': 65.62473392486572, 'train_loss': 0.4517034888267517, 'val_acc_balanced': 0.5400612814295679, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.5576360979174279, '_step': 12}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",restful-dawn-420
488,"{'_wandb': {'runtime': 60}, '_runtime': 61.31590723991394, '_timestamp': 1686269207.3398972, 'val_acc_rec': 0.46303501945525294, 'trainer/global_step': 179, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 12, 'epoch': 9, 'val_acc': 0.46303501945525294, 'val_loss': 0.7111417651176453, 'train_loss': 0.6776608824729919}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",woven-armadillo-419
489,"{'val_acc': 0.4803678811460913, 'val_loss': 1.6439911127090454, '_timestamp': 1686269053.678337, 'val_acc_rec': 0.48249027237354086, 'val_acc_balanced': 0.5044909268055048, 'trainer/global_step': 179, '_wandb': {'runtime': 63}, 'epoch': 9, '_runtime': 63.97199511528015, 'train_loss': 0.24356894195079803, 'val_acc_rec_balanced': 0.5088600657654366, '_step': 12}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': False}",wobbly-disco-418
490,"{'_step': 12, '_wandb': {'runtime': 67}, 'val_loss': 0.8307515382766724, '_timestamp': 1686268900.723379, 'train_loss': 0.547988772392273, 'trainer/global_step': 179, 'epoch': 9, 'val_acc': 0.48921117792713126, '_runtime': 68.04311180114746, 'val_acc_rec': 0.490272373540856, 'val_acc_balanced': 0.5022336998040322, 'val_acc_rec_balanced': 0.5022226281817075}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",worldly-waterfall-417
491,"{'_step': 12, 'epoch': 9, '_runtime': 61.31509113311768, 'val_loss': 0.700261652469635, '_timestamp': 1686268741.803445, 'train_loss': 0.6729968190193176, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 60}, 'val_acc': 0.5369649805447471, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, 'trainer/global_step': 189}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",noble-feather-416
492,"{'_wandb': {'runtime': 62}, 'val_acc': 0.5160948001414928, 'val_loss': 1.270811915397644, 'val_acc_rec_balanced': 0.4836195347704299, 'val_acc_rec': 0.5175097276264592, 'val_acc_balanced': 0.49140011735919664, 'trainer/global_step': 189, '_step': 12, 'epoch': 9, '_runtime': 63.20293951034546, '_timestamp': 1686268589.7835064, 'train_loss': 0.36643996834754944}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': False}",radiant-paper-415
493,"{'_runtime': 66.10438704490662, '_timestamp': 1686268438.016556, 'train_loss': 0.6005511283874512, 'trainer/global_step': 189, 'val_acc_rec_balanced': 0.5076421873097064, 'epoch': 9, '_wandb': {'runtime': 65}, 'val_loss': 0.790665328502655, 'val_acc_rec': 0.5252918287937743, 'val_acc_balanced': 0.5035027845130147, '_step': 12, 'val_acc': 0.5196321188539087}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",happy-pyramid-414
494,"{'_wandb': {'runtime': 66}, 'val_acc': 0.5369649805447471, '_runtime': 71.7987871170044, 'val_loss': 0.6904728412628174, '_timestamp': 1686268278.474174, 'trainer/global_step': 159, '_step': 12, 'epoch': 9, 'train_loss': 0.6917235255241394, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",amber-totem-413
495,"{'val_acc_rec': 0.5175097276264592, 'val_acc_balanced': 0.49545233113008047, 'trainer/global_step': 159, 'val_acc_rec_balanced': 0.5137011326269638, '_runtime': 67.28464031219482, 'epoch': 9, '_wandb': {'runtime': 66}, 'val_acc': 0.497347010965688, 'val_loss': 1.3181108236312866, '_timestamp': 1686268113.5719583, 'train_loss': 0.26968637108802795, '_step': 12}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': False}",rich-tree-412
496,"{'_wandb': {'runtime': 66}, '_runtime': 67.16268038749695, 'val_acc_balanced': 0.500703048017626, 'val_acc_rec_balanced': 0.5055717939349653, 'val_acc_rec': 0.5019455252918288, 'trainer/global_step': 159, '_step': 12, 'epoch': 9, 'val_acc': 0.4931022285107888, 'val_loss': 0.800719141960144, '_timestamp': 1686267956.1097145, 'train_loss': 0.6229553818702698}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 10, 'freeze_encoder': True}",good-planet-411
497,"{'val_acc_balanced': 0.5, '_step': 16, 'val_acc': 0.5369649805447471, '_runtime': 68.16815710067749, 'val_loss': 0.6924805641174316, '_timestamp': 1686267799.693758, 'train_loss': 0.6936478018760681, 'val_acc_rec': 0.5369649805447471, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 67}}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",eager-flower-410
498,"{'val_acc': 0.5021223912274496, '_runtime': 77.38594675064087, 'val_loss': 1.0041587352752686, '_timestamp': 1686267640.8303816, 'train_loss': 0.3681447207927704, 'trainer/global_step': 369, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 77}, 'val_acc_rec': 0.490272373540856, 'val_acc_balanced': 0.5021340552031088, 'val_acc_rec_balanced': 0.48891730605285594}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': False}",earthy-firebrand-409
499,"{'val_loss': 0.7869381308555603, 'train_loss': 0.5823816061019897, 'val_acc_rec': 0.4396887159533074, 'val_acc_balanced': 0.4713950797710389, '_step': 16, '_wandb': {'runtime': 65}, 'val_acc': 0.45843650512911216, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.45338570210692974, 'epoch': 9, '_runtime': 66.14147901535034, '_timestamp': 1686267474.035897}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",happy-firefly-408
500,"{'_step': 16, 'epoch': 9, 'val_loss': 0.6926370859146118, '_timestamp': 1686267317.3421907, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, '_wandb': {'runtime': 67}, 'val_acc': 0.5369649805447471, '_runtime': 67.66159582138062, 'train_loss': 0.6929469704627991, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",leafy-moon-407
501,"{'_runtime': 74.20926022529602, 'val_loss': 0.6926368474960327, 'train_loss': 0.69294673204422, 'val_acc_rec_balanced': 0.5, '_step': 16, 'epoch': 9, '_timestamp': 1686267161.8669932, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, 'trainer/global_step': 369, '_wandb': {'runtime': 73}, 'val_acc': 0.5369649805447471}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': False}",polished-sea-406
502,"{'train_loss': 0.6780027747154236, 'trainer/global_step': 369, 'epoch': 9, 'val_acc': 0.5212239122744959, '_runtime': 73.69073987007141, 'val_loss': 0.7449001669883728, 'val_acc_balanced': 0.5215246177522392, 'val_acc_rec_balanced': 0.5038363171355499, '_step': 16, '_wandb': {'runtime': 73}, '_timestamp': 1686266998.278714, 'val_acc_rec': 0.5019455252918288}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",fluent-rain-405
503,"{'_runtime': 64.87477779388428, 'val_loss': 0.6932283639907837, '_timestamp': 1686266831.7321217, 'val_acc_rec': 0.46303501945525294, 'epoch': 9, 'val_acc': 0.4637424831977361, 'train_loss': 0.6935033798217773, 'val_acc_balanced': 0.4999487937467477, 'trainer/global_step': 359, 'val_acc_rec_balanced': 0.5, '_step': 16, '_wandb': {'runtime': 64}}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",apricot-feather-404
504,"{'val_acc_rec': 0.5408560311284046, 'val_acc_balanced': 0.5209779563999513, 'trainer/global_step': 359, 'epoch': 9, 'val_acc': 0.527414220021224, '_runtime': 74.88117980957031, '_timestamp': 1686266675.365903, 'val_acc_rec_balanced': 0.533704786262331, '_step': 16, '_wandb': {'runtime': 74}, 'val_loss': 1.243701457977295, 'train_loss': 0.30875834822654724}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': False}",sandy-resonance-403
505,"{'_timestamp': 1686266510.1107433, 'train_loss': 0.6260198950767517, 'trainer/global_step': 359, 'val_loss': 0.7415304780006409, 'epoch': 9, '_wandb': {'runtime': 55}, 'val_acc': 0.5164485320127343, '_runtime': 55.87068819999695, 'val_acc_rec': 0.5330739299610895, 'val_acc_balanced': 0.5157632222849614, 'val_acc_rec_balanced': 0.5334003166483985, '_step': 16}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",sage-forest-402
506,"{'val_acc': 0.5369649805447471, '_runtime': 67.56624627113342, 'val_loss': 0.6907410025596619, 'train_loss': 0.6918807625770569, 'val_acc_balanced': 0.5, 'trainer/global_step': 389, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 67}, '_timestamp': 1686266361.3765252, 'val_acc_rec': 0.5369649805447471, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",snowy-elevator-401
507,"{'_timestamp': 1686266203.5847125, 'val_acc_balanced': 0.5156608097784569, 'trainer/global_step': 389, 'val_acc_rec_balanced': 0.5343137254901961, '_step': 16, 'epoch': 9, '_runtime': 75.69818925857544, 'val_loss': 0.9195619821548462, '_wandb': {'runtime': 75}, 'val_acc': 0.5245843650512911, 'train_loss': 0.4867468476295471, 'val_acc_rec': 0.556420233463035}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': False}",pleasant-planet-400
508,"{'epoch': 9, '_wandb': {'runtime': 60}, 'val_acc': 0.4840820657941281, 'val_loss': 0.7725383043289185, 'val_acc_rec': 0.48249027237354086, 'trainer/global_step': 389, '_step': 16, '_timestamp': 1686266037.9137545, 'train_loss': 0.5517354011535645, 'val_acc_balanced': 0.4773682200152789, 'val_acc_rec_balanced': 0.4758860065765437, '_runtime': 61.217424392700195}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",warm-silence-399
509,"{'val_acc': 0.5369649805447471, 'val_loss': 0.6908031702041626, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, '_step': 15, 'epoch': 9, '_wandb': {'runtime': 66}, '_runtime': 66.92013311386108, '_timestamp': 1686265885.204015, 'train_loss': 0.6919722557067871, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",eternal-leaf-398
510,"{'val_acc': 0.5217545100813583, 'val_loss': 1.022524356842041, '_timestamp': 1686265729.363355, 'train_loss': 0.2938414216041565, 'val_acc_rec': 0.5408560311284046, 'val_acc_balanced': 0.5163652417488734, 'epoch': 9, '_wandb': {'runtime': 73}, 'val_acc_rec_balanced': 0.5325478017293874, 'trainer/global_step': 329, '_step': 15, '_runtime': 74.13850688934326}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': False}",curious-sea-397
511,"{'val_acc_rec_balanced': 0.541529655340397, 'epoch': 9, '_wandb': {'runtime': 59}, '_runtime': 59.87672019004822, '_timestamp': 1686265565.2507732, 'train_loss': 0.6259040832519531, 'val_acc_balanced': 0.5249236058059588, 'trainer/global_step': 329, '_step': 15, 'val_acc': 0.5314821365405022, 'val_loss': 0.7518794536590576, 'val_acc_rec': 0.5486381322957199}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 5, 'freeze_encoder': True}",ethereal-frost-396
512,"{'val_acc_rec_balanced': 0.5, '_step': 27, '_runtime': 80.10003972053528, 'val_loss': 0.6972426772117615, 'train_loss': 0.6924960613250732, 'val_acc_rec': 0.46303501945525294, 'trainer/global_step': 929, 'epoch': 9, '_wandb': {'runtime': 79}, 'val_acc': 0.46303501945525294, '_timestamp': 1686265413.3425667, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",flowing-thunder-395
513,"{'val_loss': 0.7587658166885376, '_timestamp': 1686265239.359754, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.4981731823164048, 'val_acc_rec_balanced': 0.49415418341249545, '_step': 27, '_runtime': 97.7453510761261, 'val_acc': 0.5070746374248319, 'train_loss': 0.5440535545349121, 'trainer/global_step': 929, 'epoch': 9, '_wandb': {'runtime': 97}}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': False}",sweet-wave-394
514,"{'val_acc_rec': 0.47470817120622566, 'val_acc_rec_balanced': 0.5016136889538424, '_step': 27, '_wandb': {'runtime': 82}, 'val_acc': 0.4646268128758401, '_runtime': 82.4700140953064, 'val_loss': 0.7289329767227173, 'train_loss': 0.6871124505996704, 'epoch': 9, '_timestamp': 1686265047.68769, 'val_acc_balanced': 0.49170043511475736, 'trainer/global_step': 929}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",effortless-donkey-393
515,"{'val_acc': 0.5369649805447471, 'val_loss': 0.6928623914718628, '_timestamp': 1686264867.247784, 'val_acc_balanced': 0.5, 'val_acc_rec': 0.5369649805447471, 'trainer/global_step': 939, 'val_acc_rec_balanced': 0.5, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 81}, '_runtime': 81.75799798965454, 'train_loss': 0.6920425295829773}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",prime-serenity-392
516,"{'_wandb': {'runtime': 99}, 'val_acc': 0.5369649805447471, '_runtime': 99.30013346672058, 'val_loss': 0.6928624510765076, 'train_loss': 0.6920427083969116, 'trainer/global_step': 939, 'val_acc_rec_balanced': 0.5, '_step': 27, 'epoch': 9, '_timestamp': 1686264688.4772725, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': False}",rich-violet-391
517,"{'train_loss': 0.7126936316490173, 'val_acc_rec': 0.4980544747081712, 'trainer/global_step': 939, 'epoch': 9, '_wandb': {'runtime': 78}, '_runtime': 78.65095257759094, '_timestamp': 1686264493.1451545, 'val_acc_balanced': 0.4961636828644501, 'val_acc_rec_balanced': 0.49327122153209113, '_step': 27, 'val_acc': 0.5014149274849664, 'val_loss': 0.7113850116729736}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",northern-grass-390
518,"{'_runtime': 80.6419608592987, 'val_loss': 0.6909440755844116, '_timestamp': 1686264317.042615, 'train_loss': 0.6927096843719482, 'val_acc_rec': 0.5369649805447471, 'trainer/global_step': 909, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 80}, 'val_acc': 0.5369649805447471, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",splendid-thunder-389
519,"{'val_acc': 0.542270958613371, '_runtime': 97.76740431785583, 'val_loss': 0.9905314445495604, 'train_loss': 0.4147360324859619, 'trainer/global_step': 909, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 97}, 'val_acc_rec_balanced': 0.553434417245159, '_timestamp': 1686264139.8700483, 'val_acc_rec': 0.5719844357976653, 'val_acc_balanced': 0.5281592874303872}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': False}",laced-river-388
520,"{'val_loss': 0.7506362199783325, '_timestamp': 1686263948.6583235, 'train_loss': 0.673119843006134, 'val_acc_rec': 0.4941634241245136, 'trainer/global_step': 909, 'val_acc_rec_balanced': 0.4902265253927658, 'epoch': 9, '_wandb': {'runtime': 83}, 'val_acc': 0.5065440396179696, '_runtime': 83.9906415939331, 'val_acc_balanced': 0.5007819333266904, '_step': 27}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",deep-glitter-387
521,"{'_timestamp': 1686263767.5995677, 'train_loss': 0.6965538263320923, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, 'val_loss': 0.6905564069747925, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.5, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 80}, 'val_acc': 0.5369649805447471, '_runtime': 80.39912056922913}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",lunar-pine-386
522,"{'epoch': 9, 'val_acc': 0.516802263883976, 'val_loss': 0.8462964296340942, 'train_loss': 0.6090246438980103, 'val_acc_balanced': 0.4991059664972709, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.5003958104981123, '_step': 27, '_wandb': {'runtime': 95}, '_runtime': 95.43570399284364, '_timestamp': 1686263593.113757, 'val_acc_rec': 0.5175097276264592}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': False}",cosmic-resonance-385
523,"{'val_acc': 0.5169791298195967, '_runtime': 81.37705564498901, 'val_loss': 0.7177146673202515, '_timestamp': 1686263402.1351676, 'val_acc_rec': 0.4980544747081712, 'val_acc_balanced': 0.506791056343486, '_wandb': {'runtime': 81}, 'epoch': 9, 'train_loss': 0.6429541707038879, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.4857508220679576, '_step': 27}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",fresh-bee-384
524,"{'_step': 26, '_timestamp': 1686263223.0705934, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, 'trainer/global_step': 889, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 79}, 'val_acc': 0.5369649805447471, '_runtime': 80.24615335464478, 'val_loss': 0.6914551854133606, 'train_loss': 0.6917787194252014}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",clean-microwave-383
525,"{'_step': 26, 'epoch': 9, '_wandb': {'runtime': 96}, '_runtime': 96.37326073646544, 'train_loss': 0.5035983920097351, 'val_acc_balanced': 0.4937043987555496, 'trainer/global_step': 889, 'val_acc': 0.5010611956137248, 'val_loss': 0.9845016002655028, '_timestamp': 1686263047.3517098, 'val_acc_rec': 0.5175097276264592, 'val_acc_rec_balanced': 0.5096516867616612}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': False}",fast-mountain-382
526,"{'trainer/global_step': 889, 'val_acc_rec_balanced': 0.5536779929363049, 'val_loss': 0.7037403583526611, '_timestamp': 1686262856.6810355, 'train_loss': 0.6818315982818604, 'val_acc_rec': 0.5486381322957199, '_runtime': 76.023113489151, 'val_acc_balanced': 0.5397664994851696, '_step': 26, 'epoch': 9, '_wandb': {'runtime': 75}, 'val_acc': 0.5378493102228511}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 2, 'freeze_encoder': True}",denim-sun-381
527,"{'epoch': 9, '_wandb': {'runtime': 101}, 'train_loss': 0.6950967907905579, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, '_step': 46, 'val_acc': 0.5369649805447471, '_runtime': 101.32689666748048, 'val_loss': 0.6910391449928284, '_timestamp': 1686262684.0256076, 'val_acc_rec': 0.5369649805447471, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",vital-dragon-380
528,"{'train_loss': 0.6361835598945618, 'val_acc_rec_balanced': 0.5157715260017051, 'epoch': 9, '_wandb': {'runtime': 139}, 'val_loss': 0.7417570948600769, '_timestamp': 1686262475.6708107, 'val_acc_rec': 0.5408560311284046, 'val_acc_balanced': 0.5096918213925886, 'trainer/global_step': 1869, '_step': 46, 'val_acc': 0.5313052706048815, '_runtime': 139.39201259613037}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': False}",fanciful-totem-379
529,"{'val_acc_rec': 0.5369649805447471, 'val_acc_rec_balanced': 0.5023139690658872, 'epoch': 9, '_runtime': 103.99188446998596, '_timestamp': 1686262233.5640914, 'val_loss': 0.711550772190094, 'train_loss': 0.6867206692695618, 'val_acc_balanced': 0.499296951982374, 'trainer/global_step': 1869, '_step': 46, '_wandb': {'runtime': 103}, 'val_acc': 0.5337813937035727}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",upbeat-brook-378
530,"{'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, '_step': 46, 'epoch': 9, '_runtime': 97.66486740112305, '_timestamp': 1686262021.5655165, 'train_loss': 0.692678689956665, '_wandb': {'runtime': 97}, 'val_acc': 0.5369649805447471, 'val_loss': 0.691074788570404, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",jolly-butterfly-377
531,"{'_timestamp': 1686261802.1661031, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_runtime': 134.46516013145447, 'val_loss': 0.6910742521286011, 'train_loss': 0.6926788091659546, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, '_step': 46, '_wandb': {'runtime': 134}, 'val_acc': 0.5369649805447471}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': False}",youthful-sky-376
532,"{'val_acc_rec_balanced': 0.5358360735598587, '_step': 46, 'epoch': 9, 'val_acc': 0.5116731517509727, '_timestamp': 1686261562.3720653, 'val_acc_rec': 0.5214007782101168, 'val_acc_balanced': 0.5305383022774327, 'trainer/global_step': 1869, '_wandb': {'runtime': 104}, '_runtime': 104.28926229476927, 'val_loss': 0.7007747292518616, 'train_loss': 0.6675536632537842}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",astral-disco-375
533,"{'val_acc_balanced': 0.5, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 100}, 'val_acc': 0.5369649805447471, 'train_loss': 0.6926944851875305, '_runtime': 101.07083439826964, 'val_loss': 0.6910004615783691, '_timestamp': 1686261350.2614703, 'val_acc_rec': 0.5369649805447471, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",chocolate-dust-374
534,"{'_runtime': 135.10054421424866, 'val_loss': 0.6910011172294617, 'train_loss': 0.6926941275596619, 'val_acc_rec_balanced': 0.5, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 134}, 'val_acc': 0.5369649805447471, '_timestamp': 1686261143.377926, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': False}",hearty-brook-373
535,"{'_step': 46, 'val_acc': 0.48585072515033606, '_runtime': 104.51238012313844, 'val_loss': 0.7188992500305176, 'train_loss': 0.6975281238555908, 'trainer/global_step': 1869, 'epoch': 9, '_wandb': {'runtime': 104}, '_timestamp': 1686260905.2132971, 'val_acc_rec': 0.4785992217898833, 'val_acc_balanced': 0.4996567797079306, 'val_acc_rec_balanced': 0.4942455242966752}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",vibrant-aardvark-372
536,"{'val_acc_balanced': 0.5, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 101}, 'val_acc': 0.5369649805447471, '_runtime': 101.2803144454956, 'train_loss': 0.6926952600479126, 'val_acc_rec': 0.5369649805447471, 'val_acc_rec_balanced': 0.5, 'val_loss': 0.6909974813461304, '_timestamp': 1686260697.9866214, 'trainer/global_step': 1869}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",fresh-shape-371
537,"{'_timestamp': 1686260491.8490548, 'train_loss': 0.6926953196525574, 'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, '_runtime': 134.95142579078674, 'epoch': 9, '_wandb': {'runtime': 135}, 'val_acc': 0.5369649805447471, 'val_loss': 0.6909974813461304, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, '_step': 46}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': False}",zany-donkey-370
538,"{'_wandb': {'runtime': 107}, '_timestamp': 1686260253.9054224, 'train_loss': 0.6938138604164124, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, 'val_acc': 0.5238769013088079, '_runtime': 107.7847135066986, 'val_loss': 0.7148293256759644, 'val_acc_rec': 0.5486381322957199, 'val_acc_balanced': 0.5339871126316139, 'val_acc_rec_balanced': 0.5484715625380587}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",flowing-yogurt-369
539,"{'val_acc_rec': 0.5369649805447471, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, 'epoch': 9, 'val_acc': 0.5369649805447471, '_timestamp': 1686260037.635331, 'train_loss': 0.6939694285392761, '_step': 46, '_wandb': {'runtime': 103}, '_runtime': 103.70097970962524, 'val_loss': 0.6909712553024292}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",ruby-valley-368
540,"{'epoch': 9, 'val_acc': 0.5316590024761231, '_timestamp': 1686259830.452734, 'train_loss': 0.6353899240493774, 'val_acc_balanced': 0.5052091983038275, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.497229326513214, '_step': 46, '_runtime': 137.30391907691956, 'val_loss': 0.7438102960586548, 'val_acc_rec': 0.5252918287937743, '_wandb': {'runtime': 137}}","{'weights': 'random', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': False}",unique-night-367
541,"{'trainer/global_step': 1869, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 109}, 'val_acc': 0.5005305978068624, '_runtime': 109.67437672615053, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5193213649096002, 'val_loss': 0.7131699919700623, '_timestamp': 1686259590.459715, 'train_loss': 0.6536719799041748, 'val_acc_rec_balanced': 0.5232310315430521}","{'weights': 'eegclip', 'task_name': 'seizure', 'train_frac': 1, 'freeze_encoder': True}",deft-darkness-366
542,"{'val_acc': 0.4046692607003891, '_runtime': 59.08186411857605, 'val_loss': 1.0152662992477417, '_timestamp': 1686259376.2639692, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, '_wandb': {'runtime': 58}, 'epoch': 9, 'val_acc_rec': 0.4046692607003891, 'val_acc_rec_balanced': 0.5, '_step': 9}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",fearless-fire-365
543,"{'_runtime': 60.98386979103088, 'val_loss': 1.8930394649505615, '_timestamp': 1686259226.6332808, 'epoch': 9, 'val_acc': 0.4043155288291475, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.4988630650395357, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, '_step': 9, '_wandb': {'runtime': 60}}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': False}",dauntless-oath-364
544,"{'_runtime': 54.85683822631836, '_timestamp': 1686259079.3447332, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.4994229626582567, 'val_acc_rec_balanced': 0.5, '_step': 9, '_wandb': {'runtime': 54}, 'val_loss': 1.1036466360092163, 'trainer/global_step': 29, 'epoch': 9, 'val_acc': 0.4043155288291475}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",radiant-moon-363
545,"{'_step': 9, '_wandb': {'runtime': 57}, '_runtime': 58.38233065605163, 'trainer/global_step': 29, 'epoch': 9, 'val_acc': 0.4046692607003891, 'val_loss': 0.76560378074646, '_timestamp': 1686258936.0746646, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",dashing-cosmos-362
546,"{'val_acc': 0.4457021577644146, '_runtime': 58.02774262428284, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5169054801407742, 'epoch': 9, '_wandb': {'runtime': 57}, 'val_loss': 1.0502971410751345, '_timestamp': 1686258790.9933827, 'val_acc_rec': 0.4357976653696498, 'val_acc_balanced': 0.5041578111431052, '_step': 9}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': False}",atomic-sea-361
547,"{'_step': 9, 'epoch': 9, '_wandb': {'runtime': 58}, 'val_acc': 0.4112133003183587, 'val_loss': 1.087169528007507, '_timestamp': 1686258645.6932402, 'val_acc_rec': 0.40077821011673154, 'trainer/global_step': 29, '_runtime': 59.29034209251404, 'val_acc_balanced': 0.4736519607843137, 'val_acc_rec_balanced': 0.46747737556561086}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",sweet-terrain-360
548,"{'val_loss': 0.7466523051261902, '_timestamp': 1686258497.1233177, 'val_acc_balanced': 0.5000257095845331, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 57}, '_runtime': 57.738120555877686, 'val_acc_rec': 0.4046692607003891, '_step': 9, 'val_acc': 0.4051998585072515}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",absurd-armadillo-359
549,"{'_wandb': {'runtime': 59}, 'val_acc': 0.4105058365758754, '_runtime': 59.68741798400879, '_timestamp': 1686258351.438144, 'val_acc_rec_balanced': 0.49076168929110103, '_step': 9, 'epoch': 9, 'val_loss': 2.6437089443206787, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.4931441107911696, 'trainer/global_step': 29}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': False}",youthful-snow-358
550,"{'trainer/global_step': 29, '_step': 9, 'epoch': 9, 'val_acc': 0.4642730810045985, 'val_loss': 0.8375782370567322, 'val_acc_balanced': 0.4895904748845925, '_wandb': {'runtime': 62}, '_runtime': 63.092482805252075, '_timestamp': 1686258204.2041907, 'val_acc_rec': 0.4474708171206226, 'val_acc_rec_balanced': 0.4820575666163901}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",rosy-lion-357
551,"{'val_acc': 0.5953307392996109, 'val_loss': 0.6802812814712524, 'val_acc_rec': 0.5953307392996109, '_runtime': 55.55647826194763, '_timestamp': 1686258050.9559112, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 55}}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",upbeat-energy-356
552,"{'_runtime': 55.90009427070618, 'val_loss': 1.5575580596923828, 'val_acc_balanced': 0.48118058412176057, 'val_acc_rec_balanced': 0.4972347913524384, '_step': 9, 'epoch': 9, 'val_acc': 0.44959320834807215, 'trainer/global_step': 29, '_wandb': {'runtime': 55}, '_timestamp': 1686257908.3764932, 'val_acc_rec': 0.47470817120622566}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': False}",wandering-fog-355
553,"{'val_acc_rec': 0.4474708171206226, 'val_acc_balanced': 0.4886834978746744, 'val_acc_rec_balanced': 0.480517848164907, '_timestamp': 1686257766.5158632, 'epoch': 9, '_wandb': {'runtime': 59}, 'val_acc': 0.4527767951892466, '_runtime': 60.4337272644043, 'val_loss': 1.300383687019348, 'trainer/global_step': 29, '_step': 9}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",twilight-terrain-354
554,"{'_step': 9, 'epoch': 9, 'val_loss': 0.6970911622047424, '_timestamp': 1686257616.240245, 'val_acc_rec': 0.4046692607003891, '_wandb': {'runtime': 53}, 'val_acc': 0.4044923947647683, '_runtime': 53.5241003036499, 'val_acc_balanced': 0.49978146853146854, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",crimson-bush-353
555,"{'val_acc': 0.4642730810045985, '_runtime': 56.95884871482849, '_timestamp': 1686257476.1100135, 'val_acc_rec_balanced': 0.4543426344896933, 'val_acc_balanced': 0.4765028680469857, 'trainer/global_step': 29, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 56}, 'val_loss': 1.3565608263015747, 'val_acc_rec': 0.4474708171206226}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': False}",feasible-elevator-352
556,"{'epoch': 9, '_runtime': 61.35927724838257, 'trainer/global_step': 29, '_step': 9, '_wandb': {'runtime': 60}, 'val_acc': 0.5611956137247966, 'val_loss': 0.7559539675712585, '_timestamp': 1686257330.4613302, 'val_acc_rec': 0.5680933852140078, 'val_acc_balanced': 0.5460073015220074, 'val_acc_rec_balanced': 0.5479512317747612}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 50, 'freeze_encoder': True}",different-eon-351
557,"{'val_acc_rec_balanced': 0.5, 'epoch': 9, 'train_loss': 0.6818990707397461, 'trainer/global_step': 79, '_runtime': 54.282289266586304, 'val_loss': 0.7231993079185486, '_timestamp': 1686257178.0684743, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.5, '_step': 10, '_wandb': {'runtime': 53}, 'val_acc': 0.4046692607003891}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",crisp-shape-350
558,"{'_step': 10, 'epoch': 9, '_wandb': {'runtime': 61}, 'val_acc_rec': 0.4863813229571984, 'val_acc': 0.492041032897064, '_runtime': 61.514995098114014, 'val_loss': 1.0541744232177734, '_timestamp': 1686257034.6265242, 'train_loss': 0.46806100010871887, 'val_acc_balanced': 0.4912158919511861, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.48240321769733535}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': False}",driven-planet-349
559,"{'_wandb': {'runtime': 50}, '_runtime': 50.756577253341675, 'val_acc_balanced': 0.4704853969559852, 'epoch': 9, 'val_acc': 0.4455252918287938, 'val_loss': 1.2285152673721311, '_timestamp': 1686256882.0572562, 'train_loss': 0.4172780513763428, 'val_acc_rec': 0.45136186770428016, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.482246103569633, '_step': 10}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",apricot-totem-348
560,"{'val_acc': 0.4046692607003891, '_runtime': 53.12517595291138, '_timestamp': 1686256741.323919, 'train_loss': 0.6711221933364868, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.5, 'trainer/global_step': 79, '_step': 10, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 52}, 'val_loss': 0.7422195672988892, 'epoch': 9}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",giddy-totem-347
561,"{'val_acc': 0.50442164839052, '_runtime': 59.74923825263977, 'val_loss': 1.038273811340332, '_timestamp': 1686256600.1559582, '_step': 10, 'epoch': 9, 'val_acc_rec': 0.4941634241245136, 'val_acc_balanced': 0.5002142465377759, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.502796631473102, '_wandb': {'runtime': 59}, 'train_loss': 0.32675355672836304}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': False}",treasured-field-346
562,"{'val_acc_rec_balanced': 0.47476747109100054, 'train_loss': 0.48078516125679016, 'val_acc_balanced': 0.4736276795100325, '_wandb': {'runtime': 63}, 'val_acc': 0.42660063671736825, '_runtime': 63.77332615852356, 'val_loss': 1.1146174669265747, '_timestamp': 1686256450.85151, 'val_acc_rec': 0.42412451361867703, '_step': 10, 'epoch': 9, 'trainer/global_step': 79}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",fresh-star-345
563,"{'val_acc_balanced': 0.49418963389551623, 'val_acc_rec_balanced': 0.48903343388637505, '_timestamp': 1686256296.6389463, 'train_loss': 0.6581025719642639, 'val_acc_rec': 0.40077821011673154, 'val_acc': 0.4060841881853555, '_runtime': 62.89634561538696, 'val_loss': 0.7014433741569519, 'trainer/global_step': 79, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 62}}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",dry-wind-344
564,"{'_step': 10, '_wandb': {'runtime': 61}, 'val_acc': 0.4481782808631058, '_runtime': 61.52900838851929, 'val_loss': 1.2958636283874512, 'train_loss': 0.344592422246933, 'val_acc_balanced': 0.4925199392111157, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5009112619406737, 'epoch': 9, '_timestamp': 1686256144.4179144, 'val_acc_rec': 0.45525291828793774}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': False}",peachy-night-343
565,"{'trainer/global_step': 79, 'val_acc_rec_balanced': 0.44359602815485166, 'epoch': 9, 'val_acc': 0.42341704987619383, 'val_loss': 0.9046035408973694, '_timestamp': 1686255989.6846297, 'train_loss': 0.5162073969841003, 'val_acc_balanced': 0.451147504456328, '_step': 10, '_wandb': {'runtime': 61}, '_runtime': 61.846460580825806, 'val_acc_rec': 0.4163424124513619}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",hopeful-sea-342
566,"{'trainer/global_step': 79, 'epoch': 9, '_wandb': {'runtime': 15972}, '_timestamp': 1686255835.4955926, 'train_loss': 0.6997642517089844, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 10, 'val_acc': 0.5953307392996109, '_runtime': 15972.701831579208, 'val_loss': 0.6910139322280884, 'val_acc_rec': 0.5953307392996109}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",sandy-night-341
567,"{'val_loss': 1.0533443689346311, '_timestamp': 1686239774.626025, 'train_loss': 0.4758782386779785, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5308572146807441, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 52}, 'val_acc': 0.5222851078882207, '_runtime': 53.22951602935791, 'val_acc_rec': 0.5330739299610895, 'val_acc_balanced': 0.5124877165318342}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': False}",colorful-paper-340
568,"{'val_acc_rec_balanced': 0.4808949220713926, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 64}, 'val_acc': 0.4594977007428369, 'val_loss': 1.101520299911499, 'val_acc_rec': 0.45525291828793774, 'trainer/global_step': 79, '_runtime': 64.65129971504211, '_timestamp': 1686239633.0678756, 'train_loss': 0.5183147192001343, 'val_acc_balanced': 0.48732945975593034}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",prime-pyramid-339
569,"{'val_acc': 0.4044923947647683, '_runtime': 59.65335249900818, 'val_loss': 0.7142327427864075, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.49978146853146854, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 59}, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5, '_timestamp': 1686239479.4429405, 'train_loss': 0.6835781335830688}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",lunar-universe-338
570,"{'_step': 9, 'epoch': 8, '_runtime': 52.722702741622925, 'val_loss': 1.3814691305160522, 'trainer/global_step': 71, 'val_acc': 0.4840820657941281, '_timestamp': 1686239326.0133717, 'train_loss': 0.25437891483306885, 'val_acc_rec': 0.5136186770428015, 'val_acc_balanced': 0.48789078568490335, 'val_acc_rec_balanced': 0.5098981900452488}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': False}",sparkling-frog-337
571,"{'_runtime': 54.70851397514343, 'val_acc_rec': 0.5642023346303502, 'val_acc_rec_balanced': 0.5308257918552036, 'epoch': 9, '_wandb': {'runtime': 54}, 'val_acc': 0.5550053059780686, 'val_loss': 0.843742311000824, '_timestamp': 1686239184.7806349, 'train_loss': 0.577619194984436, 'val_acc_balanced': 0.5221216691804927, 'trainer/global_step': 79, '_step': 10}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 20, 'freeze_encoder': True}",valiant-feather-336
572,"{'epoch': 9, 'val_acc': 0.4046692607003891, '_runtime': 61.232324838638306, 'val_loss': 0.7377938032150269, '_timestamp': 1686239039.9540238, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 12, '_wandb': {'runtime': 60}, 'train_loss': 0.6707828044891357, 'trainer/global_step': 169}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",scarlet-elevator-335
573,"{'val_acc_rec_balanced': 0.49629210658622425, 'val_acc': 0.45861337106473293, '_runtime': 60.87438178062439, 'val_loss': 1.499903678894043, 'train_loss': 0.29353535175323486, 'trainer/global_step': 169, 'val_acc_balanced': 0.4933055098496275, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 60}, '_timestamp': 1686238890.1576018, 'val_acc_rec': 0.45525291828793774}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': False}",crisp-aardvark-334
574,"{'_step': 12, 'epoch': 9, 'val_loss': 1.0695592164993286, 'train_loss': 0.47100165486335754, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.4655291603821016, '_wandb': {'runtime': 63}, 'val_acc': 0.44729395118500176, '_runtime': 63.875566244125366, '_timestamp': 1686238740.3807025, 'val_acc_rec': 0.42412451361867703, 'val_acc_balanced': 0.48708807532336945}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",quiet-snowflake-333
575,"{'val_loss': 0.7758803963661194, '_timestamp': 1686238585.8808904, 'train_loss': 0.671776294708252, 'trainer/global_step': 169, '_runtime': 57.992629289627075, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 57}, 'val_acc': 0.4046692607003891}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",pleasant-water-332
576,"{'val_acc': 0.4402193137601698, 'val_loss': 1.6483514308929443, '_timestamp': 1686238439.6809611, 'train_loss': 0.36224791407585144, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.5232528908999498, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 65}, '_runtime': 66.09564924240112, 'val_acc_rec': 0.4396887159533074, 'val_acc_balanced': 0.5001128365098954}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': False}",lucky-brook-331
577,"{'train_loss': 0.5113633275032043, 'val_acc_rec': 0.4085603112840467, '_wandb': {'runtime': 68}, '_runtime': 68.95774793624878, 'val_loss': 1.013247013092041, '_timestamp': 1686238283.059312, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.45553670186023126, '_step': 12, 'epoch': 9, 'val_acc': 0.4216483905199858, 'val_acc_balanced': 0.4661790415466886}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",decent-sun-330
578,"{'_step': 12, 'epoch': 9, '_wandb': {'runtime': 56}, '_runtime': 56.97275257110596, 'train_loss': 0.7001222372055054, 'val_acc_rec': 0.5953307392996109, 'val_acc': 0.5953307392996109, 'val_loss': 0.6821167469024658, '_timestamp': 1686238119.0875576, 'val_acc_balanced': 0.5, 'trainer/global_step': 179, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",firm-mountain-329
579,"{'train_loss': 0.2700153589248657, 'val_acc_balanced': 0.4764371657754011, 'trainer/global_step': 179, '_wandb': {'runtime': 3528}, '_runtime': 3528.9146575927734, 'val_loss': 1.6744946241378784, '_timestamp': 1686237970.037531, 'val_acc_rec': 0.5175097276264592, 'val_acc_rec_balanced': 0.5023881347410759, '_step': 12, 'epoch': 9, 'val_acc': 0.4860275910859568}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': False}",pleasant-elevator-328
580,"{'_wandb': {'runtime': 63}, 'val_acc': 0.4455252918287938, '_runtime': 63.77721548080444, 'train_loss': 0.5666689872741699, 'trainer/global_step': 179, 'val_acc_rec_balanced': 0.4395424836601307, '_step': 12, 'val_loss': 0.7619452476501465, '_timestamp': 1686234353.9995654, 'val_acc_rec': 0.4280155642023346, 'val_acc_balanced': 0.4601272910096439, 'epoch': 9}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",misunderstood-star-327
581,"{'_wandb': {'runtime': 60}, 'train_loss': 0.6842788457870483, 'val_acc_rec': 0.4046692607003891, 'epoch': 9, 'val_acc': 0.4046692607003891, '_runtime': 61.276816844940186, 'val_loss': 0.7104076147079468, '_timestamp': 1686234198.8206828, 'val_acc_balanced': 0.5, 'trainer/global_step': 189, 'val_acc_rec_balanced': 0.5, '_step': 12}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",distinctive-fog-326
582,"{'train_loss': 0.6841563582420349, 'val_acc': 0.4046692607003891, '_timestamp': 1686234048.3739717, '_wandb': {'runtime': 62}, '_runtime': 62.94611358642578, 'val_loss': 0.7108031511306763, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.5, 'trainer/global_step': 189, '_step': 12, 'epoch': 9, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': False}",genial-violet-325
583,"{'val_acc_balanced': 0.4463155308743544, 'trainer/global_step': 189, 'val_acc_rec_balanced': 0.4372800402212167, '_step': 12, 'epoch': 9, '_runtime': 59.26799392700195, 'val_loss': 0.7805056571960449, 'val_acc_rec': 0.3813229571984435, '_wandb': {'runtime': 58}, 'val_acc': 0.3915811814644499, '_timestamp': 1686233895.4277458, 'train_loss': 0.6524521112442017}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",dutiful-tree-324
584,"{'trainer/global_step': 159, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_runtime': 63.340404987335205, '_timestamp': 1686233744.860762, 'train_loss': 0.6846779584884644, 'val_acc_rec': 0.4046692607003891, '_step': 12, '_wandb': {'runtime': 62}, 'val_acc': 0.4044923947647683, 'val_loss': 0.7158583998680115, 'val_acc_balanced': 0.49978146853146854}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",youthful-field-323
585,"{'val_acc_rec': 0.46303501945525294, 'val_acc_rec_balanced': 0.5213046757164403, '_step': 12, '_wandb': {'runtime': 63}, 'train_loss': 0.34489303827285767, 'val_loss': 1.1829499006271362, '_timestamp': 1686233593.8863337, 'val_acc_balanced': 0.5095782485488368, 'trainer/global_step': 159, 'epoch': 9, 'val_acc': 0.4579059073222498, '_runtime': 63.86140155792236}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': False}",fancy-sunset-322
586,"{'_step': 12, 'epoch': 9, '_wandb': {'runtime': 58}, 'val_acc': 0.487619384506544, '_timestamp': 1686233442.572891, 'val_acc_rec': 0.4863813229571984, 'val_acc_balanced': 0.5145173453996983, 'trainer/global_step': 159, 'val_acc_rec_balanced': 0.5147373051784816, '_runtime': 58.591578006744385, 'val_loss': 0.8162509799003601, 'train_loss': 0.5725125074386597}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 10, 'freeze_encoder': True}",playful-shape-321
587,"{'val_acc': 0.40413866289352673, '_runtime': 69.29469609260559, '_timestamp': 1686233293.5882852, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.4992044311897253, 'val_acc_rec_balanced': 0.5, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 68}, 'val_loss': 0.6955556869506836, 'train_loss': 0.6883305311203003, 'trainer/global_step': 369}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",distinctive-vortex-320
588,"{'epoch': 9, '_wandb': {'runtime': 75}, 'val_acc': 0.5259992925362575, 'val_loss': 0.9924795627593994, '_timestamp': 1686233134.2210286, 'train_loss': 0.3764132857322693, 'val_acc_balanced': 0.5206462246903423, '_step': 16, 'val_acc_rec_balanced': 0.539498491704374, 'trainer/global_step': 369, 'val_acc_rec': 0.5525291828793775, '_runtime': 76.32892560958862}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': False}",woven-thunder-319
589,"{'epoch': 9, '_runtime': 65.78142046928406, 'train_loss': 0.5298281908035278, 'val_acc_balanced': 0.4775655308743544, 'val_acc_rec': 0.43190661478599224, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.47206510809451985, '_step': 16, '_wandb': {'runtime': 65}, 'val_acc': 0.4428723027944818, 'val_loss': 0.8001640439033508, '_timestamp': 1686232968.7701745}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",happy-puddle-318
590,"{'epoch': 9, 'val_loss': 0.698959469795227, 'val_acc_balanced': 0.5, 'trainer/global_step': 369, 'val_acc_rec': 0.4046692607003891, 'val_acc_rec_balanced': 0.5, '_step': 16, '_wandb': {'runtime': 69}, 'val_acc': 0.4046692607003891, '_runtime': 69.77883219718933, '_timestamp': 1686232810.8830612, 'train_loss': 0.6935496926307678}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",ancient-shadow-317
591,"{'_wandb': {'runtime': 76}, 'val_acc': 0.519455252918288, '_runtime': 77.07814955711365, '_timestamp': 1686232649.9867296, 'val_acc_rec': 0.5019455252918288, 'val_acc_balanced': 0.520819050230815, 'trainer/global_step': 369, '_step': 16, 'epoch': 9, 'val_loss': 0.9636786580085754, 'train_loss': 0.4737181663513184, 'val_acc_rec_balanced': 0.535507792860734}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': False}",efficient-sea-316
592,"{'_timestamp': 1686232483.3134046, 'train_loss': 0.6536384224891663, 'val_acc_rec': 0.3891050583657587, 'val_acc_balanced': 0.4348033502445268, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.42379964806435394, 'val_acc': 0.3995401485673859, '_runtime': 69.4460825920105, '_wandb': {'runtime': 69}, 'val_loss': 0.7865910530090332, '_step': 16, 'epoch': 9}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",atomic-vortex-315
593,"{'trainer/global_step': 359, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 63}, '_runtime': 63.78344368934631, '_timestamp': 1686232321.9671967, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, '_step': 16, 'val_acc': 0.5953307392996109, 'val_loss': 0.6768844127655029, 'train_loss': 0.6662061214447021}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",magic-bee-314
594,"{'_timestamp': 1686232167.1737075, 'train_loss': 0.24886678159236908, 'val_acc_rec': 0.4708171206225681, 'val_acc_balanced': 0.4796708601855661, 'trainer/global_step': 359, '_step': 16, '_wandb': {'runtime': 74}, 'val_acc': 0.4718783162362929, 'val_acc_rec_balanced': 0.4785696329813977, 'epoch': 9, '_runtime': 74.76687836647034, 'val_loss': 1.2798839807510376}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': False}",floral-brook-313
595,"{'_wandb': {'runtime': 70}, 'val_acc': 0.51326494517156, 'train_loss': 0.5962836742401123, 'val_acc_rec': 0.5019455252918288, 'val_acc_balanced': 0.4901446592623063, 'trainer/global_step': 359, 'val_acc_rec_balanced': 0.4739190548014077, '_step': 16, 'epoch': 9, '_runtime': 70.86408472061157, 'val_loss': 0.7648696899414062, '_timestamp': 1686232001.385017}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",swift-dragon-312
596,"{'val_loss': 0.6963244080543518, 'trainer/global_step': 389, '_wandb': {'runtime': 63}, 'val_acc': 0.4046692607003891, '_runtime': 64.2432119846344, '_timestamp': 1686231839.330984, 'train_loss': 0.6922736167907715, 'val_acc_rec': 0.4046692607003891, '_step': 16, 'epoch': 9, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",vital-firefly-311
597,"{'_step': 16, '_wandb': {'runtime': 74}, '_timestamp': 1686231686.4444282, 'val_acc_rec': 0.4046692607003891, 'val_acc_balanced': 0.5, 'trainer/global_step': 389, 'epoch': 9, 'val_acc': 0.4046692607003891, '_runtime': 74.50469827651978, 'val_loss': 0.6963090300559998, 'train_loss': 0.6922770738601685, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': False}",zesty-vortex-310
598,"{'_step': 16, 'epoch': 9, '_wandb': {'runtime': 72}, 'val_acc': 0.4771842943049169, 'trainer/global_step': 389, 'val_acc_rec_balanced': 0.455316742081448, '_runtime': 72.51476430892944, 'val_loss': 0.7820132374763489, '_timestamp': 1686231521.9269383, 'train_loss': 0.6756019592285156, 'val_acc_rec': 0.4357976653696498, 'val_acc_balanced': 0.49350547328488503}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",frosty-smoke-309
599,"{'val_acc_rec': 0.4046692607003891, '_step': 15, '_wandb': {'runtime': 64}, '_runtime': 65.08528065681458, 'val_loss': 0.6986061334609985, '_timestamp': 1686231356.9880946, 'train_loss': 0.6904179453849792, 'val_acc_balanced': 0.49978146853146854, 'trainer/global_step': 329, 'epoch': 9, 'val_acc': 0.4044923947647683, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",apricot-snowflake-308
600,"{'epoch': 9, '_runtime': 75.44924521446228, '_timestamp': 1686231200.2423391, 'train_loss': 0.47157910466194153, 'val_acc_balanced': 0.500971250971251, 'val_acc_rec_balanced': 0.4934012066365008, '_step': 15, '_wandb': {'runtime': 75}, 'val_acc': 0.4564909798372833, 'val_loss': 0.8608666062355042, 'val_acc_rec': 0.4591439688715953, 'trainer/global_step': 329}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': False}",dark-microwave-307
601,"{'val_acc': 0.46073576229218255, 'val_loss': 0.7345316410064697, '_timestamp': 1686231033.2779195, 'val_acc_rec': 0.41245136186770426, 'val_acc_balanced': 0.5060760318113259, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.474201860231272, '_step': 15, 'epoch': 9, '_wandb': {'runtime': 70}, '_runtime': 70.4816644191742, 'train_loss': 0.6020193696022034}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 5, 'freeze_encoder': True}",helpful-spaceship-306
602,"{'trainer/global_step': 929, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 1148}, 'val_acc': 0.5953307392996109, 'train_loss': 0.6928765177726746, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, '_runtime': 1149.2642991542816, 'val_loss': 0.6901735067367554, '_timestamp': 1686230866.855112, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",classic-gorge-305
603,"{'val_acc_balanced': 0.49714623611682435, 'epoch': 9, 'val_acc': 0.4411036434382738, '_runtime': 96.0786783695221, 'val_loss': 0.8002306222915649, 'train_loss': 0.5368959903717041, 'val_acc_rec_balanced': 0.5018853695324283, '_step': 27, '_wandb': {'runtime': 95}, '_timestamp': 1686229623.3456123, 'val_acc_rec': 0.44357976653696496, 'trainer/global_step': 929}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': False}",morning-glade-304
604,"{'val_acc_rec': 0.4046692607003891, '_runtime': 81.79531931877136, '_timestamp': 1686229432.5141733, 'train_loss': 0.6797152161598206, 'val_acc': 0.4136894234170499, 'val_loss': 0.7747589945793152, 'val_acc_balanced': 0.4615241784359431, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.45226872800402207, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 81}}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",hearty-puddle-303
605,"{'_step': 27, 'val_loss': 0.6841519474983215, '_timestamp': 1686229254.8499892, 'val_acc_rec': 0.5953307392996109, 'train_loss': 0.6860496997833252, 'val_acc_balanced': 0.5, 'trainer/global_step': 939, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 79}, 'val_acc': 0.5953307392996109, '_runtime': 79.58518719673157}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",hopeful-terrain-302
606,"{'val_acc': 0.5953307392996109, '_runtime': 97.50698018074036, '_timestamp': 1686229080.759941, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, 'trainer/global_step': 939, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 97}, 'val_loss': 0.684150755405426, 'train_loss': 0.6860487461090088, 'val_acc_rec': 0.5953307392996109}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': False}",worldly-sponge-301
607,"{'_wandb': {'runtime': 72}, 'val_acc': 0.4129819596745667, '_timestamp': 1686228889.7370384, 'val_acc_rec': 0.41245136186770426, 'trainer/global_step': 939, 'epoch': 9, '_runtime': 72.45073437690735, 'val_loss': 0.7515169382095337, 'train_loss': 0.6546763181686401, 'val_acc_balanced': 0.4486122537593126, 'val_acc_rec_balanced': 0.45110608345902464, '_step': 27}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",fallen-river-300
608,"{'_wandb': {'runtime': 79}, '_timestamp': 1686228717.9982352, 'val_acc_rec': 0.5953307392996109, 'val_loss': 0.6904701590538025, 'train_loss': 0.6946225166320801, 'val_acc_balanced': 0.5, 'trainer/global_step': 909, '_step': 27, 'epoch': 9, 'val_acc': 0.5953307392996109, '_runtime': 79.98937225341797, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",peach-eon-299
609,"{'val_acc_balanced': 0.518430915489739, 'trainer/global_step': 909, 'val_acc_rec_balanced': 0.54587732528909, 'epoch': 9, '_wandb': {'runtime': 1869}, 'val_acc': 0.4932790944464096, '_timestamp': 1686228539.6837952, 'val_acc_rec': 0.5252918287937743, '_step': 27, '_runtime': 1869.7652022838593, 'val_loss': 0.8639592528343201, 'train_loss': 0.5234177708625793}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': False}",vocal-deluge-298
610,"{'val_acc_rec_balanced': 0.5053104575163399, '_step': 27, '_runtime': 84.83318448066711, '_timestamp': 1686226575.9679644, 'train_loss': 0.6142203211784363, 'val_acc_rec': 0.48249027237354086, 'val_acc_balanced': 0.4904845971022441, 'trainer/global_step': 909, 'epoch': 9, '_wandb': {'runtime': 84}, 'val_acc': 0.4711708524938097, 'val_loss': 0.7117589712142944}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",sleek-wildflower-297
611,"{'val_acc': 0.5953307392996109, 'val_loss': 0.692623496055603, '_timestamp': 1686226395.2215729, 'train_loss': 0.6934971213340759, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, 'trainer/global_step': 929, '_step': 27, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 120}, '_runtime': 120.64452481269836, 'epoch': 9}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",daily-river-296
612,"{'_wandb': {'runtime': 95}, '_timestamp': 1686226179.3790624, 'val_acc_balanced': 0.5, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.5, 'val_acc_rec': 0.5953307392996109, '_step': 27, 'epoch': 9, 'val_acc': 0.5953307392996109, '_runtime': 95.48486852645874, 'val_loss': 0.6926242113113403, 'train_loss': 0.6934964656829834}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': False}",amber-vortex-295
613,"{'epoch': 9, 'val_loss': 0.8003991842269897, '_timestamp': 1686225988.899995, 'train_loss': 0.66883385181427, 'val_acc_rec_balanced': 0.4518602312719959, '_step': 27, '_wandb': {'runtime': 82}, 'val_acc': 0.4363282631765122, '_runtime': 82.55292344093323, 'val_acc_rec': 0.4280155642023346, 'val_acc_balanced': 0.4641608391608392, 'trainer/global_step': 929}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",soft-elevator-294
614,"{'val_loss': 0.688922107219696, 'train_loss': 0.6954416036605835, 'trainer/global_step': 889, 'val_acc_rec_balanced': 0.5, '_step': 26, '_wandb': {'runtime': 77}, 'val_acc': 0.5953307392996109, '_runtime': 77.76661205291748, '_timestamp': 1686225811.285018, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, 'epoch': 9}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",bright-bush-293
615,"{'val_acc_balanced': 0.4871194981489099, 'val_acc_rec_balanced': 0.468954248366013, '_step': 26, 'epoch': 9, 'val_acc': 0.4681641315882561, '_runtime': 96.4447422027588, 'val_acc_rec': 0.46303501945525294, 'trainer/global_step': 889, '_wandb': {'runtime': 96}, 'val_loss': 0.870688259601593, '_timestamp': 1686225641.2632113, 'train_loss': 0.5415058732032776}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': False}",magic-bush-292
616,"{'val_acc_balanced': 0.47018830842360254, '_step': 26, 'epoch': 9, '_wandb': {'runtime': 83}, 'train_loss': 0.6437753438949585, 'val_acc_rec': 0.44357976653696496, 'trainer/global_step': 889, 'val_acc_rec_balanced': 0.449534942182001, 'val_acc': 0.47983728333922887, '_runtime': 83.3681538105011, 'val_loss': 0.7316054105758667, '_timestamp': 1686225450.6018257}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 2, 'freeze_encoder': True}",classic-resonance-291
617,"{'_timestamp': 1686225268.4964724, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 99}, 'val_loss': 0.6845415830612183, 'train_loss': 0.6945652961730957, 'val_acc_rec': 0.5953307392996109, '_step': 46, 'val_acc': 0.5953307392996109, '_runtime': 99.47109127044678}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",divine-vortex-290
618,"{'val_loss': 0.768521249294281, '_wandb': {'runtime': 133}, 'val_acc': 0.429430491687301, '_runtime': 133.36589813232422, 'train_loss': 0.6324053406715393, 'val_acc_rec': 0.42023346303501946, 'val_acc_balanced': 0.4900718154394625, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.4807378079436903, '_step': 46, 'epoch': 9, '_timestamp': 1686225065.920572}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': False}",still-lion-289
619,"{'val_acc_rec': 0.41245136186770426, 'val_acc_balanced': 0.4691070775629599, '_step': 46, 'epoch': 9, 'val_loss': 0.7360833883285522, '_timestamp': 1686224826.0051095, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.454185520361991, '_wandb': {'runtime': 100}, 'val_acc': 0.42430137955429786, '_runtime': 100.77708649635316, 'train_loss': 0.6750697493553162}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",fearless-smoke-288
620,"{'val_acc': 0.5953307392996109, '_timestamp': 1686224621.768337, 'train_loss': 0.6977896094322205, '_step': 46, 'epoch': 9, 'val_loss': 0.6845343708992004, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 99}, '_runtime': 99.58664107322691}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",playful-vortex-287
621,"{'_timestamp': 1686224420.3329902, 'val_acc_balanced': 0.5, 'val_acc': 0.5953307392996109, '_runtime': 132.99747014045715, 'val_loss': 0.6845340132713318, 'train_loss': 0.6977899074554443, 'val_acc_rec': 0.5953307392996109, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 132}}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': False}",lilac-river-286
622,"{'val_acc_rec_balanced': 0.47706133735545503, '_step': 46, 'epoch': 9, 'val_acc': 0.4317297488503714, '_runtime': 105.62768530845642, 'val_acc_rec': 0.4396887159533074, 'trainer/global_step': 1869, '_wandb': {'runtime': 104}, 'val_loss': 0.722415566444397, '_timestamp': 1686224186.092353, 'train_loss': 0.7001945972442627, 'val_acc_balanced': 0.4705168197815257}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",azure-terrain-285
623,"{'_runtime': 97.09694242477416, 'val_loss': 0.6846064925193787, '_timestamp': 1686223973.4815774, 'train_loss': 0.6863903999328613, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5, '_wandb': {'runtime': 96}, 'val_acc': 0.5953307392996109, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, '_step': 46, 'epoch': 9}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",hopeful-silence-284
624,"{'trainer/global_step': 1869, '_step': 46, 'epoch': 9, '_runtime': 133.69611835479736, 'val_loss': 0.6846065521240234, '_timestamp': 1686223772.7863224, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 133}, 'val_acc': 0.5953307392996109, 'train_loss': 0.6863905191421509, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': False}",dulcet-breeze-283
625,"{'_step': 46, 'epoch': 9, '_runtime': 104.02039790153503, '_timestamp': 1686223537.709879, 'train_loss': 0.6620757579803467, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.48017219708396175, '_wandb': {'runtime': 103}, 'val_acc': 0.4138662893526706, 'val_loss': 0.7621784210205078, 'val_acc_rec': 0.4085603112840467, 'val_acc_balanced': 0.48385866584396}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",zany-music-282
626,"{'trainer/global_step': 1869, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 99}, '_runtime': 99.57173371315002, 'val_acc_rec': 0.5953307392996109, 'val_acc_rec_balanced': 0.5, 'val_acc': 0.5953307392996109, 'val_loss': 0.6842631101608276, '_timestamp': 1686223326.5705495, 'train_loss': 0.6912266612052917, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",astral-sea-281
627,"{'val_acc': 0.5953307392996109, 'val_loss': 0.6842630505561829, '_timestamp': 1686223123.2066553, 'train_loss': 0.6912265419960022, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 134}, '_runtime': 134.60676217079163, 'val_acc_rec': 0.5953307392996109}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': False}",valiant-sun-280
628,"{'_timestamp': 1686222884.6792972, 'train_loss': 0.6566867828369141, 'val_acc_rec': 0.4085603112840467, 'val_acc_balanced': 0.45919174779468896, 'val_acc_rec_balanced': 0.4462983911513323, 'epoch': 9, '_runtime': 104.5399661064148, 'val_loss': 0.7279967665672302, 'trainer/global_step': 1869, '_step': 46, '_wandb': {'runtime': 104}, 'val_acc': 0.4189954014856739}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",drawn-universe-279
629,"{'trainer/global_step': 1869, '_wandb': {'runtime': 97}, '_timestamp': 1686222675.6664145, 'train_loss': 0.6948604583740234, 'val_acc_rec': 0.5953307392996109, 'val_loss': 0.6836897134780884, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 46, 'epoch': 9, 'val_acc': 0.5953307392996109, '_runtime': 98.25431418418884}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",youthful-dust-278
630,"{'val_loss': 0.7576085329055786, 'train_loss': 0.5243455171585083, 'val_acc_rec': 0.44357976653696496, 'val_acc_balanced': 0.5024538370126606, 'trainer/global_step': 1869, '_wandb': {'runtime': 133}, '_runtime': 133.7779998779297, 'val_acc': 0.45242306331800497, '_timestamp': 1686222474.0869758, 'val_acc_rec_balanced': 0.49264705882352944, '_step': 46, 'epoch': 9}","{'weights': 'random', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': False}",hearty-voice-277
631,"{'epoch': 9, 'val_acc': 0.43226034665723384, '_runtime': 101.93460869789124, 'train_loss': 0.6232893466949463, 'val_acc_rec': 0.41245136186770426, 'val_acc_balanced': 0.4824403537638832, 'val_acc_rec_balanced': 0.4588046757164404, '_step': 46, '_wandb': {'runtime': 101}, 'val_loss': 0.7403850555419922, '_timestamp': 1686222240.6985376, 'trainer/global_step': 1869}","{'weights': 'eegclip', 'task_name': 'epilep', 'train_frac': 1, 'freeze_encoder': True}",rose-plant-276
632,"{'val_acc': 0.5058365758754864, '_runtime': 61.612173080444336, 'val_loss': 0.7768061757087708, 'val_acc_rec': 0.5058365758754864, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 61}, '_timestamp': 1686222037.6855152, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",worldly-music-275
633,"{'epoch': 9, '_wandb': {'runtime': 57}, 'val_acc': 0.5410328970640255, 'val_loss': 1.8800339698791504, '_timestamp': 1686221889.091016, 'val_acc_balanced': 0.53679725786025, '_step': 9, '_runtime': 58.934996128082275, 'val_acc_rec': 0.5175097276264592, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.512083585705633}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': False}",classic-wave-274
634,"{'val_acc': 0.6480367881146091, '_runtime': 61.95130372047424, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.6588431253785585, 'epoch': 9, '_wandb': {'runtime': 61}, '_timestamp': 1686221740.6544008, 'val_acc_rec': 0.6614785992217899, 'val_acc_balanced': 0.6454738175210616, '_step': 9, 'val_loss': 0.9569085240364076}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",wobbly-flower-273
635,"{'_runtime': 59.542837142944336, '_timestamp': 1686221589.2792652, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 58}, 'val_acc': 0.5058365758754864, 'val_loss': 0.7917184829711914}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",clean-shadow-272
636,"{'_runtime': 60.18683385848999, 'val_loss': 1.1048160791397097, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 9, 'epoch': 9, 'val_acc': 0.5058365758754864, 'trainer/global_step': 29, '_wandb': {'runtime': 59}, '_timestamp': 1686221443.4734058, 'val_acc_rec': 0.5058365758754864}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': False}",jolly-armadillo-271
637,"{'val_acc_rec_balanced': 0.5, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 60}, '_timestamp': 1686221296.5219269, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'val_acc': 0.5058365758754864, '_runtime': 61.23024892807007, 'val_loss': 0.8063000440597534, 'val_acc_rec': 0.5058365758754864}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",sage-butterfly-270
638,"{'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, '_step': 9, '_wandb': {'runtime': 56}, 'val_acc': 0.5058365758754864, '_timestamp': 1686221143.6965888, 'epoch': 9, '_runtime': 56.839059829711914, 'val_loss': 0.6962681412696838, 'val_acc_rec': 0.5058365758754864}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",fanciful-dew-269
639,"{'epoch': 9, 'val_acc_rec': 0.5214007782101168, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5162023016353725, '_step': 9, '_wandb': {'runtime': 55}, 'val_acc': 0.5302440749911567, '_runtime': 55.79642462730408, 'val_loss': 0.7641178369522095, '_timestamp': 1686221000.3797295, 'val_acc_balanced': 0.5265995815208413}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': False}",devout-yogurt-268
640,"{'_timestamp': 1686220858.8588092, 'val_acc_rec': 0.708171206225681, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 61}, '_runtime': 61.99435114860535, 'val_acc_rec_balanced': 0.7084494245911569, 'val_acc': 0.7154227095861337, 'val_loss': 0.6443683505058289, 'val_acc_balanced': 0.7156379054016849, 'trainer/global_step': 29}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",comic-blaze-267
641,"{'_timestamp': 1686220710.080133, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 57}, 'val_acc': 0.5058365758754864, '_runtime': 58.44813299179077, 'val_loss': 0.708710253238678, 'val_acc_rec': 0.5058365758754864, '_step': 9}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",fiery-mountain-266
642,"{'_step': 9, '_wandb': {'runtime': 52}, '_runtime': 52.59713172912598, 'val_acc_rec': 0.5058365758754864, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5050878255602664, 'epoch': 9, 'val_acc': 0.5275910859568447, 'val_loss': 1.304743766784668, '_timestamp': 1686220562.2808075, 'val_acc_balanced': 0.5296142833544408}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': False}",lemon-snow-265
643,"{'_step': 9, 'epoch': 9, '_runtime': 60.47543144226074, 'val_acc_rec': 0.4863813229571984, 'val_acc_balanced': 0.48322228952150215, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.4844033918837069, '_wandb': {'runtime': 60}, 'val_acc': 0.48478952953661125, 'val_loss': 0.8319047093391418, '_timestamp': 1686220420.9956264}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",proud-shape-264
644,"{'_timestamp': 1686220272.8076053, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 59}, 'val_acc': 0.5058365758754864, '_runtime': 60.206191301345825, 'val_loss': 0.7662988901138306, 'val_acc_rec': 0.5058365758754864}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",astral-pine-263
645,"{'_wandb': {'runtime': 58}, 'epoch': 9, 'val_acc': 0.5109656880084895, '_runtime': 59.02819752693176, 'val_loss': 3.670180559158325, '_timestamp': 1686220125.0929966, 'val_acc_rec': 0.5175097276264592, 'val_acc_balanced': 0.5061725675898905, 'trainer/global_step': 29, '_step': 9, 'val_acc_rec_balanced': 0.511992731677771}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': False}",logical-elevator-262
646,"{'val_acc_rec': 0.5058365758754864, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, '_step': 9, 'val_loss': 0.8488076329231262, 'val_acc': 0.5058365758754864, '_runtime': 60.768444776535034, '_timestamp': 1686219977.065272, 'val_acc_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 60}}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 50, 'freeze_encoder': True}",silvery-firefly-261
647,"{'epoch': 9, '_wandb': {'runtime': 55}, '_runtime': 55.68155264854431, 'val_loss': 0.6952623724937439, '_timestamp': 1686219825.6008637, 'train_loss': 0.6755957007408142, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5, '_step': 10, 'val_acc': 0.5058365758754864, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",fallen-snowball-260
648,"{'epoch': 9, '_wandb': {'runtime': 61}, '_runtime': 62.30216431617737, 'val_loss': 1.3259000778198242, '_timestamp': 1686219683.0819652, 'train_loss': 0.3067261576652527, 'val_acc_rec_balanced': 0.6123258631132646, '_step': 10, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5854440834755795, 'trainer/global_step': 79, 'val_acc': 0.5870180403254334}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': False}",genial-deluge-259
649,"{'epoch': 9, '_wandb': {'runtime': 62}, 'val_acc_rec': 0.6964980544747081, '_timestamp': 1686219532.02127, 'train_loss': 0.2524116039276123, 'val_acc_balanced': 0.682584934750289, 'trainer/global_step': 79, '_step': 10, 'val_acc': 0.6837637071100107, '_runtime': 62.53629112243652, 'val_loss': 0.7813984751701355, 'val_acc_rec_balanced': 0.6951847365233192}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",sleek-sound-258
650,"{'val_acc': 0.5058365758754864, '_runtime': 61.10968542098999, 'val_acc_rec': 0.5058365758754864, 'trainer/global_step': 79, 'epoch': 9, '_wandb': {'runtime': 60}, 'val_loss': 0.7019142508506775, '_timestamp': 1686219378.6989355, 'train_loss': 0.6159600615501404, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 10}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",woven-donkey-257
651,"{'_step': 10, 'epoch': 9, 'val_acc': 0.4729395118500177, '_runtime': 61.93534779548645, 'val_acc_rec': 0.45525291828793774, 'trainer/global_step': 79, '_wandb': {'runtime': 61}, 'val_loss': 1.5002001523971558, '_timestamp': 1686219231.3692808, 'train_loss': 0.1815882623195648, 'val_acc_balanced': 0.4731898023236606, 'val_acc_rec_balanced': 0.4552695336159903}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': False}",silver-totem-256
652,"{'val_acc': 0.5415634948708878, '_runtime': 51.75152969360352, 'val_loss': 1.078816294670105, 'train_loss': 0.515721321105957, 'val_acc_rec': 0.5369649805447471, 'trainer/global_step': 71, 'val_acc_rec_balanced': 0.5354936402180497, '_step': 9, 'epoch': 8, '_timestamp': 1686219077.3275948, 'val_acc_balanced': 0.540501624359892}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",floral-morning-255
653,"{'_step': 10, 'epoch': 9, '_wandb': {'runtime': 56}, 'val_loss': 0.69694983959198, '_timestamp': 1686218935.502072, 'val_acc_rec': 0.4941634241245136, 'trainer/global_step': 79, 'val_acc': 0.4941634241245136, '_runtime': 57.42847418785095, 'train_loss': 0.6975588798522949, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",tough-frog-254
654,"{'_timestamp': 1686218791.896994, 'val_acc_rec': 0.6303501945525292, 'val_acc_balanced': 0.6006910412422224, 'val_loss': 1.3334567546844482, 'train_loss': 0.20655448734760284, 'trainer/global_step': 79, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 62}, 'val_acc': 0.6002829854969933, '_runtime': 62.437477111816406, 'val_acc_rec_balanced': 0.631435493640218}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': False}",floral-glade-253
655,"{'val_acc_rec': 0.6848249027237354, 'trainer/global_step': 79, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 59}, 'val_acc': 0.686947293951185, 'val_loss': 0.6443265080451965, '_timestamp': 1686218642.02975, 'val_acc_rec_balanced': 0.6860084797092671, '_runtime': 59.54761791229248, 'train_loss': 0.5114163756370544, 'val_acc_balanced': 0.687941192665602}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",earnest-cloud-252
656,"{'_timestamp': 1686218491.9655025, 'train_loss': 0.6818323731422424, 'val_acc_rec': 0.5058365758754864, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5, '_step': 10, 'val_loss': 0.7095043659210205, 'val_acc': 0.5058365758754864, '_runtime': 57.87271046638489, 'val_acc_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 57}}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",efficient-serenity-251
657,"{'_runtime': 59.72174859046936, 'val_loss': 0.8658385276794434, 'train_loss': 0.4845825433731079, 'val_acc_balanced': 0.5003579098067287, 'trainer/global_step': 79, 'epoch': 9, '_wandb': {'runtime': 59}, 'val_acc': 0.506190307746728, 'val_acc_rec_balanced': 0.5, '_step': 10, '_timestamp': 1686218346.7097406, 'val_acc_rec': 0.5058365758754864}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': False}",clean-oath-250
658,"{'trainer/global_step': 79, 'epoch': 9, 'val_acc': 0.5933852140077821, '_runtime': 63.49150824546814, '_timestamp': 1686218199.8672273, 'train_loss': 0.5214575529098511, 'val_acc_rec': 0.5953307392996109, 'val_acc_balanced': 0.594393205219977, 'val_acc_rec_balanced': 0.5960932768019382, '_step': 10, '_wandb': {'runtime': 62}, 'val_loss': 0.7004615664482117}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",jolly-lake-249
659,"{'val_acc': 0.5058365758754864, 'train_loss': 0.6123749017715454, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 70}, '_runtime': 71.18541479110718, 'val_loss': 0.7471328973770142, '_timestamp': 1686218045.395342, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",autumn-sunset-248
660,"{'_timestamp': 1686217885.2551365, 'val_acc_rec': 0.4980544747081712, 'val_acc': 0.4904492394764768, '_runtime': 61.63165736198425, 'val_loss': 1.935692310333252, 'train_loss': 0.29129543900489807, 'val_acc_balanced': 0.4858391608391608, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 61}, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.4927619624470018}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': False}",efficient-sea-247
661,"{'_wandb': {'runtime': 59}, 'train_loss': 0.4600633978843689, 'val_acc_balanced': 0.6673173283409504, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.676771653543307, '_step': 10, 'val_acc': 0.6667845772904138, '_runtime': 59.49502635002136, 'val_loss': 0.6871098875999451, '_timestamp': 1686217735.488485, 'val_acc_rec': 0.6770428015564203, 'epoch': 9}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 20, 'freeze_encoder': True}",balmy-grass-246
662,"{'_timestamp': 1686217587.7935555, 'train_loss': 0.6693875789642334, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5, '_step': 12, '_wandb': {'runtime': 56}, 'val_acc': 0.5058365758754864, 'val_loss': 0.6949556469917297, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_runtime': 56.72230076789856}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",dry-spaceship-245
663,"{'val_acc': 0.5624336752741422, 'val_loss': 1.543426275253296, 'train_loss': 0.1608843356370926, 'val_acc_balanced': 0.5618289191123838, 'val_acc_rec': 0.5603112840466926, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.5597516656571775, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 66}, '_runtime': 66.96922492980957, '_timestamp': 1686217442.568773}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': False}",whole-flower-244
664,"{'val_acc_balanced': 0.7078974175430868, '_step': 12, '_wandb': {'runtime': 65}, '_timestamp': 1686217285.235616, 'val_loss': 0.6295230388641357, 'train_loss': 0.38276728987693787, 'val_acc_rec': 0.708171206225681, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.7075408843125378, 'epoch': 9, 'val_acc': 0.7083480721613017, '_runtime': 66.043142080307}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",proud-oath-243
665,"{'val_acc_balanced': 0.5, 'trainer/global_step': 169, '_runtime': 71.99781632423401, '_timestamp': 1686217128.2544544, 'train_loss': 0.6962857842445374, 'val_acc': 0.5058365758754864, 'val_loss': 0.6957821846008301, 'val_acc_rec': 0.5058365758754864, 'val_acc_rec_balanced': 0.5, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 65}}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",leafy-armadillo-242
666,"{'val_acc': 0.518570923240184, '_timestamp': 1686216962.3002682, 'val_acc_rec': 0.5136186770428015, 'trainer/global_step': 169, '_step': 12, 'epoch': 9, 'val_loss': 1.0548150539398191, 'train_loss': 0.46431320905685425, 'val_acc_balanced': 0.5190876053080777, 'val_acc_rec_balanced': 0.5135978195033313, '_wandb': {'runtime': 70}, '_runtime': 70.90844106674194}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': False}",splendid-wood-241
667,"{'_step': 12, 'val_loss': 0.7083778381347656, '_timestamp': 1686216804.3839848, 'val_acc_balanced': 0.6304952921094653, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.641701998788613, 'epoch': 9, '_wandb': {'runtime': 67}, 'val_acc': 0.63052706048815, '_runtime': 68.27194881439209, 'train_loss': 0.47824856638908386, 'val_acc_rec': 0.642023346303502}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",sage-lake-240
668,"{'_runtime': 57.49885773658752, '_timestamp': 1686216646.2936578, 'val_acc_rec': 0.4941634241245136, 'val_acc_balanced': 0.5, 'trainer/global_step': 179, 'val_acc_rec_balanced': 0.5, 'epoch': 9, 'val_acc': 0.4941634241245136, 'val_loss': 0.6935093402862549, 'train_loss': 0.69416344165802, '_step': 12, '_wandb': {'runtime': 57}}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",deep-star-239
669,"{'_wandb': {'runtime': 66}, '_runtime': 66.96317148208618, 'val_loss': 1.5766874551773071, '_timestamp': 1686216499.3850706, 'val_acc_balanced': 0.5736124112108364, 'trainer/global_step': 179, '_step': 12, 'epoch': 9, 'val_acc_rec_balanced': 0.584221683827983, 'val_acc_rec': 0.5875486381322957, 'val_acc': 0.5764060841881854, 'train_loss': 0.10777639597654344}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': False}",glowing-butterfly-238
670,"{'val_acc_rec_balanced': 0.7355542095699577, '_wandb': {'runtime': 58}, '_runtime': 59.07414436340332, 'val_loss': 0.6021698117256165, '_timestamp': 1686216340.5528977, 'train_loss': 0.4556860327720642, 'trainer/global_step': 179, '_step': 12, 'epoch': 9, 'val_acc': 0.7154227095861337, 'val_acc_rec': 0.7354085603112841, 'val_acc_balanced': 0.7156420351302241}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",curious-armadillo-237
671,"{'_timestamp': 1686216191.3962946, 'train_loss': 0.6788515448570251, 'val_acc_rec': 0.4941634241245136, 'val_acc_balanced': 0.5, 'trainer/global_step': 189, 'epoch': 9, '_wandb': {'runtime': 64}, 'val_acc': 0.4941634241245136, 'val_acc_rec_balanced': 0.5, '_step': 12, '_runtime': 64.61546659469604, 'val_loss': 0.6973650455474854}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",dashing-totem-236
672,"{'train_loss': 0.3509123921394348, 'val_acc_rec': 0.6108949416342413, 'val_acc_balanced': 0.6072173889103023, 'epoch': 9, '_wandb': {'runtime': 68}, '_runtime': 68.55095934867859, 'val_loss': 0.964544713497162, 'val_acc_rec_balanced': 0.6143852210781344, '_step': 12, 'val_acc': 0.605765829501238, '_timestamp': 1686216040.1469233, 'trainer/global_step': 189}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': False}",stoic-butterfly-235
673,"{'_timestamp': 1686215882.6648457, 'train_loss': 0.4399110972881317, 'trainer/global_step': 189, 'val_acc_rec_balanced': 0.6892186553603876, '_step': 12, '_runtime': 60.08263945579529, 'val_acc': 0.6837637071100107, 'val_loss': 0.6779223084449768, 'val_acc_rec': 0.688715953307393, 'val_acc_balanced': 0.6841913991520291, 'epoch': 9, '_wandb': {'runtime': 59}}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",floral-violet-234
674,"{'train_loss': 0.6827977895736694, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 59}, 'val_loss': 0.6945775151252747, '_timestamp': 1686215733.0025885, 'val_acc_balanced': 0.5, 'trainer/global_step': 159, '_step': 12, 'val_acc': 0.5058365758754864, '_runtime': 60.411601305007935, 'val_acc_rec': 0.5058365758754864}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",giddy-moon-233
675,"{'_step': 12, '_wandb': {'runtime': 65}, 'val_acc': 0.5601344181110718, '_timestamp': 1686215583.8639443, 'val_acc_balanced': 0.5577721491107318, 'trainer/global_step': 159, 'val_acc_rec_balanced': 0.5406420351302241, 'epoch': 9, '_runtime': 65.64549922943115, 'val_loss': 1.0494143962860107, 'train_loss': 0.3492347300052643, 'val_acc_rec': 0.5447470817120622}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': False}",splendid-shape-232
676,"{'train_loss': 0.4465439319610596, '_wandb': {'runtime': 68}, 'val_acc': 0.7331093031482137, '_runtime': 68.85316801071167, 'val_loss': 0.6649208068847656, 'val_acc_balanced': 0.7332443147403778, 'trainer/global_step': 159, 'val_acc_rec_balanced': 0.7589036947304664, '_step': 12, 'epoch': 9, '_timestamp': 1686215429.35779, 'val_acc_rec': 0.7587548638132295}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 10, 'freeze_encoder': True}",confused-breeze-231
677,"{'_runtime': 65.70211362838745, '_timestamp': 1686215269.3462026, 'train_loss': 0.6736205220222473, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5, 'trainer/global_step': 369, '_wandb': {'runtime': 65}, 'epoch': 9, 'val_acc': 0.5058365758754864, 'val_loss': 0.697199821472168, 'val_acc_rec_balanced': 0.5, '_step': 16}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",drawn-glade-230
678,"{'_runtime': 71.18183159828186, 'val_loss': 0.98346209526062, 'train_loss': 0.18667303025722504, 'val_acc_rec': 0.688715953307393, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 70}, 'val_acc': 0.6489211177927131, 'val_acc_balanced': 0.648313694179836, '_timestamp': 1686215113.0385785, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.6882192610539066}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': False}",jolly-universe-229
679,"{'_timestamp': 1686214953.4909348, 'val_acc_rec': 0.7042801556420234, 'trainer/global_step': 369, 'epoch': 9, '_runtime': 68.18443274497986, 'val_acc': 0.6752741422002122, 'val_loss': 0.625089704990387, 'train_loss': 0.5491873621940613, 'val_acc_balanced': 0.6763696932988271, 'val_acc_rec_balanced': 0.7049666868564506, '_step': 16, '_wandb': {'runtime': 67}}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",fearless-hill-228
680,"{'val_loss': 0.704257071018219, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_runtime': 71.97752165794373, '_timestamp': 1686214792.7665627, 'train_loss': 0.6624288558959961, 'val_acc_rec': 0.5058365758754864, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 71}, 'val_acc': 0.5058365758754864, 'trainer/global_step': 369}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",fragrant-morning-227
681,"{'val_loss': 0.9740397334098816, 'train_loss': 0.40446892380714417, 'val_acc_balanced': 0.6260627168107483, 'val_acc_rec_balanced': 0.6074803149606299, '_step': 16, 'epoch': 9, 'val_acc': 0.6276972055182172, '_runtime': 76.89402413368225, '_wandb': {'runtime': 76}, '_timestamp': 1686214629.627315, 'val_acc_rec': 0.6108949416342413, 'trainer/global_step': 369}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': False}",brisk-sun-226
682,"{'_wandb': {'runtime': 62}, 'val_acc': 0.6572338167668907, 'val_loss': 0.6347818374633789, 'val_acc_balanced': 0.6572696988051319, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.7040581465778316, '_step': 16, 'epoch': 9, '_runtime': 63.391172885894775, '_timestamp': 1686214460.3648338, 'train_loss': 0.44568485021591187, 'val_acc_rec': 0.7042801556420234}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",swift-puddle-225
683,"{'_runtime': 70.54816126823425, '_timestamp': 1686214306.8637333, 'val_acc_balanced': 0.5, 'trainer/global_step': 359, 'val_acc_rec_balanced': 0.5, '_step': 16, 'val_acc': 0.5058365758754864, 'val_loss': 0.6972140073776245, 'train_loss': 0.7045984864234924, 'val_acc_rec': 0.5058365758754864, 'epoch': 9, '_wandb': {'runtime': 69}}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",drawn-haze-224
684,"{'_wandb': {'runtime': 76}, 'val_acc': 0.6328263176512203, '_timestamp': 1686214147.3074028, 'trainer/global_step': 359, 'val_acc_rec_balanced': 0.6598425196850394, '_step': 16, 'epoch': 9, 'train_loss': 0.5849462151527405, 'val_acc_rec': 0.6614785992217899, 'val_acc_balanced': 0.6329043004239855, '_runtime': 76.91468286514282, 'val_loss': 0.6984516978263855}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': False}",frosty-music-223
685,"{'_step': 16, '_runtime': 68.9487144947052, '_timestamp': 1686213979.5869534, 'val_acc_balanced': 0.7137106987500688, 'trainer/global_step': 359, 'val_acc_rec_balanced': 0.7273167777104785, 'epoch': 9, '_wandb': {'runtime': 68}, 'val_acc': 0.7140077821011673, 'val_loss': 0.6490510106086731, 'train_loss': 0.500049352645874, 'val_acc_rec': 0.7276264591439688}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",summer-oath-222
686,"{'val_acc': 0.4941634241245136, '_runtime': 62.19504284858704, 'train_loss': 0.6931403875350952, 'val_acc_rec': 0.4941634241245136, '_step': 16, '_wandb': {'runtime': 61}, '_timestamp': 1686213820.4574058, 'val_acc_balanced': 0.5, 'trainer/global_step': 389, 'val_acc_rec_balanced': 0.5, 'epoch': 9, 'val_loss': 0.6931502819061279}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",azure-surf-221
687,"{'_step': 16, 'val_loss': 1.0758684873580933, 'val_acc_rec': 0.6731517509727627, 'val_acc_balanced': 0.6781798909751666, 'trainer/global_step': 389, 'epoch': 9, '_wandb': {'runtime': 68}, 'val_acc': 0.6765122037495578, '_runtime': 69.34338974952698, '_timestamp': 1686213668.9935389, 'train_loss': 0.12444878369569778, 'val_acc_rec_balanced': 0.6751968503937008}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': False}",generous-salad-220
688,"{'_timestamp': 1686213511.009744, 'val_acc_rec': 0.7120622568093385, 'val_acc_rec_balanced': 0.7126589945487583, '_wandb': {'runtime': 71}, '_runtime': 72.29623699188232, 'val_acc': 0.6963211885390874, 'val_loss': 0.6360259652137756, 'train_loss': 0.44611674547195435, 'val_acc_balanced': 0.6971325918176312, 'trainer/global_step': 389, '_step': 16, 'epoch': 9}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",hardy-fire-219
689,"{'val_acc': 0.5058365758754864, 'val_loss': 0.6974192261695862, '_timestamp': 1686213345.3816006, 'val_acc_rec_balanced': 0.5, 'train_loss': 0.6549224853515625, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5, 'trainer/global_step': 329, '_step': 15, 'epoch': 9, '_wandb': {'runtime': 66}, '_runtime': 66.57048153877258}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",elated-salad-218
690,"{'train_loss': 0.3497264087200165, 'val_acc_balanced': 0.6209569957601453, '_wandb': {'runtime': 74}, 'val_acc': 0.6211531659002476, '_runtime': 74.91392588615417, '_timestamp': 1686213188.83198, 'val_acc_rec': 0.6459143968871596, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.6456390066626287, '_step': 15, 'epoch': 9, 'val_loss': 0.9996392130851746}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': False}",polished-frog-217
691,"{'epoch': 9, '_wandb': {'runtime': 68}, '_runtime': 68.97475695610046, '_step': 15, 'val_loss': 0.5680072903633118, '_timestamp': 1686213022.504477, 'train_loss': 0.45047682523727417, 'val_acc_rec': 0.7665369649805448, 'val_acc_balanced': 0.7361709156984748, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.7659600242277407, 'val_acc': 0.7366466218606297}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 5, 'freeze_encoder': True}",classic-river-216
692,"{'_step': 27, 'epoch': 9, '_runtime': 78.74698090553284, 'val_loss': 0.6951984167098999, 'train_loss': 0.6785187125205994, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 78}, 'val_acc': 0.5058365758754864, '_timestamp': 1686212862.180899, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",comic-plant-215
693,"{'trainer/global_step': 929, 'val_acc_rec_balanced': 0.7108116293155663, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 97}, 'val_acc': 0.6865935620799434, 'train_loss': 0.226645365357399, 'val_acc_rec': 0.708171206225681, '_runtime': 97.4555597305298, 'val_loss': 0.782922089099884, '_timestamp': 1686212687.9676297, 'val_acc_balanced': 0.68885523924894}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': False}",serene-oath-214
694,"{'epoch': 9, '_wandb': {'runtime': 81}, 'val_acc': 0.6970286522815705, '_timestamp': 1686212496.124222, 'train_loss': 0.5456197261810303, 'trainer/global_step': 929, '_step': 27, '_runtime': 82.0007050037384, 'val_loss': 0.5843812227249146, 'val_acc_rec': 0.7120622568093385, 'val_acc_balanced': 0.6968903144099994, 'val_acc_rec_balanced': 0.7119321623258631}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",easy-terrain-213
695,"{'_step': 27, '_wandb': {'runtime': 75}, 'val_acc': 0.5058365758754864, 'val_acc_balanced': 0.5, 'train_loss': 0.6881319880485535, 'val_acc_rec': 0.5058365758754864, 'trainer/global_step': 939, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_runtime': 76.33273768424988, 'val_loss': 0.6950187683105469, '_timestamp': 1686212318.4523356}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",glamorous-valley-212
696,"{'_step': 27, 'epoch': 9, '_wandb': {'runtime': 100}, '_timestamp': 1686212146.4979453, 'val_acc_balanced': 0.6692500412972854, 'trainer/global_step': 939, 'val_acc': 0.6678457729041387, '_runtime': 101.2058002948761, 'val_loss': 0.6780084371566772, 'train_loss': 0.2889136075973511, 'val_acc_rec': 0.7159533073929961, 'val_acc_rec_balanced': 0.7169594185342216}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': False}",wise-tree-211
697,"{'val_acc_balanced': 0.706636473762458, 'trainer/global_step': 939, 'val_acc_rec_balanced': 0.7428831011508177, '_step': 27, 'epoch': 9, 'val_acc': 0.7071100106119561, '_runtime': 85.18406462669373, 'train_loss': 0.4934916198253632, '_wandb': {'runtime': 84}, 'val_loss': 0.5695905685424805, '_timestamp': 1686211950.3604655, 'val_acc_rec': 0.7431906614785992}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",rich-serenity-210
698,"{'epoch': 9, '_wandb': {'runtime': 77}, '_runtime': 77.83839106559753, 'val_loss': 0.6932600140571594, 'train_loss': 0.699072539806366, 'val_acc_rec_balanced': 0.5, '_step': 27, 'val_acc': 0.5058365758754864, '_timestamp': 1686211770.01429, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5, 'trainer/global_step': 909}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",winter-frog-209
699,"{'val_acc_balanced': 0.7031798909751665, 'epoch': 9, '_wandb': {'runtime': 97}, '_runtime': 97.61437368392944, '_timestamp': 1686211598.4377167, 'train_loss': 0.26433345675468445, 'val_acc_rec': 0.7276264591439688, 'trainer/global_step': 909, 'val_acc_rec_balanced': 0.7288612961841308, '_step': 27, 'val_acc': 0.7023346303501945, 'val_loss': 0.7538611888885498}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': False}",atomic-frog-208
700,"{'epoch': 9, '_runtime': 81.09416127204895, 'val_loss': 0.5706521272659302, 'train_loss': 0.4198428690433502, 'val_acc_rec_balanced': 0.7440642035130224, '_step': 27, '_wandb': {'runtime': 80}, 'val_acc': 0.713477184294305, '_timestamp': 1686211405.4653912, 'val_acc_rec': 0.7431906614785992, 'val_acc_balanced': 0.7144705688012775, 'trainer/global_step': 909}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",warm-sponge-207
701,"{'epoch': 9, '_wandb': {'runtime': 76}, 'val_loss': 0.6939254999160767, '_timestamp': 1686211228.1655383, 'val_acc_rec_balanced': 0.5, '_step': 27, 'val_acc': 0.5058365758754864, '_runtime': 78.10539221763611, 'train_loss': 0.6912399530410767, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5, 'trainer/global_step': 929}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",pious-cloud-206
702,"{'val_acc': 0.5058365758754864, '_runtime': 98.04869627952576, '_timestamp': 1686211054.8091302, 'val_acc_balanced': 0.5, 'trainer/global_step': 929, 'epoch': 9, '_wandb': {'runtime': 97}, 'train_loss': 0.6912398934364319, 'val_acc_rec': 0.5058365758754864, 'val_acc_rec_balanced': 0.5, '_step': 27, 'val_loss': 0.6939255595207214}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': False}",different-pond-205
703,"{'_timestamp': 1686210861.010506, 'train_loss': 0.570151150226593, 'val_acc_rec': 0.7198443579766537, 'val_loss': 0.6367584466934204, 'epoch': 9, '_wandb': {'runtime': 83}, 'val_acc': 0.7049876193845065, '_runtime': 84.32567691802979, 'val_acc_balanced': 0.7048730796762293, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.7199878861296185, '_step': 27}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",dauntless-blaze-204
704,"{'train_loss': 0.6635230779647827, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, 'val_loss': 0.6978484988212585, '_timestamp': 1686210680.07281, '_wandb': {'runtime': 77}, 'val_acc': 0.5058365758754864, '_runtime': 77.58297204971313, 'val_acc_rec': 0.5058365758754864, 'trainer/global_step': 889, '_step': 26, 'epoch': 9}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",winter-fog-203
705,"{'_wandb': {'runtime': 98}, 'val_acc': 0.6443226034665723, '_runtime': 98.994380235672, '_timestamp': 1686210506.0993502, 'train_loss': 0.31224003434181213, 'val_acc_rec': 0.6848249027237354, 'val_acc_rec_balanced': 0.6824651726226529, '_step': 26, 'epoch': 9, 'val_loss': 0.833122730255127, 'val_acc_balanced': 0.6417570618358021, 'trainer/global_step': 889}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': False}",helpful-vortex-202
706,"{'val_acc': 0.7304563141139017, 'val_acc_rec': 0.7392996108949417, 'val_acc_balanced': 0.73027091019217, 'trainer/global_step': 889, '_step': 26, 'epoch': 9, '_wandb': {'runtime': 75}, 'train_loss': 0.5175944566726685, 'val_acc_rec_balanced': 0.7390369473046638, '_runtime': 76.04133939743042, 'val_loss': 0.5621622800827026, '_timestamp': 1686210312.6197133}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 2, 'freeze_encoder': True}",olive-energy-201
707,"{'val_loss': 0.6933066844940186, '_timestamp': 1686210141.126232, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, '_step': 46, '_wandb': {'runtime': 99}, '_runtime': 100.14706587791444, 'train_loss': 0.6950099468231201, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5, 'epoch': 9, 'val_acc': 0.5058365758754864}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",leafy-firebrand-200
708,"{'_wandb': {'runtime': 132}, '_runtime': 132.9902675151825, '_timestamp': 1686209938.2629266, 'train_loss': 0.3438490331172943, 'val_acc_balanced': 0.7135785474368151, '_step': 46, 'epoch': 9, 'val_acc': 0.7118853908737177, 'val_loss': 0.7096554636955261, 'val_acc_rec': 0.7470817120622568, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.7491823137492428}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': False}",usual-yogurt-199
709,"{'_step': 46, 'epoch': 9, 'val_acc': 0.7178988326848249, 'train_loss': 0.5608749985694885, 'val_acc_rec': 0.7431906614785992, 'val_acc_balanced': 0.7185066901602335, '_wandb': {'runtime': 105}, '_runtime': 105.528226852417, 'val_loss': 0.5700834393501282, '_timestamp': 1686209701.728787, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.7437007874015749}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",skilled-star-198
710,"{'val_acc': 0.5058365758754864, '_timestamp': 1686209486.2565925, 'val_acc_rec': 0.5058365758754864, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 100}, 'val_loss': 0.693321704864502, 'train_loss': 0.6857802867889404, 'val_acc_balanced': 0.5, '_step': 46, '_runtime': 100.86458921432497}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",restful-wave-197
711,"{'_timestamp': 1686209280.4902484, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, '_wandb': {'runtime': 134}, 'val_acc': 0.5058365758754864, '_runtime': 134.52829837799072, 'val_loss': 0.69451904296875, 'train_loss': 0.6830257177352905, 'val_acc_rec_balanced': 0.5, '_step': 46, 'epoch': 9}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': False}",wobbly-waterfall-196
712,"{'_step': 46, 'val_loss': 0.5769728422164917, 'train_loss': 0.5508019924163818, 'epoch': 9, '_wandb': {'runtime': 103}, 'val_acc': 0.7033958259639194, '_runtime': 104.2373526096344, '_timestamp': 1686209044.6602695, 'val_acc_rec': 0.7120622568093385, 'val_acc_balanced': 0.7042783987665877, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.712931556632344}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",revived-brook-195
713,"{'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, '_step': 46, 'val_acc': 0.5058365758754864, '_runtime': 100.20234727859496, 'val_loss': 0.6945257782936096, '_timestamp': 1686208835.8030522, 'train_loss': 0.6789165139198303, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 99}}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",devoted-sky-194
714,"{'train_loss': 0.24205195903778076, 'val_acc_rec_balanced': 0.7146577831617202, 'epoch': 9, 'val_acc': 0.6873010258224266, '_runtime': 136.75085639953613, 'val_loss': 0.7419400811195374, 'val_acc_balanced': 0.6898601398601398, 'trainer/global_step': 1869, '_step': 46, '_wandb': {'runtime': 136}, '_timestamp': 1686208631.5400245, 'val_acc_rec': 0.7120622568093385}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': False}",lyric-feather-193
715,"{'epoch': 9, '_wandb': {'runtime': 101}, '_runtime': 101.84737253189088, 'train_loss': 0.6256473660469055, 'val_acc_rec': 0.7276264591439688, 'val_acc_balanced': 0.7147830515940752, 'trainer/global_step': 1869, '_step': 46, 'val_acc': 0.7143615139724089, 'val_loss': 0.578203558921814, '_timestamp': 1686208393.3335657, 'val_acc_rec_balanced': 0.7280436099333737}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",hearty-plasma-192
716,"{'_step': 46, '_wandb': {'runtime': 97}, 'val_acc': 0.5058365758754864, '_runtime': 97.61195015907288, '_timestamp': 1686208188.086837, 'train_loss': 0.6911998391151428, 'trainer/global_step': 1869, 'epoch': 9, 'val_loss': 0.6945724487304688, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",good-voice-191
717,"{'trainer/global_step': 1869, '_step': 46, '_wandb': {'runtime': 132}, 'val_acc': 0.5058365758754864, 'val_loss': 0.6945724487304688, 'train_loss': 0.6911998391151428, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_runtime': 133.09219312667847, '_timestamp': 1686207987.7458942, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': False}",sweet-universe-190
718,"{'val_acc': 0.7037495578351609, 'train_loss': 0.6930256485939026, 'val_acc_balanced': 0.7046074004735422, 'trainer/global_step': 1869, '_wandb': {'runtime': 102}, 'epoch': 9, '_runtime': 102.82071471214294, 'val_loss': 0.5827878713607788, '_timestamp': 1686207748.8391027, 'val_acc_rec': 0.7003891050583657, 'val_acc_rec_balanced': 0.7014839491217444, '_step': 46}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",unique-shape-189
719,"{'epoch': 9, '_wandb': {'runtime': 98}, 'val_acc': 0.5058365758754864, '_runtime': 99.02610349655151, 'val_acc_rec_balanced': 0.5, 'trainer/global_step': 1869, '_step': 46, 'val_loss': 0.6936824321746826, '_timestamp': 1686207541.4428184, 'train_loss': 0.6899221539497375, 'val_acc_rec': 0.5058365758754864, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",vague-silence-188
720,"{'val_loss': 0.6878786087036133, '_timestamp': 1686207340.8468127, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.776014536644458, 'epoch': 9, '_runtime': 134.9550998210907, 'val_acc': 0.7301025822426601, 'train_loss': 0.2498256713151932, 'val_acc_rec': 0.77431906614786, 'val_acc_balanced': 0.7315525026154948, '_step': 46, '_wandb': {'runtime': 134}}","{'weights': 'random', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': False}",serene-forest-187
721,"{'_timestamp': 1686207104.0560925, 'train_loss': 0.5102393627166748, 'val_acc_rec': 0.6964980544747081, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, 'val_acc': 0.6917226742129465, 'val_loss': 0.5945069193840027, '_wandb': {'runtime': 103}, '_runtime': 103.96835732460022, 'val_acc_balanced': 0.6930950938824955, 'val_acc_rec_balanced': 0.6978195033313144}","{'weights': 'eegclip', 'task_name': 'under_50', 'train_frac': 1, 'freeze_encoder': True}",cerulean-shadow-186
722,"{'epoch': 9, '_wandb': {'runtime': 54}, '_runtime': 55.14991617202759, 'val_acc_rec_balanced': 0.5, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, '_step': 9, 'val_acc': 0.490272373540856, 'val_loss': 0.913345456123352, '_timestamp': 1686206897.1698112, 'val_acc_rec': 0.490272373540856}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",giddy-serenity-185
723,"{'_wandb': {'runtime': 53}, 'val_acc': 0.5037141846480367, '_timestamp': 1686206755.5398977, '_step': 9, 'epoch': 9, '_runtime': 54.008848667144775, 'val_loss': 1.4650579690933228, 'val_acc_rec': 0.4863813229571984, 'val_acc_balanced': 0.5124417567165659, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.4960317460317461}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': False}",ethereal-donkey-184
724,"{'_step': 9, '_wandb': {'runtime': 55}, 'val_acc': 0.5357269189954015, '_runtime': 55.63040065765381, 'val_loss': 1.2954028844833374, '_timestamp': 1686206613.7025397, 'val_acc_rec_balanced': 0.5363504180298073, 'epoch': 9, 'val_acc_rec': 0.5291828793774319, 'val_acc_balanced': 0.5422945375235452, 'trainer/global_step': 29}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",resilient-snow-183
725,"{'trainer/global_step': 29, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 55}, '_runtime': 55.90847635269165, 'val_loss': 0.6927785277366638, 'val_acc_rec': 0.5097276264591439, 'val_acc': 0.5097276264591439, '_timestamp': 1686206469.0147264, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",good-wildflower-182
726,"{'_step': 9, '_wandb': {'runtime': 53}, 'val_acc': 0.4915104350902017, '_runtime': 54.361069440841675, 'val_loss': 1.2611432075500488, 'val_acc_rec': 0.47470817120622566, 'val_acc_rec_balanced': 0.4812492426996244, 'epoch': 9, '_timestamp': 1686206325.1544735, 'val_acc_balanced': 0.4964296729563905, 'trainer/global_step': 29}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': False}",daily-serenity-181
727,"{'val_acc': 0.6188539087371773, 'val_loss': 0.9245901107788086, '_timestamp': 1686206181.6999035, 'val_acc_balanced': 0.617639040348964, 'val_acc_rec': 0.6303501945525292, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.6287713558705925, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 59}, '_runtime': 59.6019926071167}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",hopeful-totem-180
728,"{'_wandb': {'runtime': 52}, '_timestamp': 1686206031.8031778, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, '_step': 9, 'epoch': 9, 'val_acc': 0.5097276264591439, '_runtime': 53.23935294151306, 'val_loss': 0.7870882153511047, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",worldly-butterfly-179
729,"{'val_acc_rec_balanced': 0.5, 'epoch': 9, '_runtime': 50.509743213653564, '_timestamp': 1686205892.629228, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, '_step': 9, '_wandb': {'runtime': 49}, 'val_acc': 0.5097276264591439, 'val_loss': 1.340012550354004, 'trainer/global_step': 29}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': False}",eternal-sea-178
730,"{'_wandb': {'runtime': 56}, 'val_acc': 0.6101874778917581, 'val_loss': 0.7677657604217529, '_timestamp': 1686205753.2663915, 'val_acc_rec': 0.6186770428015564, 'val_acc_balanced': 0.6053101902338544, '_step': 9, '_runtime': 57.06668257713318, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.6132315521628499, 'epoch': 9}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",colorful-sunset-177
731,"{'_step': 9, 'epoch': 9, '_runtime': 50.09931302070618, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 49}, 'val_acc': 0.5097276264591439, 'val_loss': 1.0876563787460327, '_timestamp': 1686205607.727712, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",drawn-snow-176
732,"{'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, 'val_acc': 0.5097276264591439, '_runtime': 56.78197455406189, 'val_acc_rec': 0.5097276264591439, 'val_loss': 2.5554916858673096, '_timestamp': 1686205470.1799626, 'val_acc_balanced': 0.5, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 56}}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': False}",azure-violet-175
733,"{'val_acc': 0.5097276264591439, '_runtime': 54.96502900123596, 'val_loss': 1.1976778507232666, '_timestamp': 1686205327.030785, 'val_acc_rec': 0.5097276264591439, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 54}, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",effortless-shadow-174
734,"{'epoch': 9, 'val_loss': 0.6932705640792847, '_timestamp': 1686205181.2471955, 'val_acc_balanced': 0.49550714340790675, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, '_step': 9, '_wandb': {'runtime': 55}, 'val_acc': 0.48620445702157766, '_runtime': 56.54615378379822, 'val_acc_rec': 0.490272373540856}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",peachy-sponge-173
735,"{'val_loss': 1.1230124235153198, '_wandb': {'runtime': 52}, 'val_acc': 0.5286522815705695, '_runtime': 52.986937522888184, '_timestamp': 1686205037.2407994, 'val_acc_rec': 0.5408560311284046, 'val_acc_balanced': 0.5269488780939162, '_step': 9, 'epoch': 9, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.537350054525627}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': False}",easy-pond-172
736,"{'val_acc_rec_balanced': 0.486974433539319, '_wandb': {'runtime': 60}, '_runtime': 60.91114830970764, 'val_loss': 0.8878642320632935, '_timestamp': 1686204892.6987965, 'val_acc_balanced': 0.4987800579403633, 'trainer/global_step': 29, '_step': 9, 'epoch': 9, 'val_acc': 0.5015917934205872, 'val_acc_rec': 0.490272373540856}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 50, 'freeze_encoder': True}",denim-plant-171
737,"{'_step': 10, 'val_acc': 0.490272373540856, '_timestamp': 1686204733.2825167, 'epoch': 9, '_wandb': {'runtime': 57}, '_runtime': 57.45767569541931, 'val_loss': 0.8424707651138306, 'train_loss': 0.6087741851806641, 'val_acc_rec': 0.490272373540856, 'val_acc_balanced': 0.5, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",crisp-spaceship-170
738,"{'_runtime': 57.7129693031311, 'val_loss': 2.2437541484832764, 'train_loss': 0.2244817614555359, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 57}, 'val_acc': 0.5282985496993279, '_timestamp': 1686204584.8914342, 'val_acc_rec': 0.5136186770428015, 'val_acc_balanced': 0.5363986098718924, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5224463831334061}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': False}",genial-flower-169
739,"{'val_acc': 0.5585426246904847, 'val_loss': 0.9261531233787536, 'train_loss': 0.4855105578899383, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 61}, 'val_acc_balanced': 0.5647436744764989, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5595541015388343, '_runtime': 61.96404409408569, '_timestamp': 1686204431.0924802, 'val_acc_rec': 0.5525291828793775}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",fresh-lake-168
740,"{'val_acc': 0.5097276264591439, '_runtime': 62.49031281471253, 'val_loss': 0.6930118203163147, 'trainer/global_step': 79, '_wandb': {'runtime': 61}, 'epoch': 9, '_timestamp': 1686204279.7948887, 'train_loss': 0.6926566958427429, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 10}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",misty-morning-167
741,"{'val_acc_rec': 0.5603112840466926, 'val_acc_balanced': 0.5425354967339701, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5662789288743487, '_wandb': {'runtime': 58}, 'val_acc': 0.5374955783516094, '_runtime': 59.19352722167969, 'val_loss': 1.6922532320022583, '_step': 10, 'epoch': 9, '_timestamp': 1686204130.800182, 'train_loss': 0.0867694839835167}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': False}",bright-silence-166
742,"{'_runtime': 60.944870471954346, '_timestamp': 1686203983.0533304, 'train_loss': 0.3834593892097473, 'val_acc_rec_balanced': 0.6405246577002301, 'val_acc_balanced': 0.6120308868400473, 'trainer/global_step': 79, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 60}, 'val_acc': 0.6139016625397948, 'val_loss': 0.8755006194114685, 'val_acc_rec': 0.642023346303502}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",vibrant-bee-165
743,"{'_step': 10, 'epoch': 9, 'val_loss': 0.8267545700073242, 'train_loss': 0.5918424725532532, 'val_acc_rec_balanced': 0.5, 'val_acc_balanced': 0.5, 'trainer/global_step': 79, '_wandb': {'runtime': 56}, 'val_acc': 0.5097276264591439, '_runtime': 57.0063099861145, '_timestamp': 1686203831.465475, 'val_acc_rec': 0.5097276264591439}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",jumping-paper-164
744,"{'_runtime': 63.80025005340576, 'val_loss': 2.601022481918335, 'val_acc_balanced': 0.4997246180452288, 'val_acc_rec_balanced': 0.494335393190355, '_step': 10, '_wandb': {'runtime': 63}, '_timestamp': 1686203687.985467, 'train_loss': 0.26026543974876404, 'val_acc_rec': 0.5019455252918288, 'trainer/global_step': 79, 'epoch': 9, 'val_acc': 0.506190307746728}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': False}",icy-morning-163
745,"{'_step': 10, '_wandb': {'runtime': 56}, 'val_acc': 0.6238061549345596, '_runtime': 57.335293769836426, 'val_loss': 0.99760901927948, 'trainer/global_step': 79, 'epoch': 9, '_timestamp': 1686203533.5859098, 'train_loss': 0.4133237600326538, 'val_acc_rec': 0.6186770428015564, 'val_acc_balanced': 0.620025224987057, 'val_acc_rec_balanced': 0.6142917726887193}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",peachy-sun-162
746,"{'_step': 10, 'val_acc': 0.5097276264591439, '_timestamp': 1686203387.2824297, 'val_acc_rec': 0.5097276264591439, 'trainer/global_step': 79, 'epoch': 9, '_wandb': {'runtime': 59}, '_runtime': 59.75608563423157, 'val_loss': 0.7880620956420898, 'train_loss': 0.6490539908409119, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",drawn-eon-161
747,"{'_runtime': 62.250248432159424, '_timestamp': 1686203239.2297685, 'trainer/global_step': 79, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 61}, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, 'val_acc': 0.5097276264591439, 'val_loss': 1.692249417304993, 'train_loss': 0.41493770480155945}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': False}",serene-paper-160
748,"{'val_loss': 0.8688722252845764, 'train_loss': 0.5142792463302612, 'val_acc_rec': 0.5680933852140078, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5620986308009208, '_step': 10, 'val_acc': 0.566147859922179, '_runtime': 61.98553490638733, '_timestamp': 1686203088.680466, 'val_acc_balanced': 0.5605413458466894, 'epoch': 9, '_wandb': {'runtime': 61}}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",fanciful-wildflower-159
749,"{'val_loss': 0.6935809254646301, 'train_loss': 0.680031418800354, 'val_acc_balanced': 0.49665273233975527, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.4957288258814976, 'val_acc': 0.4874425185709232, 'epoch': 9, '_wandb': {'runtime': 55}, '_runtime': 56.041499376297, '_timestamp': 1686202936.9137294, 'val_acc_rec': 0.4863813229571984, '_step': 10}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",quiet-elevator-158
750,"{'val_loss': 1.3368133306503296, 'train_loss': 0.20251482725143433, 'val_acc_rec': 0.5525291828793775, 'val_acc_balanced': 0.5480527741978123, 'trainer/global_step': 79, 'epoch': 9, 'val_acc': 0.546692607003891, '_runtime': 56.35490083694458, '_timestamp': 1686202793.1875515, 'val_acc_rec_balanced': 0.5551617593602326, '_step': 10, '_wandb': {'runtime': 55}}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': False}",cerulean-gorge-157
751,"{'train_loss': 0.5953190326690674, 'val_acc_balanced': 0.5408928433737594, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5393190355022416, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 64}, '_timestamp': 1686202648.3561084, 'val_acc': 0.542270958613371, '_runtime': 65.23303651809692, 'val_loss': 0.7718104124069214, 'val_acc_rec': 0.5408560311284046}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 20, 'freeze_encoder': True}",quiet-universe-156
752,"{'_step': 12, 'epoch': 9, '_wandb': {'runtime': 59}, '_runtime': 60.25424838066101, 'val_loss': 0.7184391617774963, 'val_acc_balanced': 0.5, 'trainer/global_step': 169, 'val_acc': 0.490272373540856, '_timestamp': 1686202492.537465, 'train_loss': 0.6823786497116089, 'val_acc_rec': 0.490272373540856, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",devout-tree-155
753,"{'val_loss': 1.2033557891845703, 'train_loss': 0.22017230093479156, 'val_acc_balanced': 0.6059242919929944, 'val_acc_rec_balanced': 0.6279837634799468, 'val_acc': 0.6050583657587548, 'epoch': 9, '_wandb': {'runtime': 62}, '_runtime': 62.51661992073059, '_timestamp': 1686202344.5194168, 'val_acc_rec': 0.6264591439688716, 'trainer/global_step': 169, '_step': 12}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': False}",revived-donkey-154
754,"{'_wandb': {'runtime': 64}, 'val_acc': 0.628758401131942, 'val_acc_balanced': 0.631278708568785, '_step': 12, 'epoch': 9, '_runtime': 64.61698746681213, 'val_loss': 0.6919754147529602, '_timestamp': 1686202191.9963734, 'train_loss': 0.4820488095283508, 'val_acc_rec': 0.6381322957198443, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.6409487459105779}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",elated-pine-153
755,"{'_timestamp': 1686202037.5241227, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.5, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 59}, 'val_acc': 0.5097276264591439, '_runtime': 60.138747692108154, 'val_loss': 0.6948172450065613, 'train_loss': 0.6870836019515991}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",silvery-lion-152
756,"{'_wandb': {'runtime': 65}, '_runtime': 66.27192544937134, 'val_loss': 0.6957969665527344, 'val_acc_rec': 0.5097276264591439, 'val_acc_rec_balanced': 0.5, 'val_acc_balanced': 0.5, 'trainer/global_step': 169, '_step': 12, 'epoch': 9, 'val_acc': 0.5097276264591439, '_timestamp': 1686201888.3170545, 'train_loss': 0.685783326625824}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': False}",true-sun-151
757,"{'val_acc': 0.6328263176512203, 'val_loss': 0.7653694748878479, '_timestamp': 1686201733.1411998, 'val_acc_rec': 0.6381322957198443, 'epoch': 9, '_wandb': {'runtime': 63}, 'train_loss': 0.3988981246948242, 'val_acc_balanced': 0.6307045371930868, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.6359505634314795, '_step': 12, '_runtime': 63.55236291885376}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",leafy-donkey-150
758,"{'_wandb': {'runtime': 58}, '_runtime': 59.1460485458374, 'val_loss': 0.6966906189918518, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 12, 'epoch': 9, 'val_acc': 0.5097276264591439, '_timestamp': 1686201578.4099977, 'train_loss': 0.699165940284729, 'val_acc_rec': 0.5097276264591439, 'trainer/global_step': 179}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",vague-bee-149
759,"{'epoch': 9, '_wandb': {'runtime': 65}, 'val_acc': 0.551998585072515, '_runtime': 65.76093363761902, 'val_loss': 1.4745986461639404, 'trainer/global_step': 179, '_step': 12, '_timestamp': 1686201431.6487937, 'train_loss': 0.1642397940158844, 'val_acc_rec': 0.556420233463035, 'val_acc_balanced': 0.5496499895354857, 'val_acc_rec_balanced': 0.5523143099478978}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': False}",sandy-night-148
760,"{'_timestamp': 1686201276.4010625, 'train_loss': 0.34898078441619873, 'val_acc_balanced': 0.6631954771267748, 'val_acc_rec_balanced': 0.6748758027383981, '_step': 12, 'val_acc': 0.6650159179342059, 'val_loss': 0.6848124861717224, 'val_acc_rec': 0.6770428015564203, 'trainer/global_step': 179, 'epoch': 9, '_wandb': {'runtime': 60}, '_runtime': 61.29817724227905}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",swift-durian-147
761,"{'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, 'trainer/global_step': 189, 'val_acc_rec_balanced': 0.5, '_step': 12, 'val_acc': 0.5097276264591439, '_timestamp': 1686201124.391148, 'train_loss': 0.6704280376434326, 'epoch': 9, '_wandb': {'runtime': 63}, '_runtime': 64.1902129650116, 'val_loss': 0.746181845664978}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",hopeful-fire-146
762,"{'epoch': 9, '_runtime': 66.20724606513977, '_timestamp': 1686200971.39721, 'val_acc_balanced': 0.5419296564334732, 'val_acc_rec_balanced': 0.5045740942687508, '_step': 12, '_wandb': {'runtime': 65}, 'val_acc': 0.5481075344888574, 'val_loss': 1.4504138231277466, 'train_loss': 0.3370263874530792, 'val_acc_rec': 0.5136186770428015, 'trainer/global_step': 189}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': False}",denim-pond-145
763,"{'epoch': 9, '_runtime': 67.43721032142639, 'val_loss': 0.7534738183021545, 'val_acc_balanced': 0.5816521264994547, 'trainer/global_step': 189, 'val_acc_rec': 0.603112840466926, 'val_acc_rec_balanced': 0.5987216769659518, '_step': 12, '_wandb': {'runtime': 67}, 'val_acc': 0.5859568447117085, '_timestamp': 1686200816.4244072, 'train_loss': 0.5973525643348694}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",whole-silence-144
764,"{'_wandb': {'runtime': 56}, 'val_acc': 0.5097276264591439, 'val_loss': 0.699162483215332, '_timestamp': 1686200658.0092478, 'train_loss': 0.6989377737045288, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, 'epoch': 9, '_runtime': 57.51744270324707, 'trainer/global_step': 159, 'val_acc_rec_balanced': 0.5, '_step': 12}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",lunar-durian-143
765,"{'val_acc_rec_balanced': 0.5556767236156549, '_wandb': {'runtime': 67}, 'val_acc': 0.5604881499823134, 'val_loss': 1.5561320781707764, '_timestamp': 1686200513.024094, 'train_loss': 0.2206903845071793, 'trainer/global_step': 159, '_step': 12, 'epoch': 9, '_runtime': 68.12534689903259, 'val_acc_rec': 0.5603112840466926, 'val_acc_balanced': 0.5570756639458929}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': False}",efficient-firebrand-142
766,"{'epoch': 9, '_runtime': 51.237128257751465, 'val_loss': 0.7623509764671326, 'train_loss': 0.44391608238220215, 'trainer/global_step': 159, 'val_acc_rec_balanced': 0.6055676723615655, '_step': 12, '_wandb': {'runtime': 50}, 'val_acc': 0.5956844711708525, '_timestamp': 1686200354.1583433, 'val_acc_rec': 0.6070038910505836, 'val_acc_balanced': 0.5941200445017238}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 10, 'freeze_encoder': True}",wise-brook-141
767,"{'_wandb': {'runtime': 56}, 'val_loss': 0.6935477256774902, 'train_loss': 0.6922048926353455, 'trainer/global_step': 369, '_step': 16, 'epoch': 9, '_timestamp': 1686200211.1427162, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, 'val_acc': 0.5097276264591439, '_runtime': 57.42204022407532}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",summer-snow-140
768,"{'val_acc_rec_balanced': 0.5808493881012965, '_runtime': 72.81509804725647, 'val_loss': 1.2074849605560305, '_timestamp': 1686200064.221975, 'trainer/global_step': 369, 'train_loss': 0.19598983228206637, 'val_acc_rec': 0.5836575875486382, 'val_acc_balanced': 0.5850599781897492, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 72}, 'val_acc': 0.5877255040679166}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': False}",crisp-moon-139
769,"{'_runtime': 69.62365508079529, 'train_loss': 0.522363543510437, 'val_acc_rec': 0.6459143968871596, 'val_acc_balanced': 0.6482821673661369, 'val_acc_rec_balanced': 0.6473706530958439, '_wandb': {'runtime': 69}, 'val_acc': 0.6471524584365052, 'val_loss': 0.657414972782135, '_timestamp': 1686199902.3982131, 'trainer/global_step': 369, '_step': 16, 'epoch': 9}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",misunderstood-dream-138
770,"{'_runtime': 66.44764518737793, 'train_loss': 0.688876748085022, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_timestamp': 1686199740.4460342, 'val_acc_rec': 0.490272373540856, 'trainer/global_step': 369, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 65}, 'val_acc': 0.490272373540856, 'val_loss': 0.6976844668388367}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",logical-surf-137
771,"{'train_loss': 0.3108006715774536, 'trainer/global_step': 369, '_step': 16, 'epoch': 9, 'val_acc': 0.62610541209763, '_runtime': 73.61818146705627, 'val_loss': 0.9283128380775452, '_timestamp': 1686199582.9038155, '_wandb': {'runtime': 73}, 'val_acc_rec': 0.6264591439688716, 'val_acc_balanced': 0.6263493715783792, 'val_acc_rec_balanced': 0.6284381437053193}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': False}",serene-river-136
772,"{'val_loss': 0.6499873995780945, '_timestamp': 1686199420.0710998, 'val_acc_rec': 0.622568093385214, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.6246213498121895, '_wandb': {'runtime': 69}, 'epoch': 9, 'val_acc': 0.6222143615139724, '_runtime': 70.12474465370178, 'train_loss': 0.679446816444397, 'val_acc_balanced': 0.6241917539627464, '_step': 16}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",kind-sky-135
773,"{'_wandb': {'runtime': 54}, '_runtime': 55.08409810066223, 'train_loss': 0.7159507274627686, 'val_acc_balanced': 0.5, 'trainer/global_step': 359, '_step': 16, 'epoch': 9, '_timestamp': 1686199255.2704282, 'val_acc_rec': 0.5097276264591439, 'val_acc_rec_balanced': 0.5, 'val_acc': 0.5097276264591439, 'val_loss': 0.6982498168945312}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",fiery-sun-134
774,"{'val_acc': 0.5652635302440749, '_runtime': 75.08621335029602, '_timestamp': 1686199110.3825142, 'train_loss': 0.1770615428686142, 'trainer/global_step': 359, 'val_acc_rec_balanced': 0.5783957348842845, '_step': 16, 'epoch': 9, 'val_acc_rec': 0.5797665369649806, 'val_acc_balanced': 0.5643829241157485, '_wandb': {'runtime': 74}, 'val_loss': 1.1678285598754885}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': False}",crimson-thunder-133
775,"{'_wandb': {'runtime': 71}, '_runtime': 71.8047308921814, '_timestamp': 1686198944.232896, 'train_loss': 0.516604483127594, 'val_acc_rec': 0.669260700389105, 'val_acc_balanced': 0.649776114470771, 'val_acc_rec_balanced': 0.6695141160790016, '_step': 16, 'epoch': 9, 'val_acc': 0.6498054474708171, 'val_loss': 0.6524068713188171, 'trainer/global_step': 359}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",icy-grass-132
776,"{'_timestamp': 1686198781.2752564, 'train_loss': 0.6385998725891113, 'val_acc_balanced': 0.5, 'trainer/global_step': 389, 'epoch': 9, '_wandb': {'runtime': 55}, '_runtime': 55.898579359054565, 'val_acc_rec': 0.5097276264591439, 'val_acc_rec_balanced': 0.5, '_step': 16, 'val_acc': 0.5097276264591439, 'val_loss': 0.7014695405960083}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",comic-valley-131
777,"{'val_acc_rec_balanced': 0.6519750393796195, '_step': 16, 'epoch': 9, 'val_acc': 0.6519278386982668, '_runtime': 76.20487332344055, '_timestamp': 1686198636.3846264, 'train_loss': 0.291677862405777, 'val_acc_rec': 0.6536964980544747, '_wandb': {'runtime': 75}, 'val_loss': 0.8279575705528259, 'val_acc_balanced': 0.6510318561845279, 'trainer/global_step': 389}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': False}",fallen-water-130
778,"{'_wandb': {'runtime': 70}, 'val_acc': 0.6551114255394411, '_runtime': 70.50424456596375, 'val_loss': 0.6939094662666321, '_timestamp': 1686198471.3171237, 'val_acc_rec_balanced': 0.6571549739488671, '_step': 16, 'train_loss': 0.5113458037376404, 'val_acc_rec': 0.6575875486381323, 'val_acc_balanced': 0.6548775651829087, 'trainer/global_step': 389, 'epoch': 9}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",vibrant-voice-129
779,"{'_step': 15, '_wandb': {'runtime': 54}, 'val_acc': 0.5097276264591439, 'val_loss': 0.6930919885635376, 'train_loss': 0.6906296014785767, 'val_acc_rec': 0.5097276264591439, 'epoch': 9, '_runtime': 55.23179244995117, '_timestamp': 1686198307.7524245, 'val_acc_balanced': 0.5, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",royal-rain-128
780,"{'train_loss': 0.26464685797691345, 'trainer/global_step': 329, 'epoch': 9, '_timestamp': 1686198163.2310634, 'val_acc': 0.5847187831623629, '_runtime': 67.65663528442383, 'val_loss': 1.2795186042785645, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5815736426423449, 'val_acc_rec_balanced': 0.6112322791712105, '_step': 15, '_wandb': {'runtime': 67}}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': False}",laced-sunset-127
781,"{'_timestamp': 1686198004.396018, 'train_loss': 0.47264185547828674, 'val_acc_rec': 0.6459143968871596, 'val_acc_rec_balanced': 0.6467648127953471, '_wandb': {'runtime': 64}, 'val_acc': 0.6466218606296428, '_runtime': 64.74382305145264, 'val_acc_balanced': 0.6465706685172334, 'trainer/global_step': 329, '_step': 15, 'epoch': 9, 'val_loss': 0.7905296087265015}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 5, 'freeze_encoder': True}",comic-lake-126
782,"{'train_loss': 0.6856030821800232, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 73}, 'val_acc': 0.5097276264591439, '_runtime': 73.88204073905945, 'val_loss': 0.6975129842758179, '_timestamp': 1686197847.6850798, 'val_acc_balanced': 0.5, 'trainer/global_step': 929, 'val_acc_rec': 0.5097276264591439, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",chocolate-lion-125
783,"{'val_loss': 0.7742576003074646, 'val_acc_rec': 0.6264591439688716, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.6199563794983642, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 91}, '_runtime': 92.40553879737854, 'val_acc': 0.6094800141492749, '_timestamp': 1686197680.7912228, 'train_loss': 0.36227384209632874, 'val_acc_balanced': 0.6035973144751771}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': False}",lemon-cherry-124
784,"{'_step': 27, 'val_acc_rec': 0.622568093385214, 'val_acc_rec_balanced': 0.6268932509390525, '_runtime': 79.59058165550232, 'val_loss': 0.6730937957763672, '_timestamp': 1686197491.6306386, 'train_loss': 0.6426426768302917, 'val_acc_balanced': 0.612281484418889, 'epoch': 9, '_wandb': {'runtime': 79}, 'val_acc': 0.6080650866643085, 'trainer/global_step': 929}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",dandy-oath-123
785,"{'val_acc': 0.490272373540856, '_timestamp': 1686197313.4014757, 'val_acc_rec': 0.490272373540856, '_step': 27, 'epoch': 9, 'val_loss': 0.6931729912757874, 'train_loss': 0.6932286620140076, 'val_acc_balanced': 0.5, 'trainer/global_step': 939, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 75}, '_runtime': 75.91237378120422}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",comfy-cloud-122
786,"{'val_acc': 0.490272373540856, '_runtime': 94.21484565734865, 'val_loss': 0.6931734085083008, 'val_acc_rec': 0.490272373540856, 'trainer/global_step': 939, 'val_acc_rec_balanced': 0.5, '_step': 27, '_wandb': {'runtime': 94}, 'train_loss': 0.693229615688324, 'val_acc_balanced': 0.5, 'epoch': 9, '_timestamp': 1686197140.2983706}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': False}",breezy-star-121
787,"{'val_loss': 0.6276642680168152, '_timestamp': 1686196947.683226, 'trainer/global_step': 939, 'val_acc_rec_balanced': 0.6205016357688113, 'epoch': 9, '_wandb': {'runtime': 78}, 'val_acc': 0.6299964626812876, '_runtime': 78.57619404792786, 'train_loss': 0.5924298167228699, 'val_acc_rec': 0.6186770428015564, 'val_acc_balanced': 0.6315706134408425, '_step': 27}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",denim-thunder-120
788,"{'epoch': 9, '_runtime': 75.95709156990051, 'val_acc_rec': 0.5097276264591439, 'val_acc_rec_balanced': 0.5, 'trainer/global_step': 909, '_step': 27, '_wandb': {'runtime': 75}, 'val_acc': 0.5097276264591439, 'val_loss': 0.6950770616531372, '_timestamp': 1686196772.1361835, 'train_loss': 0.7046564817428589, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",grateful-leaf-119
789,"{'_step': 27, '_wandb': {'runtime': 94}, 'val_acc': 0.647329324372126, '_timestamp': 1686196601.156655, 'train_loss': 0.38424673676490784, 'val_acc_rec': 0.6809338521400778, 'val_acc_balanced': 0.6480770078098322, 'epoch': 9, '_runtime': 94.78615617752077, 'val_loss': 0.8067032098770142, 'trainer/global_step': 909, 'val_acc_rec_balanced': 0.6814188779837635}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': False}",dashing-leaf-118
790,"{'val_acc_rec_balanced': 0.6284381437053193, '_runtime': 81.3476014137268, 'train_loss': 0.6545119881629944, 'val_acc_balanced': 0.6386079992950222, 'trainer/global_step': 909, 'val_loss': 0.6478813290596008, '_timestamp': 1686196412.0371163, 'val_acc_rec': 0.6264591439688716, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 80}, 'val_acc': 0.6363636363636364}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",denim-microwave-117
791,"{'val_acc': 0.5097276264591439, '_timestamp': 1686196233.232025, 'train_loss': 0.6755253672599792, 'trainer/global_step': 929, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 72}, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_runtime': 72.92994260787964, 'val_loss': 0.6967244148254395, 'val_acc_rec': 0.5097276264591439}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",absurd-flower-116
792,"{'_wandb': {'runtime': 95}, '_runtime': 95.27639579772948, '_timestamp': 1686196062.8522727, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 27, 'epoch': 9, 'train_loss': 0.6755248308181763, 'val_acc_rec': 0.5097276264591439, 'trainer/global_step': 929, 'val_acc': 0.5097276264591439, 'val_loss': 0.6967250108718872}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': False}",smart-serenity-115
793,"{'epoch': 9, 'train_loss': 0.5902268290519714, '_runtime': 80.7174015045166, 'val_loss': 0.6335825324058533, '_timestamp': 1686195872.9980345, 'val_acc_rec': 0.6614785992217899, 'val_acc_balanced': 0.6565381183701795, '_step': 27, '_wandb': {'runtime': 80}, 'val_acc': 0.6570569508312699, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.6614261480673694}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",crimson-eon-114
794,"{'val_acc': 0.5097276264591439, '_runtime': 75.94798541069031, 'val_loss': 0.6930648684501648, '_timestamp': 1686195696.2354624, 'train_loss': 0.6785959601402283, 'val_acc_rec': 0.5097276264591439, 'val_acc_rec_balanced': 0.5, '_step': 26, 'epoch': 9, '_wandb': {'runtime': 75}, 'val_acc_balanced': 0.5, 'trainer/global_step': 889}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",polished-universe-113
795,"{'val_acc': 0.6271666077113548, '_runtime': 93.89030194282532, 'val_loss': 0.9709616303443908, 'train_loss': 0.3814717829227447, 'val_acc_balanced': 0.6231907405571527, 'val_acc_rec_balanced': 0.6333757421543681, '_step': 26, '_wandb': {'runtime': 93}, '_timestamp': 1686195525.294708, 'val_acc_rec': 0.6381322957198443, 'trainer/global_step': 889, 'epoch': 9}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': False}",silvery-wave-112
796,"{'_step': 26, 'val_acc': 0.6448532012734347, '_runtime': 80.33211517333984, 'val_loss': 0.6579471230506897, '_timestamp': 1686195337.0889273, 'train_loss': 0.5379294157028198, 'val_acc_rec': 0.6614785992217899, 'epoch': 9, '_wandb': {'runtime': 79}, 'val_acc_balanced': 0.6459372900212594, 'trainer/global_step': 889, 'val_acc_rec_balanced': 0.6629407488186114}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 2, 'freeze_encoder': True}",wandering-hill-111
797,"{'trainer/global_step': 1869, 'epoch': 9, '_wandb': {'runtime': 90}, 'val_loss': 0.6936151385307312, 'train_loss': 0.687730610370636, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 46, 'val_acc': 0.5097276264591439, '_runtime': 90.773455619812, '_timestamp': 1686195159.0697846, 'val_acc_rec': 0.5097276264591439}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",kind-rain-110
798,"{'_step': 46, 'epoch': 9, '_wandb': {'runtime': 130}, 'val_acc': 0.644676335337814, 'val_acc_balanced': 0.6409996915722107, 'val_acc_rec_balanced': 0.6571852659638919, '_runtime': 130.2740216255188, 'val_loss': 0.749303936958313, '_timestamp': 1686194965.5917857, 'train_loss': 0.4924376606941223, 'val_acc_rec': 0.6614785992217899, 'trainer/global_step': 1869}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': False}",elated-wildflower-109
799,"{'_step': 46, 'val_acc': 0.5864874425185709, 'val_acc_rec': 0.5680933852140078, 'val_acc_balanced': 0.5915768921494112, 'epoch': 9, '_wandb': {'runtime': 99}, '_runtime': 99.62013339996338, 'val_loss': 0.6817893981933594, '_timestamp': 1686194733.1137674, 'train_loss': 0.5729513764381409, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5739125166606083}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",glorious-cherry-108
800,"{'_wandb': {'runtime': 94}, 'val_acc': 0.5097276264591439, '_timestamp': 1686194528.3313515, 'train_loss': 0.698582112789154, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, '_step': 46, 'epoch': 9, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, '_runtime': 94.43063950538637, 'val_loss': 0.6937415599822998}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",dainty-shape-107
801,"{'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 46, 'epoch': 9, 'val_acc': 0.5097276264591439, '_runtime': 128.53988432884216, 'train_loss': 0.6985825896263123, '_wandb': {'runtime': 128}, 'val_loss': 0.6937413215637207, '_timestamp': 1686194329.3456693, 'trainer/global_step': 1869}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': False}",treasured-wildflower-106
802,"{'val_acc_rec': 0.642023346303502, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 98}, 'val_acc': 0.6218606296427308, '_runtime': 99.12804746627808, 'val_loss': 0.6426606774330139, '_timestamp': 1686194098.7425754, 'train_loss': 0.6559277772903442, 'val_acc_balanced': 0.6254351034885386, 'val_acc_rec_balanced': 0.6461286804798255}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",different-sky-105
803,"{'_wandb': {'runtime': 91}, 'val_acc': 0.5097276264591439, 'train_loss': 0.6892586350440979, 'val_acc_rec_balanced': 0.5, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, '_runtime': 91.55204820632936, 'val_loss': 0.6938571333885193, '_timestamp': 1686193894.531752, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",cosmic-resonance-104
804,"{'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.6792984369320247, '_step': 46, 'epoch': 9, 'val_acc': 0.671913689423417, 'val_acc_rec': 0.6809338521400778, 'train_loss': 0.42692500352859497, 'val_acc_balanced': 0.6708084112664265, '_wandb': {'runtime': 128}, '_runtime': 129.11614632606506, 'val_loss': 0.7003797292709351, '_timestamp': 1686193700.7119572}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': False}",solar-mountain-103
805,"{'epoch': 9, 'val_acc': 0.5417403608065087, '_runtime': 97.24627494812012, 'train_loss': 0.6361225247383118, 'val_acc_rec': 0.5330739299610895, 'val_acc_balanced': 0.5494875141821707, 'trainer/global_step': 1869, '_step': 46, '_wandb': {'runtime': 97}, 'val_loss': 0.7782089114189148, '_timestamp': 1686193468.845758, 'val_acc_rec_balanced': 0.5415303525990549}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",peach-morning-102
806,"{'val_acc': 0.5097276264591439, '_runtime': 92.480859041214, 'val_loss': 0.6936954259872437, 'train_loss': 0.6839688420295715, 'trainer/global_step': 1869, '_step': 46, '_wandb': {'runtime': 92}, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_timestamp': 1686193266.7184072}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",fresh-breeze-101
807,"{'epoch': 9, '_wandb': {'runtime': 129}, '_step': 46, '_runtime': 129.40904760360718, 'val_loss': 0.6936952471733093, '_timestamp': 1686193071.4579446, 'train_loss': 0.6839686036109924, 'val_acc_rec': 0.5097276264591439, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'val_acc': 0.5097276264591439, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': False}",serene-blaze-100
808,"{'val_acc': 0.5624336752741422, '_runtime': 96.86965775489809, 'val_loss': 0.7268841862678528, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5635223555070883, 'epoch': 9, '_wandb': {'runtime': 96}, 'train_loss': 0.695738673210144, 'val_acc_rec': 0.556420233463035, 'val_acc_balanced': 0.5689528876551777, '_step': 46, '_timestamp': 1686192840.0905068}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",eager-sky-99
809,"{'val_acc_rec': 0.5097276264591439, '_step': 46, '_wandb': {'runtime': 91}, 'val_acc': 0.5097276264591439, '_runtime': 91.75823259353638, '_timestamp': 1686192638.6333146, 'train_loss': 0.6988294124603271, 'epoch': 9, 'val_loss': 0.6937940120697021, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",different-frost-98
810,"{'val_acc': 0.6777502652989035, '_runtime': 129.4355800151825, '_timestamp': 1686192445.655691, 'val_acc_rec': 0.7120622568093385, 'val_acc_balanced': 0.6789294251126312, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 129}, 'val_loss': 0.7359344959259033, 'train_loss': 0.4684476554393768, 'val_acc_rec_balanced': 0.7137707500302921}","{'weights': 'random', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': False}",brisk-thunder-97
811,"{'_step': 46, 'epoch': 9, 'val_acc': 0.5316590024761231, 'val_loss': 0.8110684752464294, 'train_loss': 0.614775538444519, 'val_acc_rec_balanced': 0.5338967648127954, '_wandb': {'runtime': 96}, '_runtime': 96.63652539253236, '_timestamp': 1686192215.3829694, 'val_acc_rec': 0.5252918287937743, 'val_acc_balanced': 0.5398463919456286, 'trainer/global_step': 1869}","{'weights': 'eegclip', 'task_name': 'gender', 'train_frac': 1, 'freeze_encoder': True}",resilient-sponge-96
812,"{'_step': 9, 'epoch': 9, '_wandb': {'runtime': 57}, 'val_loss': 0.7383630275726318, '_timestamp': 1686192016.064857, 'val_acc_rec': 0.6147859922178989, 'val_acc': 0.6147859922178989, '_runtime': 57.49038791656494, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",trim-rain-95
813,"{'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 59}, 'val_acc': 0.6147859922178989, 'val_loss': 0.9022261500358582, 'trainer/global_step': 29, '_step': 9, '_runtime': 60.06492471694946, '_timestamp': 1686191869.4814646, 'val_acc_rec': 0.6147859922178989}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': False}",sunny-plasma-94
814,"{'val_acc_balanced': 0.5995745719566203, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5909090909090909, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 57}, 'val_acc': 0.691368942341705, 'val_acc_rec': 0.6848249027237354, '_runtime': 57.466593742370605, 'val_loss': 0.5913490056991577, '_timestamp': 1686191722.2229958}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",sunny-sunset-93
815,"{'_wandb': {'runtime': 62}, 'val_acc': 0.6147859922178989, '_runtime': 62.77864623069763, 'val_acc_rec': 0.6147859922178989, 'val_acc_rec_balanced': 0.5, '_step': 9, 'epoch': 9, 'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'val_loss': 0.6826403141021729, '_timestamp': 1686191574.2904463}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",polished-sky-92
816,"{'val_acc_rec_balanced': 0.5, 'val_acc': 0.6147859922178989, '_timestamp': 1686191420.2485137, 'val_acc_rec': 0.6147859922178989, 'trainer/global_step': 29, 'val_loss': 0.7196682691574097, 'val_acc_balanced': 0.5, '_step': 9, 'epoch': 9, '_wandb': {'runtime': 58}, '_runtime': 58.47557234764099}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': False}",twilight-monkey-91
817,"{'trainer/global_step': 29, 'val_acc_rec_balanced': 0.7493606955632273, 'epoch': 9, '_runtime': 63.69389796257019, 'val_loss': 0.5090827941894531, '_timestamp': 1686191274.6722898, 'val_acc_rec': 0.8054474708171206, 'val_acc_balanced': 0.7481707175320524, '_step': 9, '_wandb': {'runtime': 63}, 'val_acc': 0.8020870180403255}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",efficient-dew-90
818,"{'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 9, 'epoch': 9, 'val_acc': 0.6147859922178989, 'val_loss': 0.7529855370521545, '_timestamp': 1686191119.8286529, '_wandb': {'runtime': 60}, '_runtime': 61.44155597686768, 'val_acc_rec': 0.6147859922178989, 'trainer/global_step': 29}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",dashing-haze-89
819,"{'epoch': 9, '_timestamp': 1686190970.2972915, 'val_acc_rec': 0.6381322957198443, 'val_acc_balanced': 0.550429496344341, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5303030303030303, '_step': 9, '_wandb': {'runtime': 58}, 'val_acc': 0.6399009550760524, '_runtime': 59.43872356414795, 'val_loss': 1.2508665323257446}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': False}",quiet-silence-88
820,"{'val_acc': 0.6793420587194906, 'trainer/global_step': 29, '_step': 9, '_wandb': {'runtime': 57}, 'val_loss': 0.5698684453964233, '_timestamp': 1686190824.2212958, 'val_acc_rec': 0.6614785992217899, 'val_acc_balanced': 0.5837924701561066, 'val_acc_rec_balanced': 0.5606060606060606, 'epoch': 9, '_runtime': 58.23588871955872}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",ethereal-galaxy-87
821,"{'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_runtime': 47.623302698135376, '_timestamp': 1686190675.2469656, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5, '_step': 9, '_wandb': {'runtime': 47}, 'val_acc': 0.6147859922178989, 'val_loss': 0.7392012476921082}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",brisk-glitter-86
822,"{'_step': 9, '_wandb': {'runtime': 56}, 'val_acc': 0.63937035726919, 'val_acc_balanced': 0.5597706059443689, 'val_acc_rec_balanced': 0.5088224012274646, 'epoch': 9, '_runtime': 57.316558599472046, 'val_loss': 1.2959431409835815, '_timestamp': 1686190537.9019415, 'val_acc_rec': 0.6186770428015564, 'trainer/global_step': 29}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': False}",different-durian-85
823,"{'_step': 9, 'val_acc_balanced': 0.7181422975439086, 'epoch': 9, '_wandb': {'runtime': 60}, 'val_acc': 0.7714892111779271, '_runtime': 60.58949685096741, 'val_loss': 0.5277207493782043, '_timestamp': 1686190389.3751528, 'val_acc_rec': 0.7704280155642024, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.7114499424626007}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",comfy-breeze-84
824,"{'val_acc_balanced': 0.5, 'trainer/global_step': 29, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 58}, 'val_acc': 0.6147859922178989, '_runtime': 59.00130891799927, 'val_loss': 0.6660215854644775, '_step': 9, '_timestamp': 1686190236.476513, 'val_acc_rec': 0.6147859922178989}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",genial-river-83
825,"{'_wandb': {'runtime': 55}, 'val_acc': 0.6805801202688362, '_timestamp': 1686190088.444223, 'val_acc_balanced': 0.6512361823063779, 'val_acc_rec_balanced': 0.672132719601074, '_step': 9, 'epoch': 9, '_runtime': 55.848212003707886, 'val_loss': 1.2788742780685425, 'val_acc_rec': 0.708171206225681, 'trainer/global_step': 29}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': False}",rich-shape-82
826,"{'_step': 9, 'val_acc': 0.7458436505129112, '_timestamp': 1686189943.694562, 'val_acc_rec': 0.77431906614786, 'val_acc_rec_balanced': 0.7561053573711802, 'epoch': 9, '_wandb': {'runtime': 58}, '_runtime': 59.50170087814331, 'val_loss': 0.5603055953979492, 'val_acc_balanced': 0.7344038776720019, 'trainer/global_step': 29}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 50, 'freeze_encoder': True}",upbeat-fog-81
827,"{'_step': 10, '_timestamp': 1686189792.4524815, 'val_acc_rec': 0.6147859922178989, 'val_acc_rec_balanced': 0.5, 'val_acc_balanced': 0.5, 'trainer/global_step': 79, 'epoch': 9, '_wandb': {'runtime': 58}, 'val_acc': 0.6147859922178989, '_runtime': 59.445860624313354, 'val_loss': 0.6811002492904663, 'train_loss': 0.6229144930839539}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",wandering-surf-80
828,"{'epoch': 9, 'val_acc': 0.6641315882561019, '_runtime': 58.8206844329834, '_timestamp': 1686189643.9968643, 'train_loss': 0.348827064037323, 'val_acc_rec': 0.7042801556420234, '_step': 10, 'val_loss': 0.9898674488067628, 'val_acc_balanced': 0.6352869895735258, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.6274773046924946, '_wandb': {'runtime': 58}}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': False}",winter-energy-79
829,"{'val_acc_rec_balanced': 0.7581830967906917, '_step': 10, 'val_acc': 0.7943049168730103, 'val_loss': 0.4580632150173187, 'val_acc_rec': 0.8093385214007782, 'val_acc_balanced': 0.7411558043031001, 'trainer/global_step': 79, 'epoch': 9, '_wandb': {'runtime': 66}, '_runtime': 66.49000000953674, '_timestamp': 1686189499.422043, 'train_loss': 0.270697683095932}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",graceful-sunset-78
830,"{'val_acc_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 56}, 'val_acc': 0.6147859922178989, '_timestamp': 1686189342.2060635, 'train_loss': 0.5613618493080139, 'val_acc_rec': 0.6147859922178989, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5, '_step': 10, '_runtime': 57.11688542366028, 'val_loss': 0.8531314134597778}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",mild-terrain-77
831,"{'_step': 10, '_wandb': {'runtime': 63}, 'val_acc': 0.6696144322603467, '_runtime': 64.25296020507812, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5574415036440352, 'epoch': 9, 'val_loss': 1.4448868036270142, '_timestamp': 1686189197.1550891, 'train_loss': 0.22376783192157745, 'val_acc_rec': 0.6575875486381323, 'val_acc_balanced': 0.5779384756657484}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': False}",mild-snowflake-76
832,"{'epoch': 9, '_wandb': {'runtime': 62}, 'val_acc': 0.7810399717014503, 'val_loss': 0.6593725085258484, 'val_acc_rec_balanced': 0.7209436133486766, '_step': 10, '_runtime': 62.89418888092041, '_timestamp': 1686189043.534266, 'train_loss': 0.20118902623653412, 'val_acc_rec': 0.7821011673151751, 'val_acc_balanced': 0.7212807011426113, 'trainer/global_step': 79}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",grateful-snowball-75
833,"{'epoch': 9, '_runtime': 60.615782737731934, 'train_loss': 0.6931557655334473, 'val_acc_rec': 0.3852140077821012, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5, '_step': 10, '_wandb': {'runtime': 60}, 'val_acc': 0.3852140077821012, 'val_loss': 0.6969193816184998, '_timestamp': 1686188889.2094908, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",graceful-dawn-74
834,"{'epoch': 9, '_wandb': {'runtime': 63}, '_runtime': 63.81719160079956, 'train_loss': 0.06433418393135071, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.8091036951796445, '_step': 10, 'val_acc': 0.7615847187831624, 'val_loss': 1.1490405797958374, '_timestamp': 1686188738.7012277, 'val_acc_rec': 0.8093385214007782, 'val_acc_balanced': 0.7595503365066081}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': False}",confused-puddle-73
835,"{'val_acc': 0.8369296073576229, 'val_acc_rec_balanced': 0.8363700294080041, '_step': 10, 'epoch': 9, '_wandb': {'runtime': 62}, 'train_loss': 0.10415957868099213, 'val_acc_rec': 0.8521400778210116, 'val_acc_balanced': 0.8203990422057166, 'trainer/global_step': 79, '_runtime': 63.25805640220642, 'val_loss': 0.49434715509414673, '_timestamp': 1686188581.2349553}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",jumping-bush-72
836,"{'_wandb': {'runtime': 56}, '_runtime': 57.262916803359985, 'train_loss': 0.5820906162261963, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5, '_step': 10, 'val_acc': 0.6147859922178989, 'val_loss': 0.7338351011276245, '_timestamp': 1686188426.8772397, 'trainer/global_step': 79, 'val_acc_rec_balanced': 0.5, 'epoch': 9}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",glamorous-capybara-71
837,"{'_runtime': 64.77347731590271, 'val_loss': 1.4041070938110352, 'train_loss': 0.21591883897781372, 'val_acc_rec': 0.6536964980544747, 'val_acc_balanced': 0.611610930943497, '_step': 10, '_wandb': {'runtime': 64}, 'val_acc': 0.6977361160240537, 'val_acc_rec_balanced': 0.5505050505050505, 'epoch': 9, '_timestamp': 1686188280.1773844, 'trainer/global_step': 79}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': False}",classic-sea-70
838,"{'epoch': 9, '_timestamp': 1686188125.316184, 'train_loss': 0.25357845425605774, 'val_acc_balanced': 0.7643262312422267, 'trainer/global_step': 79, '_step': 10, '_wandb': {'runtime': 60}, 'val_acc': 0.7831623629288998, '_runtime': 61.05045700073242, 'val_loss': 0.5909323692321777, 'val_acc_rec': 0.8015564202334631, 'val_acc_rec_balanced': 0.7763713080168776}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",vital-smoke-69
839,"{'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 61}, '_runtime': 62.240827322006226, 'val_loss': 0.7045888304710388, '_timestamp': 1686187972.7217402, 'train_loss': 0.6517446041107178, 'trainer/global_step': 79, '_step': 10, 'epoch': 9, 'val_acc': 0.6147859922178989, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",dandy-smoke-68
840,"{'epoch': 9, '_wandb': {'runtime': 60}, 'val_loss': 2.124272346496582, 'val_acc_rec_balanced': 0.6482866641094489, 'train_loss': 0.067402184009552, 'val_acc_rec': 0.7159533073929961, 'val_acc_balanced': 0.6531584545105834, 'trainer/global_step': 79, '_step': 10, 'val_acc': 0.702865228157057, '_runtime': 60.77646541595459, '_timestamp': 1686187821.155193}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': False}",fresh-bee-67
841,"{'_timestamp': 1686187671.6696544, 'train_loss': 0.261879563331604, 'val_acc_balanced': 0.7832830607571689, 'val_acc_rec_balanced': 0.8186612965093978, '_step': 10, '_wandb': {'runtime': 61}, 'val_acc': 0.7957198443579766, 'val_loss': 0.5396458506584167, 'epoch': 9, '_runtime': 61.69117832183838, 'val_acc_rec': 0.8326848249027238, 'trainer/global_step': 79}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 20, 'freeze_encoder': True}",floral-grass-66
842,"{'_step': 12, 'val_acc': 0.6147859922178989, 'val_loss': 0.6719698905944824, '_timestamp': 1686187519.8478172, 'train_loss': 0.5941102504730225, 'trainer/global_step': 169, 'epoch': 9, '_wandb': {'runtime': 65}, '_runtime': 65.41794610023499, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",rare-durian-65
843,"{'val_acc': 0.6952599929253626, '_runtime': 71.10437989234924, 'val_loss': 0.9786004424095154, '_timestamp': 1686187362.190997, 'train_loss': 0.1943066567182541, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 70}, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.689170182841069, 'val_acc_rec': 0.7198443579766537, 'val_acc_balanced': 0.6834062721577105}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': False}",blooming-tree-64
844,"{'train_loss': 0.29053840041160583, 'trainer/global_step': 169, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 66}, 'val_acc': 0.7989034311991511, 'val_acc_balanced': 0.7485819065220676, 'val_acc_rec_balanced': 0.7531325917401868, '_runtime': 66.74774718284607, 'val_loss': 0.4455455541610718, '_timestamp': 1686187201.6590705, 'val_acc_rec': 0.8054474708171206}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",restful-music-63
845,"{'val_acc_rec': 0.6147859922178989, 'val_acc': 0.6147859922178989, '_runtime': 57.193806886672974, 'val_loss': 0.6649850606918335, '_timestamp': 1686187040.898311, 'train_loss': 0.6726202368736267, 'val_acc_balanced': 0.5, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.5, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 56}}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",warm-eon-62
846,"{'val_acc_balanced': 0.658239471818298, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.6218194604270554, 'epoch': 9, 'val_acc': 0.7279801910152105, '_runtime': 65.0684905052185, '_timestamp': 1686186892.5533185, 'val_acc_rec': 0.7042801556420234, '_step': 12, '_wandb': {'runtime': 64}, 'val_loss': 1.026718258857727, 'train_loss': 0.2865142226219177}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': False}",mild-pyramid-61
847,"{'epoch': 9, '_wandb': {'runtime': 68}, 'train_loss': 0.3307073414325714, 'val_acc_rec': 0.8521400778210116, 'val_acc_balanced': 0.8091530959770316, 'trainer/global_step': 169, 'val_acc_rec_balanced': 0.823168392788646, '_step': 12, 'val_acc': 0.8392288645206933, '_runtime': 69.09393191337585, 'val_loss': 0.4232672452926636, '_timestamp': 1686186736.796233}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",rosy-firefly-60
848,"{'val_acc_rec': 0.6147859922178989, 'trainer/global_step': 179, '_step': 12, 'epoch': 9, '_wandb': {'runtime': 53}, '_runtime': 54.19546985626221, '_timestamp': 1686186576.1453998, 'val_acc': 0.6147859922178989, 'val_loss': 0.665281355381012, 'train_loss': 0.6722806692123413, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",solar-field-59
849,"{'val_loss': 1.068759560585022, 'train_loss': 0.0556170716881752, 'val_acc_balanced': 0.7075501853982866, 'trainer/global_step': 179, '_step': 12, '_wandb': {'runtime': 64}, '_runtime': 65.12758088111877, '_timestamp': 1686186431.8846989, 'val_acc_rec': 0.7859922178988327, 'val_acc_rec_balanced': 0.7410817031070196, 'epoch': 9, 'val_acc': 0.7470817120622568}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': False}",comfy-tree-58
850,"{'val_acc': 0.7939511850017686, 'val_loss': 0.5106809735298157, 'train_loss': 0.17959719896316528, 'trainer/global_step': 179, 'val_acc_rec_balanced': 0.7946873801304182, '_step': 12, '_wandb': {'runtime': 53}, '_runtime': 54.372907400131226, '_timestamp': 1686186276.5278685, 'val_acc_rec': 0.8171206225680934, 'val_acc_balanced': 0.7746437330729621, 'epoch': 9}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",olive-waterfall-57
851,"{'_step': 12, '_wandb': {'runtime': 56}, '_runtime': 57.22319173812866, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5, 'trainer/global_step': 189, 'val_acc_rec_balanced': 0.5, 'epoch': 9, 'val_acc': 0.6147859922178989, 'val_loss': 0.6827531456947327, '_timestamp': 1686186124.3178887, 'train_loss': 0.6991400122642517}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",earnest-bush-56
852,"{'epoch': 9, '_wandb': {'runtime': 66}, 'val_acc': 0.7651220374955784, '_runtime': 67.07388591766357, 'val_acc_rec': 0.8054474708171206, 'val_acc_balanced': 0.7255654938336181, 'trainer/global_step': 189, '_step': 12, '_timestamp': 1686185976.082535, 'train_loss': 0.1188700944185257, 'val_acc_rec_balanced': 0.7682201764480245, 'val_loss': 0.927125096321106}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': False}",fragrant-water-55
853,"{'_wandb': {'runtime': 67}, '_timestamp': 1686185817.452655, 'train_loss': 0.348553329706192, 'val_acc_rec': 0.8560311284046692, 'val_acc_balanced': 0.8412999383942997, 'val_acc_rec_balanced': 0.8470783787239483, '_step': 12, 'epoch': 9, 'val_acc': 0.8496639547223205, '_runtime': 67.70267415046692, 'val_loss': 0.37978482246398926, 'trainer/global_step': 189}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",vague-donkey-54
854,"{'epoch': 9, '_wandb': {'runtime': 61}, 'val_acc': 0.6147859922178989, '_runtime': 61.87298655509949, '_timestamp': 1686185656.5673175, 'trainer/global_step': 159, '_step': 12, 'train_loss': 0.5820370316505432, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, 'val_loss': 0.6797881126403809}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",crisp-sun-53
855,"{'epoch': 9, 'val_acc': 0.7741422002122391, 'val_loss': 1.0858144760131836, '_timestamp': 1686185506.1240954, 'val_acc_rec': 0.8015564202334631, 'val_acc_balanced': 0.7470461229091838, 'trainer/global_step': 159, '_step': 12, '_wandb': {'runtime': 68}, '_runtime': 68.55595850944519, 'train_loss': 0.07150610536336899, 'val_acc_rec_balanced': 0.7631696713975195}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': False}",dutiful-deluge-52
856,"{'val_acc': 0.8199504775380262, '_runtime': 67.61215400695801, 'val_loss': 0.49760764837265015, 'val_acc_rec': 0.8482490272373541, 'trainer/global_step': 159, 'val_acc_rec_balanced': 0.8369773686229383, 'epoch': 9, '_wandb': {'runtime': 67}, 'train_loss': 0.31283530592918396, 'val_acc_balanced': 0.8142195836384559, '_step': 12, '_timestamp': 1686185348.152772}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 10, 'freeze_encoder': True}",fearless-snowball-51
857,"{'val_acc': 0.6147859922178989, '_runtime': 67.2996392250061, 'val_loss': 0.6695945262908936, '_timestamp': 1686185188.3843153, 'train_loss': 0.6161422729492188, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5, '_step': 16, '_wandb': {'runtime': 67}, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.5, 'epoch': 9}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",bumbling-sunset-50
858,"{'val_acc_rec': 0.77431906614786, 'val_acc_balanced': 0.7205745603329032, 'trainer/global_step': 369, 'val_acc_rec_balanced': 0.7447896688403017, 'val_acc': 0.7460205164485321, '_runtime': 77.6613335609436, 'train_loss': 0.08005698025226593, 'val_loss': 0.7395896315574646, '_timestamp': 1686185029.5426035, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 77}}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': False}",cosmic-meadow-49
859,"{'val_acc_balanced': 0.7939550859573874, 'val_acc_rec_balanced': 0.8092954865106764, '_step': 16, 'epoch': 9, '_wandb': {'runtime': 70}, 'val_acc': 0.8295012380615493, 'val_acc_rec': 0.8443579766536965, 'trainer/global_step': 369, '_runtime': 71.08724808692932, 'val_loss': 0.4120098054409027, '_timestamp': 1686184861.264034, 'train_loss': 0.43077775835990906}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",resilient-microwave-48
860,"{'_step': 16, 'epoch': 9, 'val_acc': 0.6147859922178989, 'val_loss': 0.6743741631507874, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 67}, '_runtime': 67.9135479927063, '_timestamp': 1686184696.734446, 'train_loss': 0.6356364488601685, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5, 'trainer/global_step': 369}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",amber-meadow-47
861,"{'_timestamp': 1686184538.0847373, 'val_acc_rec': 0.8210116731517509, 'val_acc_rec_balanced': 0.7733346119422069, 'epoch': 9, 'val_acc': 0.7801556420233463, '_runtime': 77.45768737792969, 'val_loss': 0.5886248350143433, 'trainer/global_step': 369, '_step': 16, '_wandb': {'runtime': 77}, 'train_loss': 0.34049203991889954, 'val_acc_balanced': 0.7250191791331032}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': False}",driven-elevator-46
862,"{'epoch': 9, '_runtime': 71.58305263519287, 'val_acc_rec': 0.867704280155642, 'val_acc_balanced': 0.8232802710650812, 'trainer/global_step': 369, 'train_loss': 0.4165671765804291, 'val_acc_rec_balanced': 0.8433704129906661, '_step': 16, '_wandb': {'runtime': 71}, 'val_acc': 0.8464803678811461, 'val_loss': 0.3663443326950073, '_timestamp': 1686184370.0504158}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",glorious-dragon-45
863,"{'epoch': 9, '_wandb': {'runtime': 69}, 'val_acc': 0.6147859922178989, '_runtime': 69.46222186088562, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 16, 'val_loss': 0.6727816462516785, '_timestamp': 1686184205.5593348, 'train_loss': 0.6498466730117798, 'val_acc_rec': 0.6147859922178989, 'trainer/global_step': 359}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",effortless-planet-44
864,"{'epoch': 9, 'val_acc': 0.7607003891050583, 'val_loss': 0.9401468634605408, 'trainer/global_step': 359, 'val_acc_rec_balanced': 0.7303733537910753, '_step': 16, '_wandb': {'runtime': 74}, '_runtime': 74.6700599193573, '_timestamp': 1686184042.993968, 'train_loss': 0.14557603001594543, 'val_acc_rec': 0.7821011673151751, 'val_acc_balanced': 0.7099679185409911}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': False}",mild-cloud-43
865,"{'_timestamp': 1686183875.7023163, 'val_acc_rec_balanced': 0.8041810510164941, 'epoch': 9, 'val_acc': 0.7999646268128758, '_runtime': 61.011141538619995, 'val_loss': 0.4979438781738281, 'train_loss': 0.27990102767944336, 'val_acc_rec': 0.8287937743190662, 'val_acc_balanced': 0.7812489102765282, 'trainer/global_step': 359, '_step': 16, '_wandb': {'runtime': 60}}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",super-dragon-42
866,"{'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5, '_step': 16, 'epoch': 9, 'val_acc': 0.6147859922178989, '_runtime': 69.04244780540466, 'val_loss': 0.6646040081977844, '_timestamp': 1686183719.2628417, 'trainer/global_step': 389, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 68}, 'train_loss': 0.7009205222129822}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",leafy-breeze-41
867,"{'val_loss': 0.7076249122619629, '_timestamp': 1686183559.2154398, 'val_acc_rec': 0.8132295719844358, '_step': 16, '_wandb': {'runtime': 73}, '_runtime': 74.3962197303772, 'val_acc_balanced': 0.7526255070846555, 'trainer/global_step': 389, 'val_acc_rec_balanced': 0.7802071346375143, 'epoch': 9, 'val_acc': 0.7702511496285815, 'train_loss': 0.2990206480026245}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': False}",comfy-moon-40
868,"{'val_acc_rec_balanced': 0.8055235903337169, 'epoch': 9, '_wandb': {'runtime': 72}, 'val_acc': 0.8367527414220022, '_timestamp': 1686183391.3033526, 'val_acc_balanced': 0.7981381711243621, 'trainer/global_step': 389, '_step': 16, '_runtime': 72.28426361083984, 'val_loss': 0.4202711880207062, 'train_loss': 0.28661179542541504, 'val_acc_rec': 0.8443579766536965}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",chocolate-sunset-39
869,"{'epoch': 9, 'val_acc': 0.6147859922178989, '_runtime': 64.51448655128479, 'val_loss': 0.6709193587303162, 'train_loss': 0.6896387934684753, '_step': 15, '_wandb': {'runtime': 64}, '_timestamp': 1686183226.0822206, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5, 'trainer/global_step': 329, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",dark-sun-38
870,"{'trainer/global_step': 329, 'val_acc_rec_balanced': 0.7575118271320802, 'epoch': 9, '_runtime': 74.47505116462708, 'val_loss': 0.8999115824699402, 'train_loss': 0.234749972820282, 'val_acc_rec': 0.8015564202334631, '_step': 15, '_wandb': {'runtime': 74}, 'val_acc': 0.7746727980191015, '_timestamp': 1686183072.6654932, 'val_acc_balanced': 0.7334187676535202}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': False}",fresh-capybara-37
871,"{'val_acc_rec_balanced': 0.8351553509781358, '_step': 15, '_wandb': {'runtime': 64}, 'train_loss': 0.4252730906009674, 'trainer/global_step': 329, '_timestamp': 1686182906.5887578, 'val_acc_rec': 0.8599221789883269, 'val_acc_balanced': 0.8262501307668166, 'epoch': 9, 'val_acc': 0.8461266360099045, '_runtime': 64.50220775604248, 'val_loss': 0.38299450278282166}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 5, 'freeze_encoder': True}",hardy-armadillo-36
872,"{'val_acc_rec': 0.6147859922178989, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 76}, 'val_acc': 0.6147859922178989, '_timestamp': 1686182747.2185888, 'val_acc_rec_balanced': 0.5, '_runtime': 76.74381685256958, 'val_loss': 0.6686979532241821, 'train_loss': 0.6600756645202637, 'val_acc_balanced': 0.5, 'trainer/global_step': 929}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",brisk-firebrand-35
873,"{'val_acc': 0.8026176158471878, 'train_loss': 0.20762532949447632, 'val_acc_rec': 0.8443579766536965, 'val_acc_balanced': 0.7954937754995293, 'trainer/global_step': 929, '_timestamp': 1686182573.4100409, 'val_acc_rec_balanced': 0.8375847078378724, '_step': 27, 'epoch': 9, '_wandb': {'runtime': 94}, '_runtime': 95.1176028251648, 'val_loss': 0.487432599067688}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': False}",fragrant-vortex-34
874,"{'val_acc': 0.8330385567739653, 'val_loss': 0.4320590496063232, 'train_loss': 0.2998499572277069, 'val_acc_rec': 0.8287937743190662, 'trainer/global_step': 929, '_step': 27, 'epoch': 9, '_timestamp': 1686182381.0104952, 'val_acc_balanced': 0.794431658355709, 'val_acc_rec_balanced': 0.7872075182201765, '_wandb': {'runtime': 80}, '_runtime': 80.8792712688446}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",stellar-hill-33
875,"{'val_loss': 0.6718044877052307, 'train_loss': 0.6933987736701965, 'val_acc_balanced': 0.5, 'trainer/global_step': 939, '_step': 27, 'epoch': 9, 'val_acc': 0.6147859922178989, '_runtime': 80.09571957588196, 'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 79}, '_timestamp': 1686182201.9310706, 'val_acc_rec': 0.6147859922178989}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",sweet-field-32
876,"{'_step': 27, 'epoch': 9, '_wandb': {'runtime': 96}, 'val_acc': 0.8264945171559958, 'val_loss': 0.4947378635406494, '_timestamp': 1686182023.8149748, 'trainer/global_step': 939, 'val_acc_rec_balanced': 0.8092954865106764, '_runtime': 96.8391797542572, 'train_loss': 0.2479489743709564, 'val_acc_rec': 0.8443579766536965, 'val_acc_balanced': 0.792281270704746}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': False}",lilac-jazz-31
877,"{'epoch': 9, '_wandb': {'runtime': 81}, 'val_acc': 0.8546162009197029, 'train_loss': 0.3041953146457672, 'val_acc_rec': 0.8754863813229572, 'val_acc_rec_balanced': 0.8515854750031966, '_step': 27, '_runtime': 82.05450630187988, 'val_loss': 0.35715433955192566, '_timestamp': 1686181831.0587132, 'val_acc_balanced': 0.8303256965512431, 'trainer/global_step': 939}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",kind-glade-30
878,"{'val_acc_rec_balanced': 0.5, '_wandb': {'runtime': 76}, 'val_acc': 0.6147859922178989, '_runtime': 77.05463600158691, '_timestamp': 1686181651.261399, 'train_loss': 0.682304859161377, 'val_acc_balanced': 0.5, '_step': 27, 'epoch': 9, 'val_loss': 0.6654484272003174, 'val_acc_rec': 0.6147859922178989, 'trainer/global_step': 909}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",blooming-valley-29
879,"{'_runtime': 93.72285223007202, '_timestamp': 1686181480.001907, 'val_acc_rec': 0.8326848249027238, 'trainer/global_step': 909, 'epoch': 9, '_wandb': {'runtime': 93}, 'val_acc': 0.7872302794481782, 'val_acc_balanced': 0.7487751508177285, 'val_acc_rec_balanced': 0.7941439713591613, '_step': 27, 'val_loss': 0.6927260756492615, 'train_loss': 0.17384950816631317}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': False}",electric-donkey-28
880,"{'_step': 27, 'epoch': 9, '_wandb': {'runtime': 81}, 'val_acc': 0.8335691545808277, 'val_loss': 0.39867153763771057, 'val_acc_rec': 0.8560311284046692, 'val_acc_rec_balanced': 0.8301048459276308, '_runtime': 82.15052652359009, '_timestamp': 1686181290.1442895, 'train_loss': 0.3966272473335266, 'val_acc_balanced': 0.8112366472550592, 'trainer/global_step': 909}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",swift-firefly-27
881,"{'_timestamp': 1686181111.663232, 'val_acc_balanced': 0.5, 'trainer/global_step': 929, 'epoch': 9, '_wandb': {'runtime': 77}, 'val_loss': 0.665494978427887, 'train_loss': 0.6903462409973145, 'val_acc_rec': 0.6147859922178989, 'val_acc_rec_balanced': 0.5, '_step': 27, 'val_acc': 0.6147859922178989, '_runtime': 78.18459415435791}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",helpful-haze-26
882,"{'train_loss': 0.27541014552116394, 'val_acc_rec': 0.8171206225680934, 'val_acc_rec_balanced': 0.7795997954225802, 'epoch': 9, '_wandb': {'runtime': 94}, '_runtime': 95.00878643989564, 'val_loss': 0.59285569190979, 'trainer/global_step': 929, '_step': 27, 'val_acc': 0.7981959674566679, '_timestamp': 1686180935.3802235, 'val_acc_balanced': 0.7614653438876684}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': False}",colorful-eon-25
883,"{'train_loss': 0.3858291208744049, 'trainer/global_step': 929, 'val_acc_rec_balanced': 0.8250543408771257, 'epoch': 9, '_wandb': {'runtime': 80}, 'val_acc': 0.8461266360099045, '_timestamp': 1686180742.54319, 'val_acc_rec': 0.8521400778210116, 'val_acc_balanced': 0.8223067847636317, '_step': 27, '_runtime': 80.54733300209045, 'val_loss': 0.3577750027179718}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",restful-plasma-24
884,"{'epoch': 9, '_wandb': {'runtime': 76}, '_runtime': 77.2864761352539, 'train_loss': 0.6613532304763794, 'val_acc_rec_balanced': 0.5, '_step': 26, 'val_acc': 0.6153165900247612, 'val_loss': 0.6650128364562988, '_timestamp': 1686180561.59542, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5007744301472725, 'trainer/global_step': 889}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",hardy-spaceship-23
885,"{'epoch': 9, 'trainer/global_step': 889, 'val_acc_rec_balanced': 0.7897008055235903, 'train_loss': 0.17836689949035645, 'val_acc_rec': 0.8249027237354085, '_step': 26, '_wandb': {'runtime': 95}, 'val_acc': 0.799787760877255, '_runtime': 95.60669326782228, 'val_loss': 0.5942288041114807, '_timestamp': 1686180388.7881362, 'val_acc_balanced': 0.7665318315490928}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': False}",apricot-lion-22
886,"{'val_acc_balanced': 0.8310085899268869, 'trainer/global_step': 889, '_step': 26, 'val_acc': 0.8436505129112133, '_runtime': 81.1567690372467, 'val_loss': 0.3602443337440491, '_timestamp': 1686180197.332983, 'val_acc_rec': 0.8560311284046692, 'epoch': 9, '_wandb': {'runtime': 80}, 'train_loss': 0.33151811361312866, 'val_acc_rec_balanced': 0.8395345863700294}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 2, 'freeze_encoder': True}",leafy-wildflower-21
887,"{'val_loss': 0.665093719959259, 'train_loss': 0.6469064950942993, 'val_acc_balanced': 0.5003152933244993, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.5, '_step': 46, 'epoch': 9, 'val_acc': 0.6149628581535196, 'val_acc_rec': 0.6147859922178989, '_wandb': {'runtime': 92}, '_runtime': 92.52847909927368, '_timestamp': 1686180015.6618862}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",daily-snowflake-20
888,"{'val_acc_rec_balanced': 0.8130673826876358, '_step': 46, '_wandb': {'runtime': 131}, 'val_acc': 0.8220728687654758, '_runtime': 131.64818811416626, 'val_acc_balanced': 0.7934857783822111, 'trainer/global_step': 1869, 'epoch': 9, 'val_loss': 0.4666776657104492, '_timestamp': 1686179818.4475832, 'train_loss': 0.16279250383377075, 'val_acc_rec': 0.8443579766536965}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': False}",twilight-salad-19
889,"{'train_loss': 0.4192274212837219, 'val_acc_balanced': 0.817573026002255, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 96}, 'val_acc': 0.8478952953661125, '_runtime': 96.61264967918396, 'val_loss': 0.38684841990470886, 'val_acc_rec_balanced': 0.8345480117632016, '_timestamp': 1686179581.4942546, 'val_acc_rec': 0.8638132295719845, 'trainer/global_step': 1869}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",clean-dream-18
890,"{'_step': 46, 'val_loss': 0.6724455952644348, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 90}, 'val_acc': 0.6147859922178989, '_runtime': 91.33383655548096, '_timestamp': 1686179371.7005086, 'train_loss': 0.6737776398658752, 'trainer/global_step': 1869}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",winter-haze-17
891,"{'val_loss': 0.468056321144104, '_timestamp': 1686179166.8707504, 'val_acc_rec': 0.8326848249027238, 'val_acc_balanced': 0.7763030186793132, 'val_acc_rec_balanced': 0.7922580232706815, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 130}, 'val_acc': 0.8151750972762646, '_runtime': 130.37921452522278, 'train_loss': 0.22477182745933533, 'trainer/global_step': 1869}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': False}",glorious-thunder-16
892,"{'val_acc_rec_balanced': 0.8370412990666155, '_step': 46, 'epoch': 9, '_wandb': {'runtime': 95}, 'val_acc': 0.8459497700742837, 'train_loss': 0.4213252365589142, 'val_acc_balanced': 0.8279065104439098, 'trainer/global_step': 1869, '_runtime': 96.14873313903809, 'val_loss': 0.36259740591049194, '_timestamp': 1686178932.7493672, 'val_acc_rec': 0.8599221789883269}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",sweet-dust-15
893,"{'val_acc': 0.6147859922178989, 'val_loss': 0.6671459674835205, 'train_loss': 0.6879449486732483, 'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5, 'trainer/global_step': 1869, '_step': 46, '_wandb': {'runtime': 91}, '_runtime': 91.38850831985474, '_timestamp': 1686178729.7863245, 'val_acc_rec_balanced': 0.5, 'epoch': 9}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",icy-armadillo-14
894,"{'_runtime': 133.68193793296814, '_timestamp': 1686178534.77381, 'train_loss': 0.23936700820922852, 'val_acc_rec': 0.7665369649805448, 'val_acc_balanced': 0.7040049517034557, 'val_acc_rec_balanced': 0.7007415931466565, 'epoch': 9, '_wandb': {'runtime': 133}, 'val_acc': 0.7675981605942696, 'val_loss': 0.6116151213645935, 'trainer/global_step': 1869, '_step': 46}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': False}",prime-snowball-13
895,"{'epoch': 9, '_wandb': {'runtime': 95}, '_runtime': 95.86161661148073, 'train_loss': 0.29560181498527527, 'trainer/global_step': 1869, '_step': 46, 'val_acc': 0.7714892111779271, 'val_loss': 0.5617945790290833, '_timestamp': 1686178296.9430947, 'val_acc_rec': 0.7626459143968871, 'val_acc_balanced': 0.7049406609245504, 'val_acc_rec_balanced': 0.6919191919191919}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",neat-aardvark-12
896,"{'val_acc': 0.6147859922178989, 'val_acc_balanced': 0.5, 'epoch': 9, '_wandb': {'runtime': 92}, '_runtime': 92.440899848938, 'val_loss': 0.6724774241447449, '_timestamp': 1686178093.9512808, 'train_loss': 0.6814811825752258, 'val_acc_rec': 0.6147859922178989, 'trainer/global_step': 1869, '_step': 46, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",wandering-cosmos-11
897,"{'trainer/global_step': 1869, '_step': 46, '_wandb': {'runtime': 129}, 'val_acc': 0.8141139016625398, '_runtime': 129.48289704322815, 'val_loss': 0.5077124238014221, 'val_acc_rec': 0.8210116731517509, 'epoch': 9, '_timestamp': 1686177897.268002, 'train_loss': 0.2591497004032135, 'val_acc_balanced': 0.7710679871209215, 'val_acc_rec_balanced': 0.7752205600306866}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': False}",true-breeze-10
898,"{'_step': 46, 'epoch': 9, 'val_acc': 0.8424124513618677, '_timestamp': 1686177663.5681355, 'train_loss': 0.5695804357528687, 'val_acc_rec_balanced': 0.8143459915611815, '_wandb': {'runtime': 95}, '_runtime': 95.66571879386902, 'val_loss': 0.39094242453575134, 'val_acc_rec': 0.8482490272373541, 'val_acc_balanced': 0.8088276319001291, 'trainer/global_step': 1869}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",solar-thunder-9
899,"{'_runtime': 93.18398952484132, 'val_loss': 0.6672725081443787, 'val_acc_rec': 0.6147859922178989, 'trainer/global_step': 1869, 'epoch': 9, '_wandb': {'runtime': 92}, '_timestamp': 1686177461.1753504, 'train_loss': 0.7040512561798096, 'val_acc_balanced': 0.5, 'val_acc_rec_balanced': 0.5, '_step': 46, 'val_acc': 0.6147859922178989}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",swift-wildflower-8
900,"{'val_loss': 0.4720900356769562, 'val_acc_rec_balanced': 0.8080168776371308, 'val_acc': 0.8144676335337814, '_runtime': 130.66858792304993, '_wandb': {'runtime': 130}, '_timestamp': 1686177258.481201, 'train_loss': 0.2448565661907196, 'val_acc_rec': 0.8404669260700389, 'val_acc_balanced': 0.7844715858237147, 'trainer/global_step': 1869, '_step': 46, 'epoch': 9}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': False}",mild-butterfly-7
901,"{'_wandb': {'runtime': 96}, 'val_acc': 0.8105765829501238, '_runtime': 97.07156133651732, '_step': 46, 'epoch': 9, 'val_loss': 0.46789002418518066, '_timestamp': 1686176994.9380684, 'train_loss': 0.3325057029724121, 'val_acc_rec': 0.8132295719844358, 'val_acc_balanced': 0.7591042763654962, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.7594617056642373}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",azure-shape-6
902,"{'val_acc_rec': 0.8365758754863813, 'val_acc_balanced': 0.783538782531878, '_step': 22, 'epoch': 4, '_wandb': {'runtime': 77}, 'val_acc': 0.8111071807569862, '_runtime': 70.40469360351562, '_timestamp': 1686176773.1317346, 'trainer/global_step': 934, 'val_loss': 0.44072574377059937, 'train_loss': 0.31779956817626953, 'val_acc_rec_balanced': 0.808624216852065}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': False}",logical-wind-5
903,"{'_step': 46, 'epoch': 9, 'val_acc': 0.8517863459497701, '_runtime': 95.9426064491272, 'val_loss': 0.3683018982410431, '_timestamp': 1686176597.7537804, 'val_acc_balanced': 0.8264811521428322, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.8383199079401611, '_wandb': {'runtime': 95}, 'train_loss': 0.40630972385406494, 'val_acc_rec': 0.8638132295719845}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",fine-firebrand-4
904,"{'val_acc_rec': 0.6147859922178989, 'val_acc_balanced': 0.5, '_step': 46, '_wandb': {'runtime': 91}, 'val_acc': 0.6147859922178989, 'val_loss': 0.6723492741584778, '_timestamp': 1686176395.8191335, 'train_loss': 0.6775659322738647, 'trainer/global_step': 1869, 'epoch': 9, '_runtime': 92.11845445632936, 'val_acc_rec_balanced': 0.5}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",floral-darkness-3
905,"{'val_loss': 0.5033456683158875, 'trainer/global_step': 1869, 'val_acc_rec_balanced': 0.7682841068917019, '_step': 46, 'epoch': 9, '_runtime': 129.98840045928955, 'train_loss': 0.1585996150970459, 'val_acc_rec': 0.8171206225680934, 'val_acc_balanced': 0.7547773477002475, '_wandb': {'runtime': 129}, 'val_acc': 0.8031482136540502, '_timestamp': 1686176199.6079843}","{'weights': 'random', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': False}",wobbly-cloud-2
906,"{'_wandb': {'runtime': 96}, 'val_acc': 0.8517863459497701, 'val_acc_rec_balanced': 0.8383199079401611, '_step': 46, 'epoch': 9, '_runtime': 96.83964228630066, 'val_loss': 0.3683018982410431, '_timestamp': 1686175966.0494952, 'train_loss': 0.40630972385406494, 'val_acc_rec': 0.8638132295719845, 'val_acc_balanced': 0.8264811521428322, 'trainer/global_step': 1869}","{'weights': 'eegclip', 'task_name': 'pathological', 'train_frac': 1, 'freeze_encoder': True}",desert-sky-1
