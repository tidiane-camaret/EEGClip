{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CLIP implementation on medical reports, where the two modalities are :\n",
    "- natural language description of the report\n",
    "- classification of the report (normal/abormal)\n",
    "\n",
    "Inspired by M.Shariatnia implementation : https://colab.research.google.com/drive/1hYHb0FTdKQCXZs3qCwVZnSuVGrZU2Z1w?usp=sharing#scrollTo=l9V91XcNi6lW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tidiane/dev/envs/env/lib/python3.7/site-packages/ipykernel_launcher.py:6: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read dataset and turn LABEL into a categorical var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "\n",
    "tuh_data = pd.read_csv('data/TUH_Abnormal_EEG_rep.csv') #open the original dataset\n",
    "tuh_data = tuh_data.drop([0]).dropna(subset=['DESCRIPTION OF THE RECORD']) #drop first line\n",
    "tuh_data = tuh_data.rename(columns={\"DESCRIPTION OF THE RECORD\": \"DESC\"})\n",
    "tuh_data['CAT'] = tuh_data.LABEL.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each category, calculate the mean normalized BERT embeddings and put it as embedding initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DistilBertModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12134/310683730.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"distilbert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbert_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"distilbert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfeatures_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DistilBertModel' is not defined"
     ]
    }
   ],
   "source": [
    "bert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "bert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "features_0, features_1 = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for desc in tuh_data[\"DESC\"][tuh_data[\"CAT\"]==0].to_list():\n",
    "        desc_tokenized = bert_tokenizer(desc, return_tensors=\"pt\")\n",
    "        outputs = bert_model(**desc_tokenized)\n",
    "        enc = outputs.to_tuple()[0][0][0].detach().numpy()\n",
    "\n",
    "        features_0.append(enc/np.linalg.norm(enc))\n",
    "\n",
    "    features_0 = np.array(features_0)\n",
    "\n",
    "    for desc in tuh_data[\"DESC\"][tuh_data[\"CAT\"]==1].to_list():\n",
    "        desc_tokenized = bert_tokenizer(desc, return_tensors=\"pt\")\n",
    "        outputs = bert_model(**desc_tokenized)\n",
    "        enc = outputs.to_tuple()[0][0][0].detach().numpy()\n",
    "\n",
    "        features_1.append(enc/np.linalg.norm(enc))\n",
    "\n",
    "    features_1 = np.array(features_0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_embs = np.array([\n",
    "    np.mean(features_0, axis=0),\n",
    "    np.mean(features_1, axis=0)\n",
    "    \n",
    "])\n",
    "\n",
    "initial_embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug = False\n",
    "    #image_path = image_path\n",
    "    #captions_path = captions_path\n",
    "    batch_size = 32\n",
    "    num_workers = 2\n",
    "    head_lr = 1e-3\n",
    "    #image_encoder_lr = 1e-4\n",
    "    category_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    weight_decay = 1e-3\n",
    "    patience = 1\n",
    "    factor = 0.8\n",
    "    epochs = 4\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_name = 'resnet50'\n",
    "    #image_embedding = 2048\n",
    "    nb_categories = 2\n",
    "    category_embedding = 768\n",
    "    text_encoder_model = \"distilbert-base-uncased\"\n",
    "    text_embedding = 768\n",
    "    text_tokenizer = \"distilbert-base-uncased\"\n",
    "    max_length = 200\n",
    "\n",
    "    pretrained_category = True \n",
    "    pretrained_text = True\n",
    "    trainable = True # for both encoders\n",
    "    temperature = 1.0\n",
    "\n",
    "    # image size\n",
    "    size = 224\n",
    "\n",
    "    # for projection head; used for both encoders\n",
    "    num_projection_layers = 1\n",
    "    projection_dim = 256 \n",
    "    dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.descs = df.DESC.to_list()\n",
    "        self.cats = df.CAT.to_list()\n",
    "        self.encoded_descs = tokenizer(\n",
    "            self.descs, padding=True, truncation=True, max_length=CFG.max_length\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_descs.items()\n",
    "        }\n",
    "        item['category'] = self.cats[idx]\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained_text, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryEncoder(nn.Module):\n",
    "    def __init__(self, pretrained=CFG.pretrained_category, trainable=CFG.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = nn.Embedding.from_pretrained(initial_embs)\n",
    "        else:\n",
    "            self.model = nn.Embedding(CFG.nb_categories, CFG.category_embedding)\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.model(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=CFG.projection_dim,\n",
    "        dropout=CFG.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=CFG.temperature,\n",
    "        category_embedding=CFG.category_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.category_encoder = CategoryEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.category_projection = ProjectionHead(embedding_dim=category_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Getting Image and Text Features\n",
    "        category_features = self.category_encoder(batch[\"category\"])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        category_embeddings = self.category_projection(category_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        # Calculating the Loss\n",
    "        logits = (text_embeddings @ category_embeddings.T) / self.temperature\n",
    "        categories_similarity = category_embeddings @ category_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (categories_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        categories_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (categories_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    dataset = CLIPDataset(\n",
    "        df=dataframe,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        num_workers=CFG.num_workers,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CFG.device) for k, v in batch.items()}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        count = batch[\"category\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter\n",
    "\n",
    "\n",
    "def valid_epoch(model, valid_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CFG.device) for k, v in batch.items()}\n",
    "        loss = model(batch)\n",
    "\n",
    "        count = batch[\"category\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df2d8c2feb540ab9ab5486f2fee4948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, valid_df = train_test_split(tuh_data, test_size=0.2)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
    "\n",
    "train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
    "valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "\n",
    "model = CLIPModel().to(CFG.device)\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "        {\"params\": model.category_encoder.parameters(), \"lr\": CFG.category_encoder_lr},\n",
    "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
    "        {\"params\": itertools.chain(\n",
    "            model.category_projection.parameters(), model.text_projection.parameters()\n",
    "        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay} \n",
    "                               ]\n",
    "                                , weight_decay=0.)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n",
    "    )\n",
    "\n",
    "step = \"epoch\"\n",
    "\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(CFG.epochs):\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    model.train()\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = valid_epoch(model, valid_loader)\n",
    "    \n",
    "    if valid_loss.avg < best_loss:\n",
    "        best_loss = valid_loss.avg\n",
    "        torch.save(model.state_dict(), \"best.pt\")\n",
    "        print(\"Saved Best Model!\")\n",
    "    \n",
    "    lr_scheduler.step(valid_loss.avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tid_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4ac7f30aa65cc641f5967c96ff1707a40f829f0dd175c4897139c8c0fb2d9eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
